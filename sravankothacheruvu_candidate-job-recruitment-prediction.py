# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
df=pd.read_csv("/kaggle/input/factors-affecting-campus-placement/Placement_Data_Full_Class.csv")
df.head(26)
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import numpy as np
from sklearn.metrics import mean_absolute_error
from sklearn.linear_model import LogisticRegression


df=df.drop(['ssc_b','hsc_b'],axis=1)

y=df['status'].copy()
#Dropping the output label from main dataframe and storing it in y
df=df.drop(['status','salary'],axis=1)
#drop salary since we are only predicted if the candidate is placed or not. So if we include salary in this situation, it would
#lead to target data leakage
l=LabelEncoder()
#encode work column as 0 or 1
df['workex']=l.fit_transform(df['workex'])
s=(df.dtypes=='object')
p=list(s[s].index)
print(p)












#split the data in to training and testing sets
xtrain,xtest,ytrain,ytest=train_test_split(df,y,test_size=0.2,random_state=3)
#One hot encoding all the object columns whose names are stored in p
one=OneHotEncoder(handle_unknown='ignore',sparse=False)
pd1=pd.DataFrame(one.fit_transform(xtrain[p]))
pd2=pd.DataFrame(one.fit_transform(xtest[p]))
pd1.index=xtrain.index
pd2.index=xtest.index
pd1.columns=one.get_feature_names(p)
pd2.columns=one.get_feature_names(p)
xtrain=xtrain.drop(p,axis=1)
xtest=xtest.drop(p,axis=1)
df1=pd.concat([xtrain,pd1],axis=1)
df2=pd.concat([xtest,pd2],axis=1)
print(df1)

#since we only need n-1 columns out of n columns generated by onehotencoding, drop them too
df1=df1.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)
df2=df2.drop(['gender_F','hsc_s_Arts','degree_t_Comm&Mgmt','specialisation_Mkt&Fin','sl_no'],axis=1)

ytrain=l.fit_transform(ytrain)
ytest=l.fit_transform(ytest)
model=DecisionTreeClassifier(random_state=1)
model1=LogisticRegression(random_state=1)

m=model.fit(df1,ytrain)
m1=model1.fit(df1,ytrain)

print("Model accuracy with test samples for Decision trees:")
print(m.score(df2,ytest))
yx=m.predict(df2)
print("Mean absolute error of model with testing data for decision trees:")
print(mean_absolute_error(yx,ytest))

print("Model accuracy with test samples for Logistic Regression:")
print(m1.score(df2,ytest))
yx1=m1.predict(df2)
print("Mean absolute error of model with testing data for Logistic Regression:")
print(mean_absolute_error(yx1,ytest))


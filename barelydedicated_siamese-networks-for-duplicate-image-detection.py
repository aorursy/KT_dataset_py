import numpy as np

from datetime import datetime

import matplotlib.pyplot as plt

import seaborn as sns

sns.set()



from tqdm import trange

from itertools import combinations

from tensorflow.keras.applications.resnet50 import ResNet50

from tensorflow.keras.models import Model, Sequential

from tensorflow.keras.preprocessing.image import ImageDataGenerator

from tensorflow.keras.applications.resnet50 import preprocess_input

from tensorflow.keras.preprocessing import image

from tensorflow.keras.layers import Dense, Flatten, Conv2D, Reshape, Lambda, Input, LeakyReLU

from tensorflow.keras.optimizers import Adam

import tensorflow.keras.backend as K
img_dim=512
train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

train_generator = train_datagen.flow_from_directory(

    directory=r"../input/airbnb-duplicate-image-detection/Airbnb Data/Training Data",

    color_mode="rgb",

    batch_size=1,

    target_size=(img_dim, img_dim),

    class_mode="sparse",

    shuffle=False,

    seed=42

)
test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)

test_generator = test_datagen.flow_from_directory(

    directory=r"../input/airbnb-duplicate-image-detection/Airbnb Data/Test Data",

    color_mode="rgb",

    batch_size=1,

    target_size=(img_dim, img_dim),

    class_mode="sparse",

    shuffle=False,

    seed=42

)
# https://jkjung-avt.github.io/keras-image-cropping/



def random_crop(img, random_crop_size=(img_dim, img_dim)):

    # Note: image_data_format is 'channel_last'

    assert img.shape[2] == 3

    height, width = img.shape[0], img.shape[1]

    dy, dx = random_crop_size

    x = np.random.randint(0, width - dx + 1)

    y = np.random.randint(0, height - dy + 1)

    return img[y:(y+dy), x:(x+dx), :]



def crop(batch):

    batch_crops = np.zeros((batch.shape[0], img_dim, img_dim, 3))

    for i in range(batch.shape[0]):

        batch_crops[i] = random_crop(batch[i])

    return batch_crops



def crop_generator(batches):

    """Take as input a Keras ImageGen (Iterator) and generate random

    crops from the image batches generated by the original iterator.

    """

    while True:

        tup = next(batches)

        yield (crop(tup[0]), crop(tup[1]), *tup[2:])
# id_dict = (id, cls) -> [positive examples]

# class_dict = (id) -> {cls}



def get_dicts(image_generator):

  id_dict = {}

  class_dict = {}

  for idx in trange(len(image_generator)):

    img, cls = image_generator.next()

    img_name = image_generator.filenames[idx]

    img = img[0]

    cls = int(cls[0])

    id = img_name.split('_')[1]

    if id in class_dict:

      class_dict[id].add(cls)

    else:

      class_dict[id] = {cls}

    if (id, cls) in id_dict:

      id_dict[(id, cls)].append(img)

    else:

      id_dict[(id, cls)] = [img]

  return id_dict, class_dict
train_id_dict, train_class_dict = get_dicts(train_generator)

test_id_dict, test_class_dict = get_dicts(test_generator)
!nvidia-smi
# returns a tuple of (batch_a, batch_b, epoch_end)

#                    (Tensor, Tensor, bool) where bool indicates end of epoch



def get_pos_batch(class_dict, id_dict, batch_size = 32):

  batch_a =  np.zeros((batch_size, img_dim, img_dim, 3)) # (16, 256, 256, 3)

  batch_b =  np.zeros((batch_size, img_dim, img_dim, 3)) # (16, 256, 256, 3)

  batch_idx = 0

  class_keys = list(class_dict.keys())

  np.random.shuffle(class_keys)

  for id in class_keys:

    classes = class_dict[id]

    for cls in classes:

      pos_examples = id_dict[(id,cls)] # list of positive examples

      for a, b in combinations(pos_examples, 2):

        batch_a[batch_idx, :, :, :] = a

        batch_b[batch_idx, :, :, :] = b

        batch_idx += 1

        if batch_idx == batch_size:

          yield [batch_a[:batch_idx], batch_b[:batch_idx], False]

          batch_a =  np.zeros((batch_size, img_dim, img_dim, 3))

          batch_b =  np.zeros((batch_size, img_dim, img_dim, 3))

          batch_idx = 0

  yield [batch_a[:batch_idx], batch_b[:batch_idx], True]
def get_neg_batch(class_dict, id_dict, batch_size = 32):

  batch_a =  np.zeros((batch_size, img_dim, img_dim, 3)) # (16, 256, 256, 3)

  batch_b =  np.zeros((batch_size, img_dim, img_dim, 3)) # (16, 256, 256, 3)

  batch_idx = 0

  ids = list(class_dict.keys())



  while True:

    random_ids = np.random.choice(ids, 2)

    classes_for_id_one = list(class_dict[random_ids[0]])

    random_class_from_one = np.random.choice(classes_for_id_one ,1)[0]

    classes_for_id_two = list(class_dict[random_ids[1]])

    random_class_from_two = np.random.choice(classes_for_id_two ,1)[0]



    if random_class_from_two != random_class_from_one:

      a, b = id_dict[(random_ids[0],random_class_from_one)], id_dict[(random_ids[1],random_class_from_two)]

      batch_a[batch_idx, :, :, :] = a[np.random.choice(len(a))]

      batch_b[batch_idx, :, :, :] = b[np.random.choice(len(b))]

      batch_idx += 1

      if batch_idx == batch_size:

        yield [batch_a[:batch_idx], batch_b[:batch_idx]]

        batch_a =  np.zeros((batch_size, img_dim, img_dim, 3))

        batch_b =  np.zeros((batch_size, img_dim, img_dim, 3))

        batch_idx = 0 
def euclidean_distance(vects):

    x, y = vects

    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)

    return K.sqrt(K.maximum(sum_square, K.epsilon()))



def eucl_dist_output_shape(shapes):

    shape1, shape2 = shapes

    return (shape1[0], 1)



def contrastive_loss(y_true, y_pred):

    '''Contrastive loss from Hadsell-et-al.'06

    http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf

    '''

    margin = 1

    square_pred = K.square(y_pred)

    margin_square = K.square(K.maximum(margin - y_pred, 0))

    return K.mean(y_true * square_pred + (1 - y_true) * margin_square)
optimizer = Adam()

base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(img_dim, img_dim, 3))

base_model.trainable=False

model = Sequential([

       base_model,   

       Conv2D(filters=1, kernel_size=(1,1)),

       Reshape((256,)),

       Dense(32, activation=LeakyReLU(0.1)),

       Dense(16)

])



model.summary()
inp_a = Input(shape=(img_dim, img_dim, 3), name='a') 

inp_b = Input(shape=(img_dim, img_dim, 3), name='b') 

out_a, out_b = model(inp_a), model(inp_b)

distance = Lambda(euclidean_distance, 

                  output_shape=eucl_dist_output_shape) ([out_a, out_b])



final_model = Model([inp_a, inp_b], distance)

final_model.compile(optimizer=optimizer, loss=contrastive_loss)

final_model.summary()
epochs, k = 20, 5
pos_loss = []

neg_loss = []

for epoch in trange(epochs):

  pos_gen = crop_generator(get_pos_batch(train_class_dict, train_id_dict, 32))

  neg_gen = crop_generator(get_neg_batch(train_class_dict, train_id_dict, 32))

  ###### 1 epoch ####################################

  p_loss, p_batches = 0, 0

  n_loss, n_batches = 0, 0

  while True:

    # positive examples

    batch_a, batch_b, epoch_end = next(pos_gen)

    pos_y = np.ones((batch_a.shape[0], 1))

    loss = final_model.train_on_batch({'a' : batch_a, 'b' : batch_b}, pos_y)

    p_loss += loss

    p_batches += 1



    # negative examples

    for _ in range(k):

      batch_a, batch_b = next(neg_gen)

      neg_y = np.zeros((batch_a.shape[0], 1))

      loss = final_model.train_on_batch({'a' : batch_a, 'b' : batch_b}, neg_y)

      n_loss += loss

      n_batches += 1



    if epoch_end:

      p_loss = p_loss / p_batches

      n_loss = n_loss / n_batches

      break

  ###### 1 epoch ####################################

  print(f'Epoch: {epoch+1}, time: {datetime.now().strftime("%H:%M:%S")}')

  print(f'avg (+) contrastive loss: {p_loss}')

  print(f'avg (-) contrastive loss: {n_loss}')

  pos_loss.append(p_loss)

  neg_loss.append(n_loss)
# Get some positive examples and negative examples (TRAIN)

pos_gen = crop_generator(get_pos_batch(train_class_dict, train_id_dict))

neg_gen = crop_generator(get_neg_batch(train_class_dict, train_id_dict))



X_distance= []

y_distance = []



pos_batch_count = 0

neg_to_pos_ratio = 1



while True:

  batch_a, batch_b, epoch_end = next(pos_gen)

  for x in final_model.predict([batch_a, batch_b]):

    X_distance.append(x)

  y_distance += [1] * len(batch_a)

  pos_batch_count += 1

  if epoch_end:

    break



for _ in range(pos_batch_count * neg_to_pos_ratio):

  batch_a, batch_b = next(neg_gen)

  for x in final_model.predict([batch_a, batch_b]):

    X_distance.append(x)

  y_distance += [0] * len(batch_a)



X_distance, y_distance = np.array(X_distance).reshape((-1,)), np.array(y_distance)

X_distance = X_distance.reshape((-1, 1))



print(X_distance.shape, y_distance.shape)
from sklearn.linear_model import LogisticRegression

from sklearn.metrics import f1_score



clf = LogisticRegression(random_state=0, solver='lbfgs').fit(X_distance, y_distance)



print('Training F1 Score:', f1_score(y_distance, clf.predict(X_distance)))
print(pos_loss)

plt.plot(np.arange(epochs), pos_loss)

plt.title('Positive Contrastive Loss for given configuration')

plt.xlabel('Epochs')

plt.ylabel('Loss')

plt.show
print(neg_loss)

plt.plot(np.arange(epochs), neg_loss)

plt.title('Negative Contrastive Loss for given configuration')

plt.xlabel('Epochs')

plt.ylabel('Loss')

plt.show
#TESTING

# Get some positive examples and negative examples (TRAIN)

pos_gen = crop_generator(get_pos_batch(test_class_dict, test_id_dict))

neg_gen = crop_generator(get_neg_batch(test_class_dict, test_id_dict))



X_distance= []

y_distance = []



pos_batch_count = 0

neg_to_pos_ratio = 1 # DONT CHANGE THIS



while True:

  batch_a, batch_b, epoch_end = next(pos_gen)

  for x in final_model.predict([batch_a, batch_b]):

    X_distance.append(x)

  y_distance += [1] * len(batch_a)

  pos_batch_count += 1

  if epoch_end:

    break



for _ in range(pos_batch_count * neg_to_pos_ratio):

  batch_a, batch_b = next(neg_gen)

  for x in final_model.predict([batch_a, batch_b]):

    X_distance.append(x)

  y_distance += [0] * len(batch_a)



X_distance, y_distance = np.array(X_distance).reshape((-1,)), np.array(y_distance)

X_distance = X_distance.reshape((-1, 1))



print(X_distance.shape, y_distance.shape)
print('Testing F1 Score:', f1_score(y_distance, clf.predict(X_distance)))
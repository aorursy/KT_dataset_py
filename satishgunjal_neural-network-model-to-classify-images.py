import tensorflow as tf

import numpy as np

import matplotlib.pyplot as plt

import os, datetime
import pkg_resources

import types

def get_imports():

    for name, val in globals().items():

        if isinstance(val, types.ModuleType):

            name = val.__name__.split(".")[0]



        elif isinstance(val, type):

            name = val.__module__.split(".")[0]



        if name == "PIL":

            name = "Pillow"

        elif name == "sklearn":

            name = "scikit-learn"



        yield name

        

def get_versions():

    imports = list(set(get_imports()))



    requirements = []

    for m in pkg_resources.working_set:

        if m.project_name in imports and m.project_name!="pip":

            requirements.append((m.project_name, m.version))



    for r in requirements:

        print("{}== {}".format(*r))



get_versions()
# List all files under the input directory

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))



# List of physical devices

tf.config.experimental.list_physical_devices()        
fashion_mnist = tf.keras.datasets.fashion_mnist

(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()



# Creating class label array

class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']
# Shape of training nad test data

print(f'Shape of train_images: {train_images.shape}')

print(f'Shape of train_labels: {train_labels.shape}')

print(f'Shape of test_images: {test_images.shape}')

print(f'Shape of test_labels: {test_labels.shape}')
# There are 10 labels starting from 0 to 9

print(f'Unique train labels: {np.unique(train_labels)}')

print(f'Unique test labels: {np.unique(test_labels)}')
# The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255

train_images


plt.figure()

plt.imshow(train_images[0])

plt.colorbar()

plt.grid(False)

plt.show()
# Images labels(classes) possible values from 0 to 9

train_labels
# Display the first 25 images from the training set and display the class name below each image.

plt.figure(figsize=(10,10))

for i in range(25):

    plt.subplot(5,5,i+1)

    plt.xticks([])

    plt.yticks([])

    plt.grid(False)

    plt.imshow(train_images[i], cmap=plt.cm.binary)

    plt.xlabel(class_names[train_labels[i]])

plt.show()
train_images = train_images / 255.0

test_images = test_images / 255.0
model = tf.keras.Sequential([

    tf.keras.layers.Flatten(input_shape=(28,28)),

    tf.keras.layers.Dense(128, activation= 'relu'),

    tf.keras.layers.Dense(10) # linear activation function

])
# The from_logits=True attribute inform the loss function that the output values generated by the model are not normalized, a.k.a. logits.

# Since softmax layer is not being added at the last layer, hence we need to have the from_logits=True to indicate the probabilities are not normalized.



model.compile(optimizer= 'adam', 

              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),

              metrics = ['accuracy'])
%%timeit -n1 -r1 # time required toexecute this cell once



# To view in TensorBoard

logdir = os.path.join("logs/adam", datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)



model.fit(train_images, train_labels, epochs= 10, callbacks = [tensorboard_callback])
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose= 2)

print(f'\nTest accuracy: {test_acc}')
probability_model = tf.keras.Sequential([model, tf.keras.layers.Softmax()])

predictions = probability_model.predict(test_images)
# 'predictions' will contain the prediction for each image in the training set. Lets check the first prediction

predictions[0]
np.argmax(predictions[0])
test_labels[0]
def plot_image(i, predictions_array, true_label, img):

    """

    This method will plot the true image and also compare the prediction with true values if matcing write the caption in green color else in red color.

    Format is : predicted class %confidence score (true class)

    

    Input:

        i: Index of the prediction to test

        predictions_array: Every prediction contain array of 10 number

        true_label: Correct image labels. In case of test data they are test_labels

        img: Test images. In case of test data they are test_images.

    """

    true_label, img = true_label[i], img[i]

    plt.grid(False)

    plt.xticks([])

    plt.yticks([])



    plt.imshow(img, cmap=plt.cm.binary) # For grayscale colormap



    predicted_label = np.argmax(predictions_array)

    if predicted_label == true_label:

        color = 'green'

    else:

        color = 'red'



    plt.xlabel("{} {:2.0f}% ({})".format(class_names[predicted_label],

                                100*np.max(predictions_array),

                                class_names[true_label]),

                                color=color)

    

plot_image(0, predictions[0], test_labels, test_images)
def plot_value_array(i, predictions_array, true_label):

    """

    This method will plot the percentage confidence score of each class prediction.

    

    Input:

        i: Index of the prediction to test

        predictions_array: Every prediction contain array of 10 number

        true_label: Correct image labels. In case of test data they are test_labels

    """

    true_label = true_label[i]

    plt.grid(False)

    plt.xticks(range(10))

    plt.yticks([])

    thisplot = plt.bar(range(10), predictions_array, color="#777777")

    plt.ylim([0, 1])

    predicted_label = np.argmax(predictions_array)



    thisplot[predicted_label].set_color('red')

    thisplot[true_label].set_color('green')



i = 0

plot_value_array(i, predictions[i],  test_labels)
i = 12

plt.figure(figsize=(12,6))

plt.subplot(1,2,1)

plot_image(i, predictions[i], test_labels, test_images)

plt.subplot(1,2,2)

plot_value_array(i, predictions[i],  test_labels)

_ = plt.xticks(range(10), class_names, rotation=45)

plt.show()


test_list= [16, 17, 22, 23, 24, 25, 39, 40, 41, 42, 48, 49, 50, 51,66]

test_list[1]
# Plot the test images from 'test_list', their predicted labels, and the true labels.

# Color correct predictions in green and incorrect predictions in red.



test_list= [16, 17, 22, 23, 24, 25, 39, 40, 41, 42, 48, 49, 50, 51,66]

num_rows = 5

num_cols = 3

num_images = num_rows * num_cols

plt.figure(figsize=(2*2*num_cols, 2*num_rows))

for i in range(num_images):

  plt.subplot(num_rows, 2*num_cols, 2*i+1)

  plot_image(test_list[i], predictions[test_list[i]], test_labels, test_images)

  plt.subplot(num_rows, 2*num_cols, 2*i+2)

  plot_value_array(test_list[i], predictions[test_list[i]], test_labels)

plt.tight_layout()

plt.show()
# Grab an image from the test dataset.

img = test_images[49]



print(img.shape)
# Add the image to a batch where it's the only member.

img = (np.expand_dims(img, 0))

print(img.shape)
predictions_single = probability_model.predict(img)

# Remember that if we do "predictions = probability_model.predict(test_images)" then we get predictions for all test data"

print(f'Probabilty for all classes: {predictions_single}, \nBest confidence score for class: {np.argmax(predictions_single)}')
i = 49

plt.figure(figsize=(12,6))

plt.subplot(1,2,1)

plot_image(i, predictions_single[0], test_labels, test_images)

plt.subplot(1,2,2)

plot_value_array(i, predictions_single[0],  test_labels)

_ = plt.xticks(range(10), class_names, rotation=45)

plt.show()
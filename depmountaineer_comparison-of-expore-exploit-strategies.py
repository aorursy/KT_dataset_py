import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

plt.style.use("seaborn-deep")

%matplotlib inline
class MultiBandit:

    """

    Multi-armed bandit.

    

    Simulate an array of slot machines, each of which when pulled returns

    a gaussian-distributed float "payout" having specified means and 

    standard deviations.

    

    Pull method returns a float payout for the machine selected.

    """

    def __init__(self, mu=0, sigma=1):

        """

        Create a multi-armed bandit Gaussian-distributed with specified mus and sigmas.



        mu: float or 1d array_like of floats

        sigma: float or 1d array_like of floats        

        """

        self._mu = np.array(mu, dtype=float).flatten()      

        self._sigma = np.array(np.array(sigma).flatten() + np.zeros_like(self._mu), dtype=float)         

    

    def pull(self, index=0):

        """

        Pull the specified arm of the bandit and return the result.

        

        index: int

        """       

        return np.array(np.random.normal(self._mu[index], self._sigma[index]))

    

    def __len__(self):

        """

        Return number of bandits.

        """

        return self._mu.size

            

    def __repr__(self):

        return str(self.__class__.__name__) + '(mu=' + repr(self._mu)  + ", sigma=" + repr(self._sigma) + ')'

    
#keep list of all strategy classes

available_strategies = []



class RandomStrategy:

    """

    Pull a random arm.  Also, base class for strategies.

    

    num_bandits: number of bandits to choose from.

    """

    def __init__(self, num_bandits=1):

        self._num_bandits = num_bandits

    

    def get_num_bandits(self):

        """

        Return number of bandits the strategy selects from.

        """

        return self._num_bandits

    

    def update(self, playfunc=None):

        """

        Select and pull a bandit, returning the index of the bandit pulled.

        

        playfunc: the pull function to call.  If None, return index withou pulling.

        """

        index = np.random.randint(0, self._num_bandits)

        if playfunc:

            playfunc(index)

        return index

    

available_strategies.append(RandomStrategy)

    

class EpsilonGreedyStrategy(RandomStrategy):

    """

    Use epsilon-greedy strategy (constant epsilon) to decide which arm to pull.

    

    num_bandits: number of bandits to choose from.

    epsilon: fraction of random pulls (versus pulling best mean payout so far)

    """

    def __init__(self, num_bandits=1, epsilon=0.1):

        RandomStrategy.__init__(self, num_bandits)

        self._epsilon = epsilon

        self._mean_estimate = np.zeros(self._num_bandits)

        self._num_estimates = np.zeros(self._num_bandits)

        

    def update(self, playfunc):

        if(np.random.random() <= self._epsilon):

            index = RandomStrategy.update(self)

        else:

            index = np.argmax(self._mean_estimate)

        result = playfunc(index)

        self._num_estimates[index] += 1

        old_mu = self._mean_estimate[index]

        n = self._num_estimates[index]

        self._mean_estimate[index] = (1-1/n) * old_mu + result/n

        return index

    

available_strategies.append(EpsilonGreedyStrategy)

        

class EpsilonGreedyInverseTStrategy(RandomStrategy):

    """

    Use adaptive epsilon-greedy strategy to decide which arm to pull.

    

    epsilon (see EpsilonGreedyStrategy) in this case is 1/t where t is the number of pulls so far.

    

    num_bandits: number of bandits to choose from.

    """

    def __init__(self, num_bandits=1):

        RandomStrategy.__init__(self, num_bandits)

        self._epsilon = 1

        self._mean_estimate = np.zeros(self._num_bandits)

        self._num_estimates = np.zeros(self._num_bandits)

        

    def update(self, playfunc):

        if(np.random.random() <= self._epsilon):

            index = RandomStrategy.update(self)

        else:

            index = np.argmax(self._mean_estimate)

        result = playfunc(index)

        self._num_estimates[index] += 1

        old_mu = self._mean_estimate[index]

        n = self._num_estimates[index]

        self._mean_estimate[index] = (1-1/n) * old_mu + result/n

        self._epsilon = 1 / self._num_estimates.sum()

        return index

        

available_strategies.append(EpsilonGreedyInverseTStrategy)       

        

class OptimisticInitialValuesStrategy(RandomStrategy):

    """

    Use optimistic inital values strategy to decide which arm to pull.

    

    epsilon (see EpsilonGreedyStrategy) in this case is 0.

    

    The estimated means will be initialized to the mean_ceiling value instead of 0.

    

    mean_ceiling: value that is larger than the means could be for the bandits.

    

    num_bandits: number of bandits to choose from.

    """

    def __init__(self, num_bandits=1, mean_ceiling=10):

        RandomStrategy.__init__(self, num_bandits)

        self._mean_estimate = np.full(self._num_bandits, float(mean_ceiling))

        self._num_estimates = np.zeros(self._num_bandits)      

    

    def update(self, playfunc):

        index = np.argmax(self._mean_estimate)

        result = playfunc(index)

        self._num_estimates[index] += 1

        old_mu = self._mean_estimate[index]

        n = self._num_estimates[index]

        self._mean_estimate[index] = (1.0-1.0/(n+1)) * old_mu + result/(n+1)

        return index



available_strategies.append(OptimisticInitialValuesStrategy)   

    

class UCB1Strategy(RandomStrategy):

    """

    Use Upper Confidence Bound strategy to decide which arm to pull.

    

    The upper confidence bound of the estimated mean 

    of each machine will be used to decide which is the best machine 

    to pull.

        

    num_bandits: number of bandits to choose from.

    """

    def __init__(self, num_bandits=1):

        RandomStrategy.__init__(self, num_bandits)

        self._mean_estimate = np.zeros(self._num_bandits)

        self._num_estimates = np.zeros(self._num_bandits)      

    

    def update(self, playfunc):

        total_n = self._num_estimates.sum()

        index = np.argmax([self._mean_estimate[j] + np.sqrt(2*np.log(total_n+1)/(self._num_estimates[j]+.01)) for j in range(len(self._mean_estimate))])

        result = playfunc(index)

        self._num_estimates[index] += 1

        old_mu = self._mean_estimate[index]

        n = self._num_estimates[index]

        self._mean_estimate[index] = (1.0-1.0/(n+1)) * old_mu + result/(n+1)

        return index

    

available_strategies.append(UCB1Strategy)    

    

class ThompsonStrategy(RandomStrategy):

    """

    Use Bayesian-Thompson strategy to decide which arm to pull.

    

    Posterior distribution modeling the bandits is used, using Bayes Theorem

    to compute it from prior times liklihood of observed data.  Prior starts

    as simple uniformdistribution.

        

    num_bandits: number of bandits to choose from.

    """

    def __init__(self, num_bandits=1):

        RandomStrategy.__init__(self, num_bandits)

        self._mean_estimate = np.zeros(self._num_bandits)

        self._num_estimates = np.zeros(self._num_bandits)      

        self._lambdas = np.ones(self._num_bandits)

        self._sum_xs = np.zeros(self._num_bandits)  

        self._taus = np.ones(self._num_bandits)

    

    def update(self, playfunc):

        index = np.argmax([np.random.randn() / np.sqrt(self._lambdas[i]) + self._mean_estimate[i] for i in range(self._num_bandits)])

        result = playfunc(index)

        self._lambdas[index] += self._taus[index]

        self._sum_xs[index] += result        

        self._num_estimates[index] += 1

        self._mean_estimate[index] = self._taus[index]*self._sum_xs[index] / self._lambdas[index]

        return index

    

available_strategies.append(ThompsonStrategy)
class BanditPlayer:

    """

    Play the sepcified bandits using the specified strategy and keep statistics.

    

    bandit: the MultiBandit instance to play

    strategy: the strategy class (inherited from RandomStrategy) to use

    strategy_kwargs: keyword arguments to put into the strategy class's constructor.

    """

    def __init__(self, bandit=MultiBandit(), strategy=EpsilonGreedyInverseTStrategy, strategy_kwargs=dict()):

        self._bandit = bandit

        self._payouts = np.zeros_like(bandit._mu, dtype=float)

        self._Ns = np.zeros_like(bandit._mu)

        self._N = 0

        self._payout = 0

        if(strategy):

            self._strategy = strategy(num_bandits=len(bandit), **strategy_kwargs)

        else:

            self._strategy = EpsilonGreedyInverseTStrategy(num_bandits=len(bandit))

        

    def get_payouts(self):

        """

        Return array of payouts, so far, for each arm of the multibandit.

        """

        return self._payouts

    

    def get_num_trials(self):

        """

        Return array of number of pulls of each arm of the multibandit.

        """

        return np.array(self._Ns, dtype=int)

    

    def get_total_payout(self):

        """

        Return total payout so far.

        """

        return self._payout

    

    def get_total_num_trials(self):

        """

        Return total number of pulls so far.

        """

        return self._N

    

    def get_payout_rates(self):

        """

        Return array of payout rates per pull (so far) for each arm of the multibandit.

        """

        return self.get_payouts()/self.get_num_trials()

    

    def get_total_payout_rate(self):

        """

        Return the total payout rate per pull so far.

        """

        return self.get_total_payout()/self.get_total_num_trials()

        

    def play_once(self, index=0):

        """

        Pull the multibandit arm and return the resulting payout.

        

        index: which arm to pull

        """

        result = self._bandit.pull(index)

        self._payouts[index] += result

        self._payout += result

        self._N += 1

        self._Ns[index] += 1

        return result

    

    def play_strategy(self, num_trials=1000):

        """

        Pull the multibandit multiple times, using the specified strategy to select the arm each time.

        

        num_trials: number of times to pull.

        """

        for trial in range(num_trials):

            self._strategy.update(self.play_once)

                

                
x = [1, 2, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000]

df = pd.DataFrame(index=x, columns=[s.__name__ for s in available_strategies])

df['num_trials'] = df.index
for strategy in available_strategies:

    for num_trials in x:

        bp = BanditPlayer(bandit=MultiBandit((-1,0,1)), strategy=strategy)

        bp.play_strategy(num_trials=num_trials)

        y = bp.get_total_payout_rate()

        df.loc[num_trials, strategy.__name__] = y

df
plt.figure(figsize=(16,12))

sns.pointplot(x="num_trials", y="vals", hue='cols', data=df.melt('num_trials', var_name='cols',  value_name='vals'))
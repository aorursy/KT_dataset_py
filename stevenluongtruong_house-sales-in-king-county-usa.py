import pandas as pd

import matplotlib.pyplot as plt

import numpy as np

import seaborn as sns

from sklearn.pipeline import Pipeline

from sklearn.preprocessing import StandardScaler,PolynomialFeatures

from sklearn.linear_model import LinearRegression

%matplotlib inline
df = pd.read_csv('../input/housesalesprediction/kc_house_data.csv')

df.head()
df.dtypes
df.describe()
#Check if there is any null value in our dataset

df.isnull().sum()
df.columns
df['age'] = df['yr_built'].max() - df['yr_built']

df.drop(df[['yr_built', 'id', 'date']], inplace = True, axis = 1)

df.head()
df.corr()
def corr_heatmap(data):

    ax = plt.subplots(figsize = (15, 10))

    sns.heatmap(df.corr(), cmap="YlGnBu")

    plt.title("Correlation between data's features", fontsize = 20)

corr_heatmap(df.corr())  
fig = plt.figure(figsize = (18,14)) # create figure



ax0 = fig.add_subplot(221) # add subplot 1 (1 row, 2 columns, first plot) # add_subplot(1, 2, 1)

ax1 = fig.add_subplot(222) # add subplot 2 (1 row, 2 columns, second plot). See tip below** # add_subplot(1, 2, 2)

ax2 = fig.add_subplot(223)

ax3 = fig.add_subplot(224)



# Subplot 1:

sns.boxplot(x='waterfront',y='price', data=df, ax=ax0)

ax0.set_title('Correlation between waterfront and price')

ax0.set_xlabel('Waterfront')

ax0.set_ylabel('Price')



# Subplot 2:

sns.boxplot(x='bedrooms',y='price',data=df, ax=ax1)

ax1.set_title('Correlation between number of bedrooms and price')

ax1.set_xlabel('Bedrooms')

ax1.set_ylabel('Price')



#Subplot 3:

sns.boxplot(x='grade',y='price',data=df, ax=ax2)

ax2.set_title('Correlation between grade and price')

ax2.set_xlabel('Grade')

ax2.set_ylabel('Price')



#Subplot 4:

sns.boxplot(x='bathrooms',y='price',data=df, ax=ax3)

ax3.set_title('Correlation between bathrooms and price')

ax3.set_xlabel('Bathrooms')

ax3.set_ylabel('Price')





plt.show()
fig = plt.figure(figsize = (18,14)) # create figure



ax0 = fig.add_subplot(221) # add subplot 1 (1 row, 2 columns, first plot) # add_subplot(1, 2, 1)

ax1 = fig.add_subplot(222) # add subplot 2 (1 row, 2 columns, second plot). See tip below** # add_subplot(1, 2, 2)

ax2 = fig.add_subplot(223)

ax3 = fig.add_subplot(224)



# Subplot 1 (regression plot) : 

sns.regplot(x='sqft_living',y='price', data=df, ax=ax0)

ax0.set_title('Correlation between sqft living and price')

ax0.set_xlabel('sqft living')

ax0.set_ylabel('Price')



# Subplot 2 (Scatter plot) :

sns.scatterplot(x='sqft_lot',y='price',data=df, ax=ax1)

ax1.set_title('Correlation between sqft lot and price')

ax1.set_xlabel('sqft lot')

ax1.set_ylabel('Price')



# Subplot 3 (Scatter plot) :

sns.scatterplot(x='age',y='price',data=df, ax=ax2)

ax2.set_title('Correlation between age and price')

ax2.set_xlabel('Age')

ax2.set_ylabel('Price')



# Subplot 4 (Regression plot) :

sns.regplot(x='sqft_above',y='price',data=df, ax=ax3)

ax3.set_title('Correlation between sqft above and price')

ax3.set_xlabel('Sqft above')

ax3.set_ylabel('Price')





plt.show()
# Create a new dataframe df_temp with no feature 'long', 'zipcode', 'age', 'lat' : 

df_temp = df.drop(df[['long','lat', 'zipcode', 'age']], axis = 1)



# Sorting the data and reset the index

df_corr = df_temp.corr().iloc[0,:].sort_values(ascending=False).reset_index()



df_corr = df_corr[df_corr["price"] < 1]  # Eliminate the 'price' in the x-axis



plt.figure(figsize=(15,9))

sns.barplot(x = "index", y = "price", data = df_corr);

plt.xlabel("Other features")

plt.xticks(rotation = 90)

plt.title("Correlation with price");

plt.grid(True)

sns.set_style("dark")
floor_df = df.floors.value_counts()

floor_df.rename_axis('floor value', inplace = True)

floor_df_t = floor_df.transpose()

floor_df_t




colors_list = ['gold', 'yellowgreen', 'lightcoral', 'lightskyblue', 'lightgreen', 'pink']

explode_list = [0.12, 0, 0, 0, 0.05, 0.15] # ratio for each continent with which to offset each wedge.



floor_df.plot(kind='pie',

                            figsize=(15, 6),

                            autopct='%1.1f%%', 

                            startangle=90,

                              

                            shadow=True,       

                            labels=None,         # turn off labels on pie chart

                            pctdistance=1.15,    # the ratio between the center of each pie slice and the start of the text generated by autopct 

                            colors=colors_list,  # add custom colors

                            explode=explode_list # 'explode' lowest 3 floor values

                            )



# scale the title up by 12% to match pctdistance

plt.title('The distribution of floor values', y=1.12) 



plt.axis('equal') 



# add legend

plt.legend(labels=floor_df.index, loc='upper left') 



plt.show()
# Now we import folium for the tasks

import folium

from folium.plugins import FastMarkerCluster
df_geo = df[["zipcode" , "lat" , "long"]]



lat = df_geo["lat"]

long = df_geo["long"]



cordinates = list(zip(lat,long))
king_map = folium.Map(location = [47.5480, -122.257], zoom_start = 10)



FastMarkerCluster(data=cordinates).add_to(king_map)



king_map
Y = df['price']



lm1 = LinearRegression()

X1 = df[['sqft_living']]

lm1.fit(X1,Y)

print('The R^2 score between \'sqft_living\' and \'price\' is ' + str(lm1.score(X1,Y)))



lm2 = LinearRegression()

X2 = df[['bathrooms']]

lm2.fit(X2,Y)

print('The R^2 score between \'bathrooms\' and \'price\' is ' + str(lm2.score(X2,Y)))



lm3 = LinearRegression()

X3 = df[['grade']]

lm3.fit(X3,Y)

print('The R^2 score between \'grade\' and \'price\' is ' + str(lm3.score(X2,Y)))
width = 10

height = 8

plt.figure(figsize=(width, height))





ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")

sns.distplot(lm1.predict(X1), hist=False, color="b", label="Fitted Values" , ax=ax1)



#We can adjust histogram True or False if we want to include the histogram along with the distribution lines



plt.title('Actual vs Fitted Values for Price based on sqft_living')

plt.xlabel('Price (in dollars)')

plt.ylabel('Proportion of houses')



plt.show()

plt.close()
width = 10

height = 8

plt.figure(figsize=(width, height))





ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")

sns.distplot(lm2.predict(X2), hist=False, color="b", label="Fitted Values" , ax=ax1)



#We can adjust histogram True or False if we want to include the histogram along with the distribution lines



plt.title('Actual vs Fitted Values for Price based on bathrooms')

plt.xlabel('Price (in dollars)')

plt.ylabel('Proportion of houses')



plt.show()

plt.close()
width = 10

height = 8

plt.figure(figsize=(width, height))





ax1 = sns.distplot(df['price'], hist=False, color="r", label="Actual Value")

sns.distplot(lm3.predict(X3), hist=False, color="b", label="Fitted Values" , ax=ax1)



#We can adjust histogram True or False if we want to include the histogram along with the distribution lines



plt.title('Actual vs Fitted Values for Price based on grade')

plt.xlabel('Price (in dollars)')

plt.ylabel('Proportion of houses')



plt.show()

plt.close()
features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]
lm4 = LinearRegression()

X4 = df[features]

X4.head()
lm4.fit(X4,Y)

print('The R^2 score between these features and \'price\' is ' + str(lm4.score(X4,Y)))

Yhat_mult = lm4.predict(df[features])

Yhat_mult
Input = [('scale', StandardScaler()), ('polynomial', PolynomialFeatures(include_bias=False)), ('model', LinearRegression())]
pipe=Pipeline(Input)

pipe
new_features = df[features]

new_features

pipe.fit(new_features,Y)
ypipe=pipe.predict(new_features)

ypipe[0:4]
from sklearn.model_selection import cross_val_score

from sklearn.model_selection import train_test_split

print("done")
features =["floors", "waterfront","lat" ,"bedrooms" ,"sqft_basement" ,"view" ,"bathrooms","sqft_living15","sqft_above","grade","sqft_living"]    

X = df[features]

Y = df['price']



x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)





print("number of test samples:", x_test.shape[0])

print("number of training samples:",x_train.shape[0])
from sklearn.linear_model import Ridge
RidgeModel = Ridge(alpha = 0.1)

RidgeModel.fit(x_train,y_train)

RidgeModel.score(x_test,y_test)
pr = PolynomialFeatures(degree=2)

x_train_pr = pr.fit_transform(x_train[features])

x_test_pr = pr.fit_transform(x_test[features])



RidgeModel.fit(x_train_pr,y_train)

RidgeModel.score(x_test_pr,y_test)
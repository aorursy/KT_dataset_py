# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the read-only "../input/" directory

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory



import os

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))



# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 

# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import seaborn as sns

import matplotlib.pyplot as plt

from sklearn import metrics

from sklearn.svm import SVC

from sklearn.tree import DecisionTreeClassifier

sns.set(style="whitegrid")
data='/kaggle/input/income-classification/income_evaluation.csv'

df=pd.read_csv(data)
#print the shape

print('The shape of the dataset : ' , df.shape)
df.head()
col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',

             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']



df.columns = col_names



df.columns
df.dtypes
df.describe()
#check for missing value



df.isnull().sum()
categorical = [var for var in df.columns if df[var].dtype=='O']



print('There are {} categorical variables\n'.format(len(categorical)))



print('The categorical variables are :\n\n', categorical)
df[categorical].head()
for var in categorical: 

    

    print(df[var].value_counts())
# check for missing values



df['income'].isnull().sum()
# view number of unique values



df['income'].nunique()
# view the frequency distribution of values



df['income'].value_counts()
# view percentage of frequency distribution of values



df['income'].value_counts()/len(df)
# visualize frequency distribution of income variable



f,ax=plt.subplots(1,2,figsize=(18,8))



ax[0] = df['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)

ax[0].set_title('Income Share')





#f, ax = plt.subplots(figsize=(6, 8))

ax[1] = sns.countplot(x="income", data=df, palette="Set1")

ax[1].set_title("Frequency distribution of income variable")



plt.show()
# check number of unique labels 



df.workclass.nunique()
# view frequency distribution of values



df.workclass.value_counts()
# replace '?' values in workclass variable with `NaN`



df['workclass'].replace(' ?', np.NaN, inplace=True)
# again check the frequency distribution of values in workclass variable



df.workclass.value_counts()
f, ax = plt.subplots(figsize=(10, 6))

ax = df.workclass.value_counts().plot(kind="bar", color="green")

ax.set_title("Frequency distribution of workclass variable")

ax.set_xticklabels(df.workclass.value_counts().index, rotation=30)

plt.show()
# check number of unique labels



df.occupation.nunique()
# view unique labels



df.occupation.unique()
# view frequency distribution of values



df.occupation.value_counts()
# replace '?' values in occupation variable with `NaN`



df['occupation'].replace(' ?', np.NaN, inplace=True)
# again check the frequency distribution of values



df.occupation.value_counts()
# check number of unique labels



df.native_country.nunique()
# view unique labels 



df.native_country.unique()
# check frequency distribution of values



df.native_country.value_counts()
# replace '?' values in native_country variable with `NaN`



df['native_country'].replace(' ?', np.NaN, inplace=True)

# again check the frequency distribution of values



df.native_country.value_counts()
df[categorical].isnull().sum()
numerical = [var for var in df.columns if df[var].dtype!='O']



print('There are {} numerical variables\n'.format(len(numerical)))



print('The numerical variables are :\n\n', numerical)
df[numerical].head()
df[numerical].isnull().sum()
df['age'].nunique()
f, ax = plt.subplots(figsize=(10,8))

x = df['age']

ax = sns.distplot(x, bins=10, color='blue')

ax.set_title("Distribution of age variable")

plt.show()
# plot correlation heatmap to find out correlations



df.corr().style.format("{:.4}").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)
X=df.drop(['income'],axis=1)



y=df['income']
from sklearn.model_selection import train_test_split



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)
#check the shape of X_train and X_test



X_train.shape, X_test.shape
categorical = [col for col in X_train.columns if X_train[col].dtypes == 'O']



categorical
numerical = [col for col in X_train.columns if X_train[col].dtypes != 'O']



numerical
# print percentage of missing values in the categorical variables in training set



X_train[categorical].isnull().mean()
# print categorical variables with missing data



for col in categorical:

    if X_train[col].isnull().mean()>0:

        print(col, (X_train[col].isnull().mean()))
for df2 in [X_train, X_test]:

    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)

    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)

    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)
# check missing values in categorical variables in X_train



X_train[categorical].isnull().sum()
# check missing values in categorical variables in X_test



X_test[categorical].isnull().sum()
# check missing values in X_train



X_train.isnull().sum()
# check missing values in X_test



X_test.isnull().sum()
# preview categorical variables in X_train



X_train[categorical].head()
# import category encoders



import category_encoders as ce
# encode categorical variables with one-hot encoding



encoder = ce.OneHotEncoder(cols=['workclass', 'education', 'marital_status', 'occupation', 'relationship', 

                                 'race', 'sex', 'native_country'])



X_train = encoder.fit_transform(X_train)



X_test = encoder.transform(X_test)
X_train.head()
X_train.shape
X_test.head()
X_test.shape
cols = X_train.columns

from sklearn.preprocessing import RobustScaler



scaler = RobustScaler()



X_train = scaler.fit_transform(X_train)



X_test = scaler.transform(X_test)

X_train = pd.DataFrame(X_train, columns=[cols])

X_test = pd.DataFrame(X_test, columns=[cols])
# import Random Forest classifier



from sklearn.ensemble import RandomForestClassifier





# instantiate the classifier 



rfc = RandomForestClassifier(random_state=0)







# fit the model



rfc.fit(X_train, y_train)







# Predict the Test set results



y_pred = rfc.predict(X_test)



#Check accuracy score





from sklearn.metrics import accuracy_score



print('Model accuracy score with Random Forest (with 10 decision tree ) : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))



# instantiate the classifier with n_estimators = 100



rfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)







# fit the model to the training set



rfc_100.fit(X_train, y_train)







# Predict on the test set results



y_pred_100 = rfc_100.predict(X_test)







# Check accuracy score 



print('Model accuracy score with Random Forest (with 100 decision tree ) : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))

dt = DecisionTreeClassifier()

dt.fit(X_train,y_train)

y_predict_dt = dt.predict(X_test)

acc_dt = metrics.accuracy_score(y_predict_dt,y_test)

print('The accuracy of the Decision Tree is', acc_dt)
sv = SVC() #select the algorithm

sv.fit(X_train,y_train) # we train the algorithm with the training data and the training output

y_predict_svm = sv.predict(X_test) #now we pass the testing data to the trained algorithm

acc_svm = metrics.accuracy_score(y_predict_svm,y_test)

print('The accuracy of the SVM is:', acc_svm)
# Print the Confusion Matrix and slice it into four pieces



from sklearn.metrics import confusion_matrix



cm = confusion_matrix(y_test, y_pred)



print('Confusion matrix\n\n', cm)
# visualize confusion matrix with seaborn heatmap



cm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], 

                                 index=['Predict Positive:1', 'Predict Negative:0'])



sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')

from sklearn.metrics import classification_report



print(classification_report(y_test, y_pred))
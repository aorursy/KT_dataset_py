import numpy as np

import pandas as pd

import plotly.graph_objs as go

import plotly.figure_factory as ff

from plotly.subplots import make_subplots

from IPython.display import Image

# read dataset

poke = pd.read_csv('/kaggle/input/pokemon/pokemon_alopez247.csv')
# first five

poke.head()
# last five

poke.tail()
poke.info()
poke.describe()
# verify the missing data and quantify

missing = pd.DataFrame({'qtd_NaN_data':poke.isna().sum(), 

                        'perc_NaN_data':round((poke.isna().sum()*100/poke.shape[0]), 2)})

missing
# Data when 'Pr_Male' is NaN

poke[poke['Pr_Male'].isna()].head(5)
# verify if all data with NaN in Pr_Male has field hasGender equals False

print('Without Pr Male:{} - Has gender False:{}'.format(

    poke['Number'][poke['Pr_Male'].isna()].count(),

    poke['Number'][poke['hasGender'] == False].count()))



# compare quantities of isLegendary pokemons with isLegendary and hasGender equals False

print('Is Legendary :{} - Has gender False:{}'.format(

    poke['Number'][poke['isLegendary'] == True].count(),

    poke['Number'][(poke['hasGender'] == False) & (poke['isLegendary'] == True)].count()))
# list of legendery with gender

print('**   Legendary pokemons with gender  **')

print()

poke[(poke['hasGender'] == True) & (poke['isLegendary'] == True)]
# list of categorical features

cat_feat = ['Type_1', 'Type_2', 'Color', 'isLegendary', 

              'hasGender', 'Egg_Group_1', 'Egg_Group_2', 

              'hasMegaEvolution', 'Body_Style']



# instantiate a subplot fig

fig = make_subplots(rows=5,cols=2,

                    vertical_spacing=0.09,

                    horizontal_spacing=0.075,

                    subplot_titles=cat_feat)



# add each trace generated by each feature

for enu, c in enumerate(cat_feat):

    # total values

    status_values = poke[c].value_counts().to_frame()

    # plot distribuition

    trace = go.Bar(x=status_values.index, 

                   y=status_values[c], 

                   text=status_values[c], 

                   textposition='auto',

                   name=c

                  )

    

    # calculate position

    row=int(np.ceil((enu+1)/2))

    col=(enu % 2)+1

    

    # add trace

    fig.append_trace(trace, row=row, col=col)

    fig.update_xaxes(tickangle = 45)

    fig.update_yaxes(showgrid = False,showticklabels = False)



# update layout

fig.update_layout(title_text='Distribuition of Categorical Data', 

                  height=1200, width=1100)



# to be viewed on github

#img = fig.to_image(format='jpg')

#Image(img)

# to normal plot

fig.show()
# list of numerical features

num_feat = ['Total','HP','Attack','Defense','Sp_Atk','Sp_Def',

'Speed','Generation','Pr_Male','Height_m','Weight_kg','Catch_Rate']



# create a subplot

sub_fig = make_subplots(rows=int(np.ceil((len(num_feat))/3)),

                        cols=3, 

                        vertical_spacing=0.09,

                        horizontal_spacing=0.075,

                        subplot_titles=num_feat)

sub_fig.update_layout(title='Distribuition of Numerical Data', height=800, width=800)



# for each feature

for enu, i in enumerate(num_feat):

    

    # calculate position

    row=int(np.ceil((enu+1)/3))

    col=(enu % 3)+1

    

    # just a custom bin size

    if i == 'Pr_Male' or i == 'Height_m' or i == 'Generation':

        fig = ff.create_distplot(hist_data=[poke[i].dropna()], 

                                 group_labels=[i],

                                 bin_size=[.2], colors=[enu])

    else:

        fig = ff.create_distplot(hist_data=[poke[i].dropna()], 

                                 group_labels=[i],

                                 bin_size=[10], colors=[enu])

    # add each data at subplot

    for mydata in fig['data']:

        sub_fig.append_trace(mydata, row, col)

        





# to be viewed on github

#img_bytes = sub_fig.to_image(format="jpeg")

#Image(img_bytes)



# to normal plot

sub_fig.show()
# convert some categorical feature to new columns

poke_dm = pd.get_dummies(poke, columns=['Type_1', 'Type_2', 'Color'])

poke_dm.head()
# group Type_1 and Type_2

for t in poke.groupby(['Type_1']).groups.keys():

    poke_dm['Type_{}'.format(t)] = poke_dm.loc[:,

                                        poke_dm.columns.str.endswith(t)].sum(axis=1)

    del poke_dm['Type_1_{}'.format(t)]

    del poke_dm['Type_2_{}'.format(t)]

    

poke_dm.info()
# calculate correlation with pearson method

correlations = poke_dm.corr(method='pearson')

correlations
# Print the heatmap of this correlation

data = [go.Heatmap(x=correlations.columns,

                   y=correlations.index,

                   z=correlations, colorscale='RdBu')]

layout = go.Layout(title='Correlations',

                   yaxis={'autorange':"reversed"})



fig = go.Figure(data=data,layout=layout)

fig.update_layout(autosize=False, height=1000, width=1000)



# to be viewed on github

# img_bytes = fig.to_image(format="jpeg")

# Image(img_bytes)



# to normal plot

fig.show()
# list of features to print

d_list = ['Pr_Male', 'Catch_Rate', 'Height_m', 'Weight_kg', 'Total']

dimensions = []



# add dimensions

for d in d_list:

    dimensions.append(dict(label=d, values=poke[d]))



# create splom graph    

data = go.Splom(dimensions=dimensions, 

                showupperhalf=True,

                marker = dict(size=5, showscale=False,))



fig = go.Figure(data=data)

fig.update_layout(height=900, width= 900, title='Compare numerical features')

#Image(fig.to_image('jpg'))

fig.show()
# list of features to print

d_list = ['Pr_Male', 'Catch_Rate', 'Type_1', 'Weight_kg']#['Catch_Rate', 'HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def', 'Speed']

dimensions = []



# add dimensions

for d in d_list:

    dimensions.append(dict(label=d, values=poke[d], visible=True))

# create splom graph 

data = go.Splom(dimensions=dimensions, marker = dict(size=5, showscale=False,))



fig = go.Figure(data=data)

fig.update_layout(height=800, width= 800, title='Compare numerical features')

#Image(fig.to_image('jpg'))

fig.show()
# group by two fields

f1 = 'Type_1'

f2 = 'Color'

test = poke.groupby([f1,f2]).Color.count().to_frame()

test = test.rename(columns={'Color':'ct'})



# reset indexs

test.reset_index(inplace=True)



# row to col

test = test.pivot(index=f2, columns=f1)['ct']

test = test.fillna(0)



# group by two fields

f1 = 'Type_2'

f2 = 'Color'

test2 = poke.groupby([f1,f2]).Color.count().to_frame()

test2 = test2.rename(columns={'Color':'ct'})

# reset indexs

test2.reset_index(inplace=True)

# row to col

test2 = test2.pivot(index=f2, columns=f1)['ct']

test2 = test2.fillna(0)



# sum total of type_1 by color and type_2 by color

test = test+test2



# percent value of each color

for c in test.columns:

    test[c] = test[c]/test[c].sum()

test.head(2)
# print color bar by pokemon type

data = []



for field in test.index:

    y = test.columns

    x = test[test.index == field].values[0]

    # create bard

    bar = go.Bar(x=x,

                  y=y,

                  name=field,

                  orientation='h',

                  marker = dict(color=field)

                 )



    data.append(bar)



layout = go.Layout(title = 'Pokemon Color by Type', 

                   barmode='stack', 

                   yaxis=dict(title='Pokemon Type'), xaxis=dict(title='Percent'))

fig = go.Figure(data=data,layout=layout)



fig.update_layout(height=700, width=800)

#Image(fig.to_image('jpg'))

fig.show()
numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']



poke_dm.select_dtypes(include=numerics).columns
# let's print a dendogram of our data

import scipy.cluster.hierarchy as shc

from sklearn.cluster import AgglomerativeClustering



# list of features

num_feat = ['Total', 'HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def',

       'Speed', 'Height_m', 'Weight_kg', 'Catch_Rate',

       'Type_Bug', 'Type_Dark', 'Type_Dragon', 'Type_Electric', 'Type_Fairy',

       'Type_Fighting', 'Type_Fire', 'Type_Flying', 'Type_Ghost', 'Type_Grass',

       'Type_Ground', 'Type_Ice', 'Type_Normal', 'Type_Poison', 'Type_Psychic',

       'Type_Rock', 'Type_Steel', 'Type_Water']



# calculate the clusters with euclidean distance

cluster = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')



# create dendogram with custom linkagefun

fig = ff.create_dendrogram(

    poke_dm[num_feat], orientation='bottom',

    linkagefun=lambda x: shc.linkage(poke_dm[num_feat], 'ward', metric='euclidean'), 

    color_threshold=2000

)

fig.update_layout(width=800, height=500)

fig.show()

# Image(fig.to_image('jpg'))
cl = cluster.fit_predict(poke_dm[num_feat])

# add cluster in dataframe

poke_dm['Cluster'] = cl



# totalizate each group

zero, um, dois, tres = [], [], [], []

for ii, i in enumerate(cl):

    if i==0:

        zero.append(poke[poke.index == ii].Name.values[0])

    if i==1:

        um.append(poke[poke.index == ii].Name.values[0])

    if i==2:

        dois.append(poke[poke.index == ii].Name.values[0])

    if i==3:

        tres.append(poke[poke.index == ii].Name.values[0])

       

print('0:{}, 1:{}, 2:{}, 3:{}'.format(len(zero), len(um), len(dois), len(tres)))

# print the smallest group 

print(zero)
# Check the behavior of groups with respect to field combinations

comb = [['Total','Catch_Rate'],

        ['Attack','Catch_Rate'],

        ['Defense','Catch_Rate'],

        ['Weight_kg','Height_m'],

        ['Attack','Defense'],

        ['Speed','HP']]



# create a figure

fig = make_subplots(rows=3,cols=2,

                    vertical_spacing=0.09,

                    horizontal_spacing=0.075,

                    subplot_titles= ['{} x {}'.format(i[0], i[1]) for i in comb]

                   )



# each combination with a subplot

for enu, fields in enumerate(comb):         

    value_x = poke_dm[fields[0]]

    value_y = poke_dm[fields[1]]

    

    row=int(np.ceil((enu+1)/2))

    col=(enu % 2)+1

    

    trace = go.Scatter(x=value_x,

                       y=value_y, 

                       mode='markers',

                       marker=dict(color=poke_dm['Cluster'], colorscale=["red", "green", "blue"])                       

                       )

    fig.append_trace(trace, row=row, col=col)

    

fig.update_layout(title='Comparison of some features with Clusters', showlegend=False, width=800, height=800)

fig.show()

#Image(fig.to_image('jpg'))
from sklearn.decomposition import PCA as skPCA



# create two components PCA

s_pca = skPCA(n_components=2)

pca_components = s_pca.fit_transform(poke_dm[num_feat])

c_pca1 = pca_components[:,0]

c_pca2 = pca_components[:,1]

# add the componets to dataframe

poke_dm['pca1'] = c_pca1

poke_dm['pca2'] = c_pca2

poke_dm.head(2)
# Plot PCA with the previously groups

poke_dm_temp = poke_dm

trace = go.Scatter(x=poke_dm_temp['pca1'],

                   y=poke_dm_temp['pca2'],

                   mode='markers',

                   text=poke_dm_temp['Name'],

                   marker=dict(color=poke_dm_temp['Cluster'], colorscale=["red", "green", "blue"]))



fig = go.Figure(data=[trace])

fig.update_layout(title='PCA and clusters of all generations',width=800, height=500)

fig.show()

#Image(fig.to_image('jpg'))
# Plot PCA with the previously groups but only first generation with pokemons names 

poke_dm_temp = poke_dm[(poke_dm['Generation'] == 1)|(poke_dm['Generation'] == 1)]

trace = go.Scatter(x=poke_dm_temp['pca1'],

                   y=poke_dm_temp['pca2'],

                   mode='markers+text',

                   textposition="bottom center",

                   text=poke_dm_temp['Name'],

                   marker=dict(color=poke_dm_temp['Cluster'], colorscale=["red", "green", "blue"]))



fig = go.Figure(data=[trace])

fig.update_layout(title='PCA and clusters of first generations',width=800, height=500)

fig.show()

#Image(fig.to_image('jpg'))
# Plot PCA with print legendary and no legendary pokemons

poke_dm_temp = poke_dm

# no legendary

trace1 = go.Scatter(x=poke_dm_temp[poke_dm_temp['isLegendary'] == False]['pca1'],

                   y=poke_dm_temp[poke_dm_temp['isLegendary'] == False]['pca2'],

                   mode='markers',

                   text=poke_dm_temp[poke_dm_temp['isLegendary'] == False]['Name'],

                   name='Commun'

                  )

# legendary

trace2 = go.Scatter(x=poke_dm_temp[poke_dm_temp['isLegendary'] == True]['pca1'],

                   y=poke_dm_temp[poke_dm_temp['isLegendary'] == True]['pca2'],

                   mode='markers',

                   text=poke_dm_temp[poke_dm_temp['isLegendary'] == True]['Name'],

                   name='Legendary'

                  )



fig = go.Figure(data=[trace1, trace2])

fig.update_layout(title='PCA and is Legendary pokemons',width=800, height=500, showlegend=True)

fig.show()

#Image(fig.to_image('jpg'))
# function to convert categorical string to categorical int to see if there is any kernel per type pokemon

def cat_to_number(n):

    types = {'Bug': 0,  'Dark': 1,  'Dragon': 2,  'Electric': 3,  'Fairy': 4, 'Fighting': 5,

             'Fire': 6,  'Flying': 7,  'Ghost': 8,  'Grass': 9, 'Ground': 10, 'Ice': 11, 'Normal': 12,

             'Poison': 13,  'Psychic': 14, 'Rock': 15, 'Steel': 16, 'Water': 17}

    return types[n]



# Plot PCA

poke_dm_temp = poke_dm

trace = go.Scatter(x=poke_dm_temp['pca1'],

                   y=poke_dm_temp['pca2'],

                   mode='markers',

                   textposition="bottom center",

                   text=poke_dm_temp['Name'],

                   marker=dict(color=list(map(cat_to_number, poke['Type_1'])) ))



fig = go.Figure(data=[trace])

fig.update_layout(title='PCA by Type_1',width=800, height=500, showlegend=False)

fig.show()

#Image(fig.to_image('jpg'))
from sklearn.model_selection import cross_val_score # Cross validation function

from sklearn.metrics import confusion_matrix # To generate n confusion matrix

from sklearn.naive_bayes import GaussianNB # Naive Bayes with Gaussian

from sklearn.neighbors import KNeighborsClassifier # KNN model

from sklearn.svm import SVC # SVM categorical

from sklearn.gaussian_process import GaussianProcessClassifier # Gaussian process classification based on Laplace approximation

from sklearn.gaussian_process.kernels import RBF # Parameter of GPC

from sklearn.tree import DecisionTreeClassifier # Decision Tree to Classifier

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier # Random Forest and Adaboost



# features used in models

x_features = ['HP', 'Attack', 'Defense', 'Sp_Atk', 'Sp_Def',

            'Speed', 'Height_m', 'Weight_kg', 'Catch_Rate',

            'Type_Bug', 'Type_Dark', 'Type_Dragon', 'Type_Electric', 'Type_Fairy',

            'Type_Fighting', 'Type_Fire', 'Type_Flying', 'Type_Ghost', 'Type_Grass',

            'Type_Ground', 'Type_Ice', 'Type_Normal', 'Type_Poison', 'Type_Psychic',

            'Type_Rock', 'Type_Steel', 'Type_Water']

# feature target

y_features = ['isLegendary']



# separete dataset and targer

data_set = poke_dm[x_features]

target = poke_dm[y_features].values.ravel()



# list of models

models = [GaussianNB(),

          KNeighborsClassifier(3), 

          SVC(kernel="linear", C=0.025),

          SVC(gamma=10, C=2),

          GaussianProcessClassifier(1.0 * RBF(1.0)),

          AdaBoostClassifier(),

          DecisionTreeClassifier(max_depth=5),          

          RandomForestClassifier(max_depth=10, n_estimators=5, max_features=1),

         ]



# name models

models_name = ["Naive Bayes", "Nearest Neighbors", "Linear SVM", "RBF SVM", 

               "Gaussian Process","AdaBoost", "Decision Tree", "Random Forest"]
total_scores = {}

for model, model_name in zip(models, models_name):

    np.random.seed = 42

    # K-fold k=5

    scores = cross_val_score(model, data_set, target, cv=10, scoring='accuracy')

    total_scores[model_name] = [scores, scores.mean(), scores.std()]

    # Accuracy

    print("{} -- K-Fold mean accuracy: {:0.3f} (std: {:0.3f})".format(model_name, scores.mean(), scores.std()))

    # Verify prediction of all data

    y_pred = model.fit(data_set,target).predict(data_set)

    

    # Confusion Matrix

    z = confusion_matrix(target, y_pred)

    x=['No Legendary', 'Is Legendary']

    y=['No Legendary', 'Is Legendary']

    

    # Generate annotations to graph

    annotations = []

    for n, row in enumerate(z):

        for m, val in enumerate(row):

            annotations.append(go.layout.Annotation(text=str(z[n][m]), x=x[m], y=y[n],

                                             xref='x1', yref='y1', showarrow=False))

            

    data = [go.Heatmap(x=x,y=y,z=z,                   

                   colorscale=["white", "lightblue"])] #amp blues peach



    layout = go.Layout(title='Confusion Matrix - {}'.format(model_name), 

                       xaxis={'title' : 'Predicted label'},

                       yaxis={'autorange' : 'reversed', 

                              'title' : 'True Label'})

    

    fig = go.Figure(data=data, layout=layout)

    fig['layout'].update(annotations=annotations, height=350, width=350)



    fig.show()

    #display(Image(fig.to_image('jpg')))

    print('----------------------------------------------------------------------------')
# boxplot of 10 folds of each model

# Gaussian Process has better result and RBF SVM has a more stable model

fig = go.Figure()

for enu, (model, (scores, v_mean, v_std)) in enumerate(total_scores.items()):

    fig.add_trace(go.Box(

        y=scores, 

        name=model,  

        boxmean='sd'))

fig.update_layout(title = 'Cross Validation Accuracy Comparacion')

fig.update_yaxes(title = 'Accuracy')

fig.update_xaxes(title = 'Models')

fig.show()

#display(Image(fig.to_image('jpg')))
gpc = GaussianProcessClassifier(1.0 * RBF(1.0))

y_pred = gpc.fit(data_set,target).predict(data_set)



# wrong classification

poke_dm[poke_dm['isLegendary'] != y_pred][['Number', 'Name', 'Total', 'isLegendary']]
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import string # library used to deal with some text data
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import seaborn as sns # data visualization library
pd.set_option('display.max_columns', 100) # Setting pandas to display a N number of columns
pd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows
pd.set_option('display.width', 1000) # Setting pandas dataframe display width to N
from scipy import stats # statistical library
from statsmodels.stats.weightstats import ztest # statistical library for hypothesis testing
import plotly.graph_objs as go # interactive plotting library
import plotly.express as px # interactive plotting library
from itertools import cycle # used for cycling colors at plotly graphs
import matplotlib.pyplot as plt # plotting library
import pandas_profiling # library for automatic EDA
%pip install autoviz # installing and importing autoviz, another library for automatic data visualization
from autoviz.AutoViz_Class import AutoViz_Class
from IPython.display import display # display from IPython.display
from itertools import cycle # function used for cycling over values
%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables
import ppscore as pps # importing ppscore

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
print("")
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
# Importing the data and displaying some rows
df = pd.read_csv("/kaggle/input/titanic/train.csv")

display(df.head(10))
# The pandas profiling library is really useful on helping us understand the data we're working on.
# It saves us some precious time on the EDA process.
report = pandas_profiling.ProfileReport(df)
# Let's now visualize the report generated by pandas_profiling.
display(report)

# Also, there is an option to generate an .HTML file containing all the information generated by the report.
# report.to_file(output_file='report.html')
# Another great library for automatic EDA is AutoViz.
# With this library, several plots are generated with only 1 line of code.
# When combined with pandas_profiling, we obtain lots of information in a
# matter of seconds, using less than 5 lines of code.
AV = AutoViz_Class()

# Let's now visualize the plots generated by AutoViz.
report_2 = AV.AutoViz("/kaggle/input/titanic/train.csv")
# Creating different datasets for survivors and non-survivors
df_survivors = df[df['Survived'] == 1]
df_nonsurvivors = df[df['Survived'] == 0]
# Filling in the data inside the Violin Objects
violin_survivors = go.Violin(
    y=df_survivors['Age'],
    x=df_survivors['Survived'],
    name='Survivors',
    marker_color='forestgreen',
    box_visible=True)

violin_nonsurvivors = go.Violin(
    y=df_nonsurvivors['Age'],
    x=df_nonsurvivors['Survived'],
    name='Non-Survivors',
    marker_color='darkred',
    box_visible=True)

data = [violin_nonsurvivors, violin_survivors]


# Plot's Layout (background color, title, etc.)
layout = go.Layout(
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)',
    title='"Age" of survivors vs Ages of non-survivors',
  xaxis=dict(
        title='Survived or not'
    ),
    yaxis=dict(
        title='Age'
    )
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# First distribution for the hypothesis test: Ages of survivors
dist_a = df_survivors['Age'].dropna()

# Second distribution for the hypothesis test: Ages of non-survivors
dist_b = df_nonsurvivors['Age'].dropna()
# Z-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different
t_stat, p_value = ztest(dist_a, dist_b)
print("----- Z Test Results -----")
print("T stat. = " + str(t_stat))
print("P value = " + str(p_value)) # P-value is less than 0.05

print("")

# T-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different
t_stat_2, p_value_2 = stats.ttest_ind(dist_a, dist_b)
print("----- T Test Results -----")
print("T stat. = " + str(t_stat_2))
print("P value = " + str(p_value_2)) # P-value is less than 0.05
# Taking the count of each Sex value inside the Survivors
df_survivors_sex = df_survivors['Sex'].value_counts()
df_survivors_sex = pd.DataFrame({'Sex':df_survivors_sex.index, 'count':df_survivors_sex.values})

# Taking the count of each Sex value inside the Survivors
df_nonsurvivors_sex = df_nonsurvivors['Sex'].value_counts()
df_nonsurvivors_sex = pd.DataFrame({'Sex':df_nonsurvivors_sex.index, 'count':df_nonsurvivors_sex.values})


# Creating the plotting objects
pie_survivors_sex = go.Pie(  
   labels = df_survivors_sex['Sex'],
   values = df_survivors_sex['count'],
   domain=dict(x=[0, 0.5]),
   name='Survivors',
   hole = 0.5,
   marker = dict(colors=['violet', 'cornflowerblue'], line=dict(color='#000000', width=2))
)

pie_nonsurvivors_sex = go.Pie(  
   labels = df_nonsurvivors_sex['Sex'],
   values = df_nonsurvivors_sex['count'],
   domain=dict(x=[0.5, 1.0]), 
   name='non-Survivors',
   hole = 0.5,
   marker = dict(colors=['cornflowerblue', 'violet'], line=dict(color='#000000', width=2))
)

data = [pie_survivors_sex, pie_nonsurvivors_sex]


# Plot's Layout (background color, title, annotations, etc.)
layout = go.Layout(
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)',
    title='"Sex" percentage from Survivors vs non-Survivors',
    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),
                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]
)

fig = go.Figure(data=data, layout=layout)

fig.show()
# Taking the count of each Pclass value inside the Survivors
df_survivors_pclass = df_survivors['Pclass'].value_counts()
df_survivors_pclass = pd.DataFrame({'Pclass':df_survivors_pclass.index, 'count':df_survivors_pclass.values})

# Taking the count of each Pclass value inside the Survivors
df_nonsurvivors_pclass = df_nonsurvivors['Pclass'].value_counts()
df_nonsurvivors_pclass = pd.DataFrame({'Pclass':df_nonsurvivors_pclass.index, 'count':df_nonsurvivors_pclass.values})


# Creating the plotting objects
pie_survivors_pclass = go.Pie(  
   labels = df_survivors_pclass['Pclass'],
   values = df_survivors_pclass['count'],
   domain=dict(x=[0, 0.5]),
   name='Survivors',
   hole = 0.5,
   marker = dict(colors=['#636EFA', '#EF553B', '#00CC96'], line=dict(color='#000000', width=2))
)

pie_nonsurvivors_pclass = go.Pie(  
   labels = df_nonsurvivors_pclass['Pclass'],
   values = df_nonsurvivors_pclass['count'],
   domain=dict(x=[0.5, 1.0]), 
   name='non-Survivors',
   hole = 0.5,
   marker = dict(colors=['#EF553B', '#00CC96', '#636EFA'], line=dict(color='#000000', width=2))
)

data = [pie_survivors_pclass, pie_nonsurvivors_pclass]


# Plot's Layout (background color, title, annotations, etc.)
layout = go.Layout(
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)',
    title='"Pclass" percentage from Survivors vs non-Survivors',
    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),
                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]
)

fig = go.Figure(data=data, layout=layout)

fig.show()
# Checking out the differences between Fare distribution for survivors and non-survivors
fare_survivors_box = go.Box(  
   x=df_survivors['Fare'],
   name='Survivors',
   marker=dict(color='navy')
)

fare_nonsurvivors_box = go.Box(  
   x=df_nonsurvivors['Fare'],
   name='Non-Survivors',
   marker=dict(color='steelblue')
)
  
data = [fare_nonsurvivors_box, fare_survivors_box]


# Plot's Layout (background color, title, etc.)
layout = go.Layout(
    paper_bgcolor='rgba(0,0,0,0)',
    plot_bgcolor='rgba(0,0,0,0)',
    title='"Fare" value of survivors vs "Fare" value of non-survivors',
    barmode='stack',
    xaxis=dict(
        title='Fare distribution'
    )
)

fig = go.Figure(data=data, layout=layout)
fig.show()
# Third distribution for the hypothesis test - Fares of survivors
dist_c = df_survivors['Fare'].dropna()

# Fourth distribution for the hypothesis test - Fares of non-survivors
dist_d = df_nonsurvivors['Fare'].dropna()
# Z-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different
t_stat_3, p_value_3 = ztest(dist_c, dist_d)
print("----- Z Test Results -----")
print("T stat. = " + str(t_stat_3))
print("P value = " + str(p_value_3)) # P-value is less than 0.05

print("")

# T-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different
t_stat_4, p_value_4 = stats.ttest_ind(dist_c, dist_d)
print("----- T Test Results -----")
print("T stat. = " + str(t_stat_4))
print("P value = " + str(p_value_4)) # P-value is less than 0.05
matrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')
matrix_df = matrix_df.apply(lambda x: round(x, 2)) # Rounding matrix_df's values to 0,XX

sns.heatmap(matrix_df, vmin=0, vmax=1, cmap="Blues", linewidths=0.75, annot=True)
import collections
import matplotlib.pyplot as plt
from scipy import stats
from imblearn.combine import SMOTEENN
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from category_encoders import OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import SimpleImputer, IterativeImputer
from sklearn.model_selection import KFold, StratifiedKFold, StratifiedShuffleSplit, RandomizedSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures
from sklearn.metrics import f1_score, precision_score, recall_score, average_precision_score, roc_auc_score, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from xgboost import XGBClassifier, plot_importance as plot_importance_xgb
from lightgbm import LGBMClassifier, plot_importance as plot_importance_lgbm
# Creating a categorical variable for Ages
df['AgeCat'] = ''
df['AgeCat'].loc[(df['Age'] < 18)] = 'young'
df['AgeCat'].loc[(df['Age'] >= 18) & (df['Age'] < 56)] = 'mature'
df['AgeCat'].loc[(df['Age'] >= 56)] = 'senior'


# Creating a categorical variable for Family Sizes
df['FamilySize'] = ''
df['FamilySize'].loc[(df['SibSp'] <= 2)] = 'small'
df['FamilySize'].loc[(df['SibSp'] > 2) & (df['SibSp'] <= 5 )] = 'medium'
df['FamilySize'].loc[(df['SibSp'] > 5)] = 'large'


# Creating a categorical variable to tell if the passenger is alone
df['IsAlone'] = ''
df['IsAlone'].loc[((df['SibSp'] + df['Parch']) > 0)] = 'no'
df['IsAlone'].loc[((df['SibSp'] + df['Parch']) == 0)] = 'yes'


# Creating a categorical variable to tell if the passenger is a Young/Mature/Senior male or a Young/Mature/Senior female
df['SexCat'] = ''
df['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] <= 21)] = 'youngmale'
df['SexCat'].loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturemale'
df['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] > 50)] = 'seniormale'
df['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] <= 21)] = 'youngfemale'
df['SexCat'].loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturefemale'
df['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] > 50)] = 'seniorfemale'


# Creating a categorical variable for the passenger's title
# Title is created by extracting the prefix before "Name" feature
# This title needs to be a feature because all female titles are grouped with each other
# Also, creating a column to tell if the passenger is married or not
# "Is_Married" is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles
df['Title'] = df['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]
df['Is_Married'] = 0
df['Is_Married'].loc[df['Title'] == 'Mrs'] = 1
df['Title'] = df['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')
df['Title'] = df['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')


# Creating "Ticket Frequency" Feature
# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier
df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')

df.head(10)
def get_feature_names(df):
    # Splitting the target
    target = df['Survived']

    # Dropping unused columns from the feature set
    df.drop(['PassengerId', 'Survived', 'Ticket', 'Name', 'Cabin'], axis=1, inplace=True)

    # Splitting categorical and numerical column dataframes
    categorical_df = df.select_dtypes(include=['object'])
    numeric_df = df.select_dtypes(exclude=['object'])

    # And then, storing the names of categorical and numerical columns.
    categorical_columns = list(categorical_df.columns)
    numeric_columns = list(numeric_df.columns)
    
    print("Categorical columns:\n", categorical_columns)
    print("\nNumeric columns:\n", numeric_columns)

    return target, categorical_columns, numeric_columns

target, categorical_columns, numeric_columns = get_feature_names(df)
# You can call any of the functions below, if you wish, inside the "defineBestModelPipeline()" function

def balancingClassesRus(x_train, y_train):
    
    # Using RandomUnderSampler to balance our training data points
    rus = RandomUnderSampler(random_state=7)
    features_balanced, target_balanced = rus.fit_resample(x_train, y_train)
    
    print("Count for each class value after RandomUnderSampler:", collections.Counter(target_balanced))
    
    return features_balanced, target_balanced


def balancingClassesSmoteenn(x_train, y_train):
    
    # Using SMOTEEN to balance our training data points
    smn = SMOTEENN(random_state=7)
    features_balanced, target_balanced = smn.fit_resample(x_train, y_train)
    
    print("Count for each class value after SMOTEEN:", collections.Counter(target_balanced))
    
    return features_balanced, target_balanced


def balancingClassesSmote(x_train, y_train):

    # Using SMOTE to to balance our training data points
    sm = SMOTE(random_state=7)
    features_balanced, target_balanced = sm.fit_resample(x_train, y_train)

    print("Count for each class value after SMOTE:", collections.Counter(target_balanced))

    return features_balanced, target_balanced
# Function responsible for checking our model's performance on the test data
def testSetResultsClassifier(classifier, x_test, y_test):
    predictions = classifier.predict(x_test)
    
    results = []
    f1 = f1_score(y_test, predictions)
    precision = precision_score(y_test, predictions)
    recall = recall_score(y_test, predictions)
    roc_auc = roc_auc_score(y_test, predictions)
    accuracy = accuracy_score(y_test, predictions)
    
    results.append(accuracy)
    results.append(precision)
    results.append(recall)
    results.append(f1)
    results.append(roc_auc)
    
    
    print("\n\n#---------------- Test set results (Best Classifier) ----------------#\n")
    print("Accuracy, Precision, Recall, F1-Score, ROC_AUC score:")
    print(results)
    
    return results
# Now, we are going to create our Pipeline, fitting several different data preprocessing, feature selection 
# and modeling techniques inside a RandomSearchCV, to check which group of techniques has better performance.

# Building a Pipeline inside RandomSearchCV, responsible for finding the best model and it's parameters
def defineBestModelPipeline(df, target, categorical_columns, numeric_columns):
    
    # Splitting original data into Train and Test BEFORE applying transformations
    # Later in RandomSearchCV, x_train will be splitted into train/val sets
    # The transformations are going to be fitted specifically on the train set,
    # and then applied to both train/test sets. This way, information leakage is avoided!
    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.10, random_state=42)
    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays
    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays
    
    
    # # If desired, we can balance training classes using one of the functions below
    # # Obtaining balanced data for modeling using Random Under Sampling
    #x_train, y_train = balancingClassesRus(x_train, y_train)

    # # Obtaining balanced data for modeling using SMOTEENN
    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)

    # # Obtaining balanced data for modeling using SMOTE
    #x_train, y_train = balancingClassesSmote(x_train, y_train)
    
    
    # 1st -> Numeric Transformers
    # Here, we are creating different several different data transformation pipelines 
    # to be applied in our numeric features
    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),
                                            ('scaler', MinMaxScaler())])
    
    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),
                                            ('scaler', StandardScaler())])
    
    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),
                                            ('scaler', MinMaxScaler())])
    
    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),
                                            ('scaler', StandardScaler())])
    
    
    # 2nd -> Categorical Transformer
    # Despite my option of not doing it, you can also choose to create different 
    # data transformation pipelines for your categorical features.
    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),
                                              ('onehot', OneHotEncoder(use_cat_names=True))])
    
    
    # 3rd -> Combining both numerical and categorical pipelines
    # Here, we are creating different ColumnTransformers, each one with a different numerical transformation
    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),
                                                             ('cat', categorical_transformer, categorical_columns)])
    
    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),
                                                             ('cat', categorical_transformer, categorical_columns)])
    
    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),
                                                             ('cat', categorical_transformer, categorical_columns)])
    
    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),
                                                             ('cat', categorical_transformer, categorical_columns)])
    
    
    # And finally, we are going to apply these different data transformations to RandomSearchCV,
    # trying to find the best imputing strategy, the best feature engineering strategy
    # and the best model with it's respective parameters.
    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.
    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above
                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method
                           ('clf', SVC())]) # Initializing modeling step of the pipeline with any model object
                           #memory='cache_folder') -> Used to optimize memory when needed
    
    
    # Now, we define the grid of parameters that RandomSearchCV will use. It will randomly chose
    # options for each step inside the dictionaries ('data transformations', 'feature_eng', 'clf'
    # and 'clf parameters'). In the end of it's iterations, RandomSearchCV will return the best options.
    params_grid = [
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [KNeighborsClassifier()],
                     'clf__n_neighbors': stats.randint(1, 50),
                     'clf__metric': ['minkowski', 'euclidean']},

        

                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [LogisticRegression()],
                     'clf__penalty': ['l1', 'l2'],
                     'clf__C': stats.uniform(0.01, 10)},


        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [SVC()],
                     'clf__C': stats.uniform(0.1, 10),
                     'clf__gamma': stats.uniform(0.1, 10)},


        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [DecisionTreeClassifier()],
                     'clf__criterion': ['gini', 'entropy'],
                     'clf__max_features': [None, "auto", "log2"],
                     'clf__max_depth': [None, stats.randint(1, 5)]},


        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [RandomForestClassifier()],
                     'clf__n_estimators': stats.randint(10, 175),
                     'clf__max_features': [None, "auto", "log2"],
                     'clf__max_depth': [None, stats.randint(1, 5)],
                     'clf__random_state': stats.randint(1, 49)},
        
                    
        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [ExtraTreesClassifier()],
                     'clf__n_estimators': stats.randint(10, 150),
                     'clf__max_features': [None, "auto", "log2"],
                     'clf__max_depth': [None, stats.randint(1, 6)]},

                    
        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [GradientBoostingClassifier()],
                     'clf__n_estimators': stats.randint(10, 100),
                     'clf__learning_rate': stats.uniform(0.01, 0.7),
                     'clf__max_depth': [None, stats.randint(1, 6)]},

        
        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [LGBMClassifier()],
                     'clf__n_estimators': stats.randint(1, 100),
                     'clf__learning_rate': stats.uniform(0.01, 0.7),
                     'clf__max_depth': [None, stats.randint(1, 6)]},


        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [XGBClassifier()],
                     'clf__n_estimators': stats.randint(5, 125),
                     'clf__eta': stats.uniform(0.01, 1),
                     'clf__max_depth': [None, stats.randint(1, 6)],
                     'clf__gamma': stats.uniform(0.01, 1)},


        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [StackingClassifier(estimators=[('svc', SVC(C=0.1, gamma=0.1)),
                                                            ('rf', RandomForestClassifier(max_depth=5, n_estimators=50, n_jobs=-1, random_state=28)),
                                                            ('xgb', XGBClassifier(eta=0.5, gamma=0.5, max_depth=None, n_estimators=25))], 
                                                final_estimator=LogisticRegression(C=1))]},
   
   
        
                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],
                     'feature_eng': [None, 
                                     PCA(n_components=round(x_train.shape[1]*0.9)),
                                     PCA(n_components=round(x_train.shape[1]*0.8)),
                                     PCA(n_components=round(x_train.shape[1]*0.7)),
                                     PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],
                     'clf': [VotingClassifier(estimators=[('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),
                                                          ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),
                                                          ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))],
                                              voting='soft')]}
                ]
    
    
    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above
    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']
    
    # Creating our cross validation object with StratifiedShuffleSplit, 10 folds
    # Stratification assures that we split the data such that the proportions
    # between classes are the same in each fold as they are in the whole dataset
    cross_validator = StratifiedShuffleSplit(n_splits=10, train_size=0.8, test_size=0.2, random_state=49)
    
    # Creating the randomized search cv object and fitting it
    best_model_pipeline = RandomizedSearchCV(estimator=pipe, param_distributions=params_grid, 
                                             n_iter=100, scoring=metrics, refit='accuracy', 
                                             n_jobs=-1, cv=cross_validator, random_state=21)

    best_model_pipeline.fit(x_train, y_train)
    
    
    # At last, we check the final results
    print("\n\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\n\n", best_model_pipeline.best_estimator_[0])
    print("\n\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\n\n", best_model_pipeline.best_estimator_[1])
    print("\n\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\n\n", best_model_pipeline.best_estimator_[2])
    print("\n\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\n\n", best_model_pipeline.best_score_)
    
    return x_train, x_test, y_train, y_test, best_model_pipeline
# Calling the function above, returing train/test data and best model's pipeline
x_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_columns, numeric_columns)


# Checking best model's performance on test data
test_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)
# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps
df_results = pd.DataFrame(best_model_pipeline.cv_results_)

display(df_results)
# Now visualizing all results and metrics obtained only by the best classifier
display(df_results[df_results['rank_test_accuracy'] == 1])
# Here, we access the categorical feature names generated by OneHotEncoder, and then concatenate them
# with the numerical feature names, in the same order our pipeline is applying data transformations.
categorical_features_after_onehot = best_model_pipeline.best_estimator_.named_steps['data_transformations']\
                                        .transformers_[1][1].named_steps['onehot'].get_feature_names()

feature_names_in_order = numeric_columns + categorical_features_after_onehot

print(feature_names_in_order)
# # Plotting feature importances of the best model, if sklearn tree-based (top 5 features)
#print("\n#---------------- Bar plot with feature importances ----------------#")
#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].feature_importances_, index=feature_names_in_order)
#feat_importances.nlargest(5).plot(kind='barh')


# # Plotting feature importances of the best model, if linear regression-based (top 5 features)
#print("\n#---------------- Bar plot with feature importances ----------------#")
#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].coef_, index=feature_names_in_order)
#feat_importances.nlargest(5).plot(kind='barh')


# # Plotting feature importances for XGB Model
#plot_importance_xgb(best_model_pipeline.best_estimator_.named_steps['clf'], height=0.4, 
#title='Feature Importances for XGB Classifier', importance_type='gain')


# # Plotting feature importances for LGBM Model
#plot_importance_lgbm(best_model_pipeline.best_estimator_.named_steps['clf'], 
#                     figsize=(10, 4), title='Feature importances for LGBM Classifier',
#                     importance_type='gain', max_num_features=10)
# Importing data and displaying some rows
df_test = pd.read_csv("/kaggle/input/titanic/test.csv")


# Creating a categorical variable for Ages
df_test['AgeCat'] = ''
df_test['AgeCat'].loc[(df_test['Age'] < 18)] = 'young'
df_test['AgeCat'].loc[(df_test['Age'] >= 18) & (df_test['Age'] < 56)] = 'mature'
df_test['AgeCat'].loc[(df_test['Age'] >= 56)] = 'senior'


# Creating a categorical variable for Family Sizes
df_test['FamilySize'] = ''
df_test['FamilySize'].loc[(df_test['SibSp'] <= 2)] = 'small'
df_test['FamilySize'].loc[(df_test['SibSp'] > 2) & (df_test['SibSp'] <= 5 )] = 'medium'
df_test['FamilySize'].loc[(df_test['SibSp'] > 5)] = 'large'


# Creating a categorical variable to tell if the passenger is alone
df_test['IsAlone'] = ''
df_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) > 0)] = 'no'
df_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) == 0)] = 'yes'


# Creating a categorical variable to tell if the passenger is a Young/Mature/Senior male or a Young/Mature/Senior female
df_test['SexCat'] = ''
df_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] <= 21)] = 'youngmale'
df_test['SexCat'].loc[(df_test['Sex'] == 'male') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturemale'
df_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] > 50)] = 'seniormale'
df_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] <= 21)] = 'youngfemale'
df_test['SexCat'].loc[(df_test['Sex'] == 'female') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturefemale'
df_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] > 50)] = 'seniorfemale'


# Creating a categorical variable for the passenger's title
# Title is created by extracting the prefix before "Name" feature
# This title needs to be a feature because all female titles are grouped with each other
# Also, creating a column to tell if the passenger is married or not
# "Is_Married" is a binary feature based on the Mrs title. Mrs title has the highest survival rate among other female titles
df_test['Title'] = df_test['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]
df_test['Is_Married'] = 0
df_test['Is_Married'].loc[df['Title'] == 'Mrs'] = 1
df_test['Title'] = df_test['Title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Lady', 'Mme', 'the Countess', 'Dona'], 'Miss/Mrs/Ms')
df_test['Title'] = df_test['Title'].replace(['Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Dr/Military/Noble/Clergy')


# Creating "Ticket Frequency" Feature
# There are too many unique Ticket values to analyze, so grouping them up by their frequencies makes things easier
df_test['Ticket_Frequency'] = df_test.groupby('Ticket')['Ticket'].transform('count')


# Dropping unnecessary columns
df_test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)
# Applying best_model_pipeline
# Step 1 -> Transforming data the same way we did in the training set;
# Step 2 -> making predictions using the best model obtained by RandomSearchCV.
test_predictions = best_model_pipeline.predict(df_test)
print(test_predictions)
# Generating predictions file that is going to be submitted to the competition
df_submission = pd.read_csv("/kaggle/input/titanic/test.csv")

df_submission['Survived'] = test_predictions # Adding a column with predicted values

df_submission.drop(df_submission.columns.difference(['PassengerId', 'Survived']), axis=1, inplace=True) # Selecting only needed columns

df_submission.head(10)
# Checking if the number of rows is OK (the file is expected to have 418 rows)
df_submission.count()
# Writing submitions to CSV file
df_submission.to_csv('submission.csv', index=False)
# Defining a PCA Pipeline
def definePCAPipeline(categorical_columns, numeric_columns):

    # 1st -> Numeric Transformer
    numeric_transformer = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),
                                            ('scaler', StandardScaler())])
    
    
    # 2nd -> Categorical Transformer
    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),
                                              ('onehot', OneHotEncoder(use_cat_names=True))])
    
    
    # 3rd -> Combining both numerical and categorical pipelines
    data_transformations = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_columns),
                                                             ('cat', categorical_transformer, categorical_columns)])
    

    # 4th -> Final PCA Pipeline
    pca_pipeline = Pipeline(steps=[('data_transformations', data_transformations),
                           ('feature_eng', PCA(n_components=2, whiten=True))])
    
    
    return pca_pipeline
# Generating transformed data after PCA, only 2 principal components chosen
pca_pipeline = definePCAPipeline(categorical_columns, numeric_columns)
pca_arr = pca_pipeline.fit_transform(df)

# How much variance does our PCA obtained components explains from the original variance in data?
comp1 = pca_pipeline[1].explained_variance_ratio_[0]
comp2 = pca_pipeline[1].explained_variance_ratio_[1]
exp_variance_pca = comp1 + comp2

print("Compontent 1 explained variance ratio:", comp1)
print("Compontent 2 explained variance ratio:", comp2)
print("Total explained variance ratio obtained from both components:", exp_variance_pca)
# Visualizing the new 2-dimensional dataset and points' respective classes
pca_df = pd.DataFrame(pca_arr, columns=["PC1", "PC2"])
pca_df['Survived'] = target

pca_df.head(10)
# Creating different datasets for survivors and non-survivors
pca_df_survivors = pca_df[pca_df['Survived'] == 1]
pca_df_nonsurvivors = pca_df[pca_df['Survived'] == 0]


# Visualizing the two-dimensional dataset
scatter_obj_survs = go.Scatter(x=pca_df_survivors['PC1'],
                               y=pca_df_survivors['PC2'],
                               mode="markers",
                               name='Survivors',
                               marker=dict(color='forestgreen'))


scatter_obj_nonsurvs = go.Scatter(x=pca_df_nonsurvivors['PC1'],
                                  y=pca_df_nonsurvivors['PC2'],
                                  mode="markers",
                                  name='Non-survivors',
                                  marker=dict(color='darkred'))


data = [scatter_obj_survs, scatter_obj_nonsurvs]


# Plot's Layout (background color, title, etc.)
layout = go.Layout(title='2-Dimensional visualization of survivors and non-survivors',
                   xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))

fig = go.Figure(data=data, layout=layout)
fig.show()
# Generating transformed data using PCA pipeline's data transformations (pca_pipeline[0])
transformed_df = pca_pipeline[0].transform(df)

# Then transforming this data with TSNE object
tsne = TSNE(n_components=2, random_state=1)
tsne_arr = tsne.fit_transform(transformed_df)
# Visualizing the new 2-dimensional dataset and points' respective classes
tsne_df = pd.DataFrame(tsne_arr, columns=["tsne_dim1", "tsne_dim2"])
tsne_df['Survived'] = target

tsne_df.head(10)
# Creating different datasets for survivors and non-survivors
tsne_df_survivors = tsne_df[tsne_df['Survived'] == 1]
tsne_df_nonsurvivors = tsne_df[tsne_df['Survived'] == 0]


# Visualizing the two-dimensional dataset
scatter_obj_survs_tsne = go.Scatter(x=tsne_df_survivors['tsne_dim1'],
                                    y=tsne_df_survivors['tsne_dim2'],
                                    mode="markers",
                                    name='Survivors',
                                    marker=dict(color='forestgreen'))


scatter_obj_nonsurvs_tsne = go.Scatter(x=tsne_df_nonsurvivors['tsne_dim1'],
                                       y=tsne_df_nonsurvivors['tsne_dim2'],
                                       mode="markers",
                                       name='Non-survivors',
                                       marker=dict(color='darkred'))


data_tsne = [scatter_obj_survs_tsne, scatter_obj_nonsurvs_tsne]


# Plot's Layout (background color, title, etc.)
layout_tsne = go.Layout(title='2-Dimensional visualization of survivors and non-survivors (t-SNE algorithm)',
                        xaxis=dict(title='tsne_dim1'), yaxis=dict(title='tsne_dim2'))

fig_tsne = go.Figure(data=data_tsne, layout=layout_tsne)
fig_tsne.show()
# K-Means and silhouette_score libraries
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score


# Defining a function to find the optimal number of K-Means' n_clusters parameter:
def findOptimalNClustersKMeans(transformed_df):
    
    # Number of clusters to search for and silhouette_scores list
    range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
    silhouette_scores = []

    # Testing n_clusters options
    for n_clusters in range_n_clusters:
        kmeans = KMeans(n_clusters=n_clusters, random_state=7)
        cluster_labels = kmeans.fit_predict(transformed_df)
        
        # Evaluating clusters created by KMeans
        silhouette_avg = silhouette_score(transformed_df, cluster_labels)
        print("K-Means: for n_clusters =", n_clusters, ", the average silhouette_score is", silhouette_avg)
        
        # Appending iteration's avg silhouette score to scores list
        silhouette_scores.append(silhouette_avg)
        
    return range_n_clusters, silhouette_scores

range_n_clusters, silhouette_scores = findOptimalNClustersKMeans(transformed_df)
# Visualizing the "Elbow" graph
elbow_kmeans = go.Scatter(x=range_n_clusters,
                          y=silhouette_scores,
                          mode='lines',
                          name='kmeans elbow line',
                          marker_color='orange')

# Plot's Layout (background color, title, etc.)
layout_elbow_kmeans = go.Layout(title='Number of Clusters x Average Silhouette Score, K-Means Algorithm',
                          xaxis=dict(title='n_clusters'), yaxis=dict(title='Average Silhouette Score'))

elbow_kmeans = go.Figure(data=elbow_kmeans, layout=layout_elbow_kmeans)
elbow_kmeans.show()
# Since the best value for silhouette_score was found with 3 clusters,
# let's create a K-Means Object with n_clusters = 3.
kmeans = KMeans(n_clusters=3, random_state=7)

# Fitting and clusterizing with KMeans
kmeans_defined_clusters = kmeans.fit_predict(transformed_df)

# Concatenating K-Means clusters into the 2-D PCA transformed df
pca_df['KMeans_Defined_Clusters'] = ''
pca_df['KMeans_Defined_Clusters'] = kmeans_defined_clusters
pca_df['KMeans_Defined_Clusters'] = pca_df['KMeans_Defined_Clusters'].astype(str)

pca_df.head(10)
# KMeans object also stores centroids' positions for each cluster
pca_cluster_centers = pca_pipeline[1].transform(kmeans.cluster_centers_)

# Creating a dataframe to store cluster centers (centroids)
# The goal is to visualize them (in 2-D) inside each cluster
centroids_df = pd.DataFrame(pca_cluster_centers, columns=["X_coord", "Y_coord"])

# Creating columns to store which centroid belongs to each cluster
centroids_df['cluster_centroid'] = ""
centroids_df['cluster_centroid'][0] = 'centroid, cluster 0'
centroids_df['cluster_centroid'][1] = 'centroid, cluster 1'
centroids_df['cluster_centroid'][2] = 'centroid, cluster 2'

centroids_df.head(10)
# Colors
colors = ['orange', 'steelblue', 'violet']

cyclecolors = cycle(colors)
color = next(cyclecolors)

kmeans_clusters = pca_df.KMeans_Defined_Clusters.unique()
kmeans_centroids = centroids_df.cluster_centroid.unique()


# Visualizing the two-dimensional dataset
data_kmeans = []

for cluster in kmeans_clusters:
    scatter_obj_cluster_kmeans = go.Scatter(x=pca_df[(pca_df['KMeans_Defined_Clusters'] == cluster)]['PC1'],
                                            y=pca_df[(pca_df['KMeans_Defined_Clusters'] == cluster)]['PC2'],
                                            mode='markers',
                                            name=cluster,
                                            marker_color=color)
    data_kmeans.append(scatter_obj_cluster_kmeans)
    color = next(cyclecolors)
    

# Plotting the centroids of each cluster
for centroid in kmeans_centroids:
    scatter_obj_centroid_kmeans = go.Scatter(x=centroids_df[(centroids_df['cluster_centroid'] == centroid)]['X_coord'],
                                             y=centroids_df[(centroids_df['cluster_centroid'] == centroid)]['Y_coord'],
                                             mode='markers',
                                             name=centroid,
                                             marker_size=12,
                                             marker_symbol='x-dot',
                                             marker_color='black')
    data_kmeans.append(scatter_obj_centroid_kmeans)


# Plot's Layout (background color, title, etc.)
layout_kmeans = go.Layout(title='PCA 2-Dimensional visualization of clusters created by K-Means Algorithm',
                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))

fig_kmeans = go.Figure(data=data_kmeans, layout=layout_kmeans)
fig_kmeans.show()
# Agglomerative Clustering and dendrogram plotting libraries
from sklearn.cluster import AgglomerativeClustering
from scipy.cluster.hierarchy import dendrogram, ward

# Generating Linkage array (using ward linkage) and dendrogram object
linkage_array = ward(transformed_df)
dendrogram(linkage_array)

# Plotting dendrogram
ax = plt.gca()
bounds = ax.get_xbound()

ax.plot(bounds, [52, 52], '--', c='k')
ax.plot(bounds, [41, 41], '--', c='k')
ax.plot(bounds, [34.9, 34.9], '--', c='k')
ax.plot(bounds, [31.75, 31.75], '--', c='k')
ax.text(bounds[1], 52, ' two clusters', va='center', fontdict={'size': 15})
ax.text(bounds[1], 41, ' three clusters', va='center', fontdict={'size': 15})
ax.text(bounds[1], 34.9, ' four clusters', va='center', fontdict={'size': 15})
ax.text(bounds[1], 31.75, ' five clusters', va='center', fontdict={'size': 15})

plt.xlabel("Sample Index")
plt.ylabel("Cluster Distance")
# Defining a function to find the optimal number of A.C.'s n_clusters parameter:
def findOptimalNClustersAC(transformed_df):
    
    # Number of clusters to search for and silhouette_scores list
    range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10]
    silhouette_scores = []

    # Testing n_clusters options
    for n_clusters in range_n_clusters:
        ac = AgglomerativeClustering(n_clusters=n_clusters)
        cluster_labels = ac.fit_predict(transformed_df)
        
        # Evaluating clusters created by Agglomerative Clustering
        silhouette_avg = silhouette_score(transformed_df, cluster_labels)
        print("Agglomerative Clustering: for n_clusters =", n_clusters, ", the average silhouette_score is", silhouette_avg)
        
        # Appending iteration's avg silhouette score to scores list
        silhouette_scores.append(silhouette_avg)
        
    return range_n_clusters, silhouette_scores

range_n_clusters_ac, silhouette_scores_ac = findOptimalNClustersAC(transformed_df)
# Visualizing the "Elbow" graph
elbow_ac = go.Scatter(x=range_n_clusters_ac,
                      y=silhouette_scores_ac,
                      mode='lines',
                      name='agglomerative clustering elbow line',
                      marker_color='dodgerblue')

# Plot's Layout (background color, title, etc.)
layout_elbow_ac = go.Layout(title='Number of Clusters x Average Silhouette Score, Agglomerative Clustering Algorithm',
                            xaxis=dict(title='n_clusters'), yaxis=dict(title='Average Silhouette Score'))

elbow_ac = go.Figure(data=elbow_ac, layout=layout_elbow_ac)
elbow_ac.show()
# Since the best value for silhouette_score was found with 3 clusters,
# let's create an A.C. Object with n_clusters = 3.
ac = AgglomerativeClustering(n_clusters=3)
ac_defined_clusters = ac.fit_predict(transformed_df)

# Concatenating A.C. clusters into the 2-D PCA transformed df
pca_df['AgglomerativeClustering_Defined_Clusters'] = ''
pca_df['AgglomerativeClustering_Defined_Clusters'] = ac_defined_clusters
pca_df['AgglomerativeClustering_Defined_Clusters'] = pca_df['AgglomerativeClustering_Defined_Clusters'].astype(str)

pca_df.head(10)
# Colors
colors = ['orange', 'steelblue', 'violet', 'tomato', 'lime']

cyclecolors = cycle(colors)
color = next(cyclecolors)

ac_clusters = pca_df.AgglomerativeClustering_Defined_Clusters.unique()


# Visualizing the two-dimensional dataset
data_ac = []

for cluster in ac_clusters:
    scatter_obj_cluster_ac = go.Scatter(x=pca_df[(pca_df['AgglomerativeClustering_Defined_Clusters'] == cluster)]['PC1'],
                                            y=pca_df[(pca_df['AgglomerativeClustering_Defined_Clusters'] == cluster)]['PC2'],
                                            mode='markers',
                                            name=cluster,
                                            marker_color=color)
    data_ac.append(scatter_obj_cluster_ac)
    color = next(cyclecolors)


# Plot's Layout (background color, title, etc.)
layout_ac = go.Layout(title='PCA 2-Dimensional visualization of clusters created by Agglomerative Clustering algorithm',
                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))

fig_ac = go.Figure(data=data_ac, layout=layout_ac)
fig_ac.show()
# DBSCAN's sklearn library
from sklearn.cluster import DBSCAN

# Creating DBSCAN object and defining clusters for each data point
# NOTE: Noise points (outliers) are assigned to "cluster -1"
dbscan = DBSCAN(min_samples=20, eps=2.4)
dbscan_defined_clusters = dbscan.fit_predict(transformed_df)

# Evaluating clusters created by DBSCAN
silhouette_dbscan = silhouette_score(transformed_df, dbscan_defined_clusters)
print("DBSCAN: for eps = 2.4 and min_samples = 20, the average silhouette_score is", silhouette_dbscan)
# Concatenating DBSCAN clusters into the 2-D PCA transformed df
pca_df['DBSCAN_Defined_Clusters'] = ''
pca_df['DBSCAN_Defined_Clusters'] = dbscan_defined_clusters
pca_df['DBSCAN_Defined_Clusters'] = pca_df['DBSCAN_Defined_Clusters'].astype(str)

pca_df.head(10)
# Colors
colors = ['orange', 'steelblue', 'violet', 'tomato']

cyclecolors = cycle(colors)
color = next(cyclecolors)

dbscan_clusters = pca_df.DBSCAN_Defined_Clusters.unique()


# Visualizing the two-dimensional dataset
data_dbscan = []

for cluster in dbscan_clusters:
    scatter_obj_cluster_dbscan = go.Scatter(x=pca_df[(pca_df['DBSCAN_Defined_Clusters'] == cluster)]['PC1'],
                                            y=pca_df[(pca_df['DBSCAN_Defined_Clusters'] == cluster)]['PC2'],
                                            mode='markers',
                                            name=cluster,
                                            marker_color=color)
    data_dbscan.append(scatter_obj_cluster_dbscan)
    color = next(cyclecolors)


# Plot's Layout (background color, title, etc.)
layout_dbscan = go.Layout(title='PCA 2-Dimensional visualization of clusters created by DBSCAN',
                          xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))

fig_dbscan = go.Figure(data=data_dbscan, layout=layout_dbscan)
fig_dbscan.show()
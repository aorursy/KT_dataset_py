# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



from subprocess import check_output

print(check_output(["ls", "../input"]).decode("utf8"))



# Any results you write to the current directory are saved as output.
import pandas as pd

from sklearn.cross_validation import train_test_split

from sklearn.preprocessing import StandardScaler

from sklearn.feature_selection import RFE

from sklearn.linear_model import LogisticRegression

from sklearn.ensemble import ExtraTreesClassifier

from sklearn.cross_validation import KFold;

import xgboost as xgb

from sklearn.metrics import accuracy_score

from sklearn.feature_selection import SelectFromModel



# script.py



df = pd.read_csv('../input/diabetes.csv')



import matplotlib.pyplot as plt

import seaborn as sns

# sns.set(style='whitegrid', context='notebook')

drop_elements = ['BloodPressure', 'SkinThickness', 'Insulin', 'Outcome']

cols=['Pregnancies', 'Glucose','BloodPressure','SkinThickness','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']

# sns.pairplot(df[cols], size=2.5)

# plt.show()



import numpy as np

cm = np.corrcoef(df[cols].values.T)

sns.set(font_scale=1)

# hm = sns.heatmap(cm, cbar=True,annot=True,square=True,fmt='.2f',annot_kws={'size': 15},yticklabels=cols,xticklabels=cols)

# plt.show()



y = df['Outcome'].values

df = df.drop(drop_elements, axis=1)

X = df.iloc[:, :].values



X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)



stdsc = StandardScaler()

X_train_std = stdsc.fit_transform(X_train)

X_test_std = stdsc.fit_transform(X_test)



model = ExtraTreesClassifier()

model.fit(X_train_std, y_train)

# display the relative importance of each attribute



importances = model.feature_importances_

std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)

indices = np.argsort(importances)[::-1]



# Print the feature ranking

print("Feature ranking:")



for f in range(X_train_std.shape[1]):

    print("%d. feature %d (%f)" % (f + 1, indices[f], importances[indices[f]]))



import matplotlib.pyplot as plt

import seaborn as sns



from sklearn.cross_validation import StratifiedShuffleSplit

from sklearn.metrics import accuracy_score, log_loss

from sklearn.neighbors import KNeighborsClassifier

from sklearn.svm import SVC

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.lda import LDA

from sklearn.qda import QDA

from sklearn.linear_model import LogisticRegression



classifiers = [

    KNeighborsClassifier(3),

    SVC(probability=True),

    DecisionTreeClassifier(),

    RandomForestClassifier(),

    AdaBoostClassifier(),

    GradientBoostingClassifier(),

    GaussianNB(),

    LDA(),

    QDA(),

    LogisticRegression()

]





log_cols = ["Classifier", "Accuracy"]

log = pd.DataFrame(columns=log_cols)



X = X_train

y = y_train



sss = StratifiedShuffleSplit(y, n_iter=10, test_size=0.1, random_state=0)



acc_dict = {}



for train_index, test_index in sss:

    X_train, X_test = X[train_index], X[test_index]

    y_train, y_test = y[train_index], y[test_index]



    for clf in classifiers:

        name = clf.__class__.__name__

        clf.fit(X_train, y_train)

        train_predictions = clf.predict(X_test)

        acc = accuracy_score(y_test, train_predictions)

        if name in acc_dict:

            acc_dict[name] += acc

        else:

            acc_dict[name] = acc

        # print '{0}: {1}'.format(name, acc * 100)



for clf in acc_dict:

    acc_dict[clf] = acc_dict[clf] / 10.0

    log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)

    log = log.append(log_entry)



plt.xlabel('Accuracy')

plt.title('Classifier Accuracy')

# plt.show()

sns.set_color_codes("muted")

sns.barplot(x='Accuracy', y='Classifier', data=log, color="b")

from operator import itemgetter



sorted_dict = sorted(acc_dict.items(), key=itemgetter(1), reverse=True)



for k, v in sorted_dict:

    print ("{0}-{1:.2%}".format(k, v))



ntrain = X_train.shape[0]

ntest = y_test.shape[0]

SEED = 0  # for reproducibility

NFOLDS = 5  # set folds for out-of-fold prediction

kf = KFold(ntrain, n_folds=NFOLDS, random_state=SEED)



class SklearnHelper(object):

    def __init__(self, clf, seed=0, params=None):

        params['random_state'] = seed

        self.clf = clf(**params)



    def train(self, x_train, y_train):

        self.clf.fit(x_train, y_train)



    def predict(self, x):

        return self.clf.predict(x)



    def fit(self, x, y):

        return self.clf.fit(x, y)



    def feature_importances(self, x, y):

        print(self.clf.fit(x, y).feature_importances_)



def get_oof(clf, x_train, y_train, x_test):

    oof_train = np.zeros((ntrain,))

    oof_test = np.zeros((ntest,))

    oof_test_skf = np.empty((NFOLDS, ntest))



    for i, (train_index, test_index) in enumerate(kf):

        x_tr = x_train[train_index]

        y_tr = y_train[train_index]

        x_te = x_train[test_index]



        clf.train(x_tr, y_tr)



        oof_train[test_index] = clf.predict(x_te)

        oof_test_skf[i, :] = clf.predict(x_test)



    oof_test[:] = oof_test_skf.mean(axis=0)

    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)





# Put in our parameters for said classifiers

# Random Forest parameters

rf_params = {

    'n_jobs': -1,

    'n_estimators': 500,

     # 'warm_start': True,

     #'max_features': 0.2,

    'max_depth': 6,

    'min_samples_leaf': 2,

    'max_features' : 'sqrt',

    'verbose': 0

}



# Extra Trees Parameters

et_params = {

    'n_jobs': -1,

    'n_estimators':500,

    #'max_features': 0.5,

    'max_depth': 8,

    'min_samples_leaf': 2,

    'verbose': 0

}



# AdaBoost parameters

ada_params = {

    'n_estimators': 500,

    'learning_rate' : 0.75

}



# Gradient Boosting parameters

gb_params = {

    'n_estimators': 500,

     #'max_features': 0.2,

    'max_depth': 5,

    'min_samples_leaf': 2,

    'verbose': 0

}

lr_params = {}



rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)

et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)

ada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)

gb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)

lr = SklearnHelper(clf=LogisticRegression, seed=SEED, params=lr_params)



# Create our OOF train and test predictions. These base results will be used as new features

et_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees

rf_oof_train, rf_oof_test = get_oof(rf, X_train, y_train, X_test) # Random Forest

ada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost

gb_oof_train, gb_oof_test = get_oof(gb, X_train, y_train, X_test) # Gradient Boost

lr_oof_train, lr_oof_test = get_oof(lr, X_train, y_train, X_test) # Logistic Regression



print("Training is complete")



base_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),

     'ExtraTrees': et_oof_train.ravel(),

     'AdaBoost': ada_oof_train.ravel(),

      'GradientBoost': gb_oof_train.ravel(),

      'LogisticRegression': lr_oof_train.ravel()

    })



x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, lr_oof_train), axis=1)

x_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, lr_oof_test), axis=1)



gbm = xgb.XGBClassifier(

    #learning_rate = 0.02,

 n_estimators= 2000,

 max_depth= 9,

 min_child_weight= 2,

 #gamma=1,

 gamma=0.4,

 subsample=0.8,

 colsample_bytree=0.8,

 objective= 'binary:logistic',

 nthread= -1,

 scale_pos_weight=1).fit(x_train, y_train)

y_pred = gbm.predict(x_test)

predictions = [round(value) for value in y_pred]

accuracy = accuracy_score(y_test, predictions)

print("Accuracy: %.2f%%" % (accuracy * 100.0))

## Faster method to perform Text Preprocessing 

## by joining all tweets text into one single long tweet string



import pandas as pd

import numpy as np

import re

import string

import os 

from stop_words import get_stop_words # more comprehensive than nltk

from nltk.corpus import stopwords

import time 

import pdb
# read in csv

df = pd.read_csv('../input/customer-support-on-twitter/twcs/twcs.csv')

df
# join tweets text as one single long string 

# IMPT: join tweets with a newline + spacing '\n ' so we can split them back into individual tweets after preprocessing

text_list = df['text'].values.tolist()

text_list = [re.sub('\n', '', x) for x in text_list] # IMPT: remove some existing '\n'

text_one_long = '\n '.join(text_list)

assert len(text_one_long.split('\n ')) == len(text_list) # assert to ensure join later



# check 

text_one_long[0:500]
# faster method to preprocess as one long string but keep newlines



def fast_clean_text_keep_newline(text):

    print("0 Start")

    assert isinstance(text, str), "Needs to be one long joined string isntead of list of strings, if not slow"

    # lower

    text = text.lower()

    print("1")

    # first replace weird quotes for stopwords removal

    text = re.sub("â€˜", "'", text) # weird left quote

    text = re.sub("â€™", "'", text) # weird right quote

    print("2")

    # remove twitter handle without removing emails

    text = re.sub("\\B@[A-Za-z_]+", ' ', text)

    print("3")

    # remove urls 

    text = re.sub("(https?://|https?://www|www)\S+", ' ', text) # \S+ used instead since .*? more suitable regex for in between words

    print("4")

    # remove punctuation 

    text = re.sub("[â€”Â¡â€œâ€â€¦{}]".format(string.punctuation), ' ', text)

    print("5")

    # remove emoticons 

    emo_regex = 'ğŸ”£|ğŸ›°|ğŸ‘|ğŸ‘–|ğŸ•‹|ğŸŒ¦|â”|ğŸ“…|ğŸŒ‚|ğŸ˜‡|ğŸ‚|ğŸš†|ğŸ—‘|ğŸ’|ğŸ–¼|ğŸ’”|ğŸˆ³|ğŸº|ğŸš‹|ğŸ°|ğŸ˜©|ãŠ™|ğŸ¶|ğŸ¦Š|ğŸ‘|ğŸ‘Š|ğŸ¤¸|ğŸŒ¨|ğŸ©|ğŸ†|ğŸŒ°|ğŸ‹|ğŸ|ğŸš‘|ğŸ“±|ğŸ’¨|ğŸ¥˜|ğŸ˜†|ğŸ˜Ÿ|ğŸ|ğŸª|ğŸ“ˆ|ğŸ’£|ğŸ¥|ğŸ”©|ğŸŒ³|ğŸ”²|ğŸ¤´|ğŸ’|ğŸŸ|ğŸ˜|ğŸ¤¥|ğŸ˜|ğŸ’‘|ğŸ|ğŸ•³|âœ|ğŸ‘§|â™‚|ğŸ’Š|ğŸ˜¹|ğŸš‡|ğŸ™|ğŸ¡|â|ğŸ…¿|ğŸ’™|ğŸ•Œ|â†—|ğŸ’…|ğŸ•¤|ğŸ·|ğŸ”†|ğŸ—|ğŸ’¯|ğŸ“§|â•|ğŸ˜‰|ğŸ•£|ğŸŒ½|â™¥|ğŸ—|ğŸ­|â˜ƒ|ğŸ…|ğŸš–|ğŸ•–|â›„|ğŸ‡¿|ğŸŒ|ğŸ|ğŸŒ¤|âœ|âš¡|ğŸ–±|ğŸ¿|â™€|ğŸ•¸|ğŸ|ğŸ«|ğŸ‘¢|ğŸ™ˆ|ğŸ˜¤|ğŸŒ§|ğŸ’‚|ğŸ˜®|â›½|ğŸ˜µ|ğŸ¨|ğŸ™|ğŸ•|ğŸ˜¥|â™‰|ğŸ”•|ğŸšª|ğŸ’ª|ğŸ–|ğŸ“¿|ğŸ“º|â™Š|ğŸ”¬|ğŸ¯|ğŸ|ğŸˆ¯|ğŸ|ğŸ˜±|ğŸ•´|â¯|ğŸ›‹|ğŸ£|âœ–|âŒš|ğŸ”ˆ|â˜¸|ğŸ¹|ğŸŒ|ğŸ“Ÿ|â›“|ğŸ›|ğŸ¢|ğŸ”|ğŸ»|âš’|ğŸš„|ğŸ˜™|ğŸ˜’|ğŸ‡¨|ğŸ”|ğŸ‘¹|ğŸ‘™|ğŸ³|ğŸˆ²|ğŸ›|ğŸ½|âº|âš›|ğŸ¿|ğŸŒ–|ğŸ±|ğŸ¦†|ğŸ“™|â›³|â›¸|ğŸ†”|ğŸŒ¾|ğŸ‡±|ğŸ˜š|ğŸ—|âšª|ğŸš|ğŸ¹|ğŸŒš|ğŸ—¯|ğŸ‡«|ğŸšš|ğŸ“¤|ğŸ°|ğŸ’ƒ|âœ’|ğŸ“”|ğŸ”ª|ğŸ’¤|ğŸˆ|ğŸª|ğŸ”¦|ğŸ—„|â„¹|ğŸ›|âœŒ|ğŸ´|ğŸ |ğŸš¢|â°|ğŸŒ€|ğŸ |ğŸ¦Œ|â™’|ğŸ’§|ğŸ•œ|ğŸ˜ |ğŸ¤³|ğŸ…±|ğŸŒ¸|ğŸ‘·|ğŸ–²|ğŸ˜|ğŸ¥š|ğŸš­|â›ˆ|ğŸ¥|ğŸ“|ğŸŒ©|ğŸ³|ğŸŒ»|ğŸ•’|ğŸŒ˜|ğŸ•š|â™‘|ğŸ’–|ğŸ˜“|ğŸ“‹|8|ğŸ—’|â˜¹|ğŸ”¡|ğŸš’|ğŸ†˜|ğŸ|â›·|ğŸ’¸|ğŸª|ğŸ‘”|ğŸ¦|ğŸµ|ğŸ“|ğŸ‡|ğŸ…|ğŸ”ƒ|ğŸ˜˜|ğŸ¥”|ğŸ¤“|ğŸ|ğŸ’|âš”|ğŸ¦|ğŸ¤ |ğŸ˜€|â£|ğŸ”—|ğŸ”š|ğŸ––|ğŸ‡¦|ğŸ˜§|ğŸ|ğŸ|ğŸ…|ğŸŒ‰|ğŸ‘›|ğŸ·|ğŸŒŠ|ğŸ‰|ğŸœ|ğŸ•|ğŸ‡|ğŸ‡³|â˜ |ğŸ’¡|ğŸš¯|ğŸœ|ğŸ·|ğŸƒ|â›©|âš½|ğŸšˆ|ğŸ‡¹|â›|ğŸ´|ğŸ›ƒ|ğŸ„|ğŸ‘•|âš—|ğŸ–¨|ğŸŒ­|â›µ|ğŸ›£|â¬†|â˜®|ğŸ”«|ğŸ“€|ğŸ¤§|ğŸ§|ğŸ¤¶|â›…|ğŸ˜…|â«|â˜”|ğŸŒ•|â†”|ğŸ›|ğŸ• |ğŸ’„|ğŸ’¹|ğŸ›‚|ğŸŠ|ğŸŒ|ğŸ’|â†©|ğŸ”œ|â–¶|ğŸ˜—|ğŸ–Š|ğŸ“|ğŸ’‹|ğŸŒœ|ğŸ””|ğŸ”µ|ğŸ•”|ğŸ’“|ğŸ˜‹|ğŸ”’|ğŸ¥—|ğŸ›|ğŸ¥‚|ğŸ“®|ğŸ¥œ|ğŸ¾|ğŸ“¸|ğŸ’Œ|ğŸ˜ª|ğŸ“¯|ğŸœ|ğŸ’µ|ğŸˆ¹|ğŸ¬|ğŸ©|ğŸ‰|ğŸ’‡|ğŸ¸|ğŸ¥|ğŸ‘„|â‡|ğŸš˜|ğŸ†“|ğŸ‘¿|ğŸ•µ|ğŸ“­|ğŸˆ|ğŸ¥“|ğŸ“|ğŸš½|âš«|ğŸ•¦|ğŸ‡§|âœ‰|ğŸ“°|ğŸ¦‹|ğŸ˜|ğŸš¾|ğŸ–|ğŸ•¹|ğŸ“«|ğŸ•Š|ğŸ’²|ğŸ°|ğŸš¨|â˜¯|ğŸ¡|â˜˜|ğŸ’»|ğŸ”·|ğŸ¥|ğŸ”|ğŸ›¢|â„|âœ…|â€¼|ğŸ‘—|ğŸ‘³|â¿|ğŸ¥‹|â¤µ|ğŸ›µ|â–|ğŸ“†|ğŸ™|ğŸª|ğŸ‘|ğŸ’˜|ğŸ’®|ğŸ›©|â‰|ğŸ˜‘|ğŸš¹|ğŸ’°|ğŸ|ãŠ—|ğŸ›¥|âœ”|ğŸ©|ğŸ’¦|ğŸ¤|ğŸ•|ğŸ“²|ğŸŒ|ğŸ¤|ğŸ’œ|ğŸ™Š|ğŸ‘‹|ğŸ¸|ğŸ’­|ğŸ“¥|ğŸ›¶|âœ‹|ğŸ’€|ğŸ³|ğŸ“½|ğŸ¤–|â†•|ğŸ‘†|ğŸ—‚|ğŸš¼|â¤|ğŸŒ|ğŸ†š|ğŸ“|3|ğŸ“‰|ğŸš‚|ğŸ‹|ğŸ˜ˆ|ğŸŒ‡|â˜|ğŸ²|ğŸ„|ğŸ”¨|ğŸ”´|ğŸ¤¡|ğŸ¥‘|ï¸|ğŸ™„|ğŸ‘¯|ğŸ…¾|ğŸ¤¾|â™ˆ|ğŸ••|â¬|â—½|â°|ğŸ¼|ğŸ’«|ğŸš|â“|â†–|ğŸ”Œ|ğŸš±|âœ¡|ğŸ‘¬|ğŸ”…|ğŸ–|ğŸ”‡|ğŸ”‹|ğŸ‘‚|ğŸ¥|ğŸ›‘|ğŸº|ğŸŒ“|ğŸ¡|1|ğŸ—|ğŸ¦‘|ğŸ˜º|â™¿|âŒ›|ğŸŒƒ|ğŸŒˆ|ğŸ¬|ğŸ‘«|ğŸš|â“‚|ğŸŒ«|ğŸ˜›|âš°|ğŸ¦|âœ³|ğŸŒŒ|ğŸ½|ğŸ”­|ğŸ˜Š|ğŸš•|â¡|ğŸš|ğŸŒª|ğŸš“|ğŸŸ|ğŸš·|ğŸ»|ğŸ”‚|â™¨|ğŸš´|ğŸš©|ğŸ—£|ğŸ˜|ğŸµ|ğŸ¤|ğŸ¢|ğŸ”™|ğŸŒ¹|ğŸ’¶|â®|ğŸŒ†|ğŸ†|ğŸ”®|ğŸ“›|ğŸ¤º|â›¹|ğŸ”º|5|ğŸ˜¨|â™|ğŸ˜»|ğŸˆ¶|ğŸ› |âŒ|âš•|ğŸ„|ğŸšœ|ğŸ¤µ|ğŸ‘“|ğŸš|ğŸˆ|ğŸ†—|ğŸ‘¸|ğŸ“»|ğŸ˜„|ğŸ”|â—»|ğŸ¥–|ğŸ€„|ğŸ‘®|ğŸ”»|ğŸŒ›|â›°|ğŸ’|ğŸ‘»|ğŸš|ğŸš¤|â¸|ğŸ’•|â©|ğŸ˜¶|ğŸ½|ğŸ›«|âš |ğŸ“˜|ğŸ¦€|ğŸ’Ÿ|ğŸŒ—|ğŸ‘|ğŸ¤˜|4|â„¢|ğŸ›¤|ğŸ˜°|ğŸ‘Ÿ|ğŸ•¡|ğŸ“|ğŸ“‘|ğŸ˜‚|ğŸ|ğŸŒ‘|â›´|ğŸ±|ğŸš¿|â˜‚|ğŸ¯|â¤´|ğŸ“•|ğŸ¦|â›²|ğŸš°|ğŸš›|ğŸ‡©|ğŸ¤¦|ğŸ‘|ğŸ¦|ğŸˆµ|ğŸ•|âƒ£|ğŸ›|ğŸ¸|ğŸŒ‹|ğŸ¥|ğŸ˜¯|ğŸ“¡|ğŸ’½|ğŸ‹|ğŸ‘’|ğŸ“|ğŸ—|ğŸ“¦|ğŸ”€|ğŸšº|âš–|ğŸ‘¤|ğŸ¦…|âœ|ğŸš‰|ğŸ‘°|ğŸ’|Â©|ğŸ“¶|ğŸ¥’|ğŸ|ğŸ•|ğŸ“’|ğŸ•º|ğŸ“š|ğŸ”|ğŸ•˜|ğŸ•™|â—|9|ğŸ|ğŸ¦‡|ğŸ›Œ|ğŸ‡º|ğŸ›¡|ğŸŒ”|ğŸƒ|ğŸ‡²|ğŸ˜|ğŸ€|ğŸµ|ğŸ’›|ğŸ¥|ğŸ‘º|ğŸ‘˜|ğŸ¦‚|Â®|ğŸš²|ğŸ—¡|ğŸŸ|ğŸ“—|ğŸ˜|ğŸ‡¾|ğŸ‘¦|ğŸ–‹|ğŸš®|ğŸ¦‰|ğŸ|ğŸ’|â¬œ|ğŸ˜¬|ğŸ‘­|ğŸ|ğŸ™…|ğŸŒ¿|ğŸ°|ğŸ•›|ğŸ‘Œ|ğŸ‘©|ğŸ¤¤|ğŸ†‘|âª|ğŸ‘ƒ|ğŸ˜¡|ğŸ•|ğŸ›€|ğŸ®|ğŸŒ¶|ğŸ‚|ğŸ•‘|ğŸ™€|ğŸ¹|ğŸ’š|ğŸ˜¦|ğŸ„|ğŸ“¼|ğŸ¦|ğŸ‘µ|ğŸ’¢|ğŸ”˜|ğŸ’¥|ğŸš¸|ğŸ”‰|ğŸŠ|ğŸƒ|ğŸ­|â²|â³|ğŸ³|ğŸ“|ğŸ‘¥|ğŸ–‡|ğŸ•—|ğŸ’—|ğŸ“·|ğŸ†–|ğŸ‘¼|âš¾|ğŸ•‰|ğŸ£|ğŸ˜•|ğŸ¤°|ğŸ•¯|ğŸ¥•|ğŸ’±|ğŸŠ|ğŸŒ|ğŸ |ğŸ‘½|ğŸ¬|ğŸ¼|ğŸ‘…|ğŸ›´|ğŸŒ±|ğŸ‡|â—€|âš“|ğŸ£|ğŸš£|â™‹|ğŸš¶|ğŸ¤”|ğŸ™ƒ|â–«|ğŸ§|ã€°|ğŸ¶|ğŸˆ·|ğŸ“ª|ğŸ›’|â™|ğŸ‡¯|ğŸ˜Œ|ğŸš|ğŸ‘|ğŸ“¨|ğŸ•“|ğŸ¤½|ğŸš |ğŸ†|ğŸ¤š|ğŸ”|ğŸ‹|ğŸ›¬|ğŸŒ|ğŸ“„|ğŸ“–|ğŸ”Ÿ|ğŸ–•|ğŸ˜­|ğŸ‘€|ğŸ¤£|ğŸ‰|ğŸ¿|ğŸŒ |ğŸ”±|ğŸŒ·|ğŸ¥™|ğŸ’|ğŸ‘ª|ğŸ­|ğŸ¯|ğŸ—¨|ğŸ’¾|ğŸ” |ğŸ™Œ|ğŸ”¼|ğŸ¤|ğŸ‘|ğŸ™†|ğŸš|ğŸ›|ğŸ™‰|ğŸ‡·|ğŸ˜²|ğŸ¨|ğŸš§|ğŸ“|ğŸ“µ|â™“|ğŸ²|ğŸ”–|â™¦|ğŸ˜¾|ğŸ’ |ğŸ–¤|ğŸ”¶|â˜ª|ğŸ¼|ğŸŒ¯|ğŸ‘¡|ğŸ”|â–ª|ğŸ†|ğŸ‡¬|ğŸ»|ğŸ•¥|ğŸ“‚|ğŸŒŸ|ğŸ|ğŸŒ¡|#|ğŸ‡°|ğŸ—ƒ|âŒ¨|ğŸ˜¼|ğŸ…|ğŸ‡ª|ğŸ’ˆ|ğŸ§|ğŸ«|ğŸŒ¬|ğŸšŠ|ğŸ|âœ¨|ğŸ’³|ğŸ”§|â•|â—|âœˆ|ğŸ»|ğŸ|ğŸ¤¢|ğŸ¤¼|ğŸš«|ğŸ–|â˜º|â€|ğŸ—¿|ğŸ¾|ğŸˆ¸|0|ğŸ¥‰|ğŸ‘¨|ğŸ¤™|ğŸ›…|ğŸ­|ğŸ¢|â™ |ğŸ¥ˆ|ğŸ¥‡|ğŸ‡¶|ğŸ—œ|ğŸ’¬|ğŸ™‡|ğŸ‡µ|ğŸ”³|ğŸŒ„|ğŸ¤|ğŸ¤•|ğŸ”‘|ğŸ¤‘|ğŸšƒ|ğŸš…|ğŸ›|ğŸŠ|ğŸ¹|ğŸ|ğŸ‰|ğŸ¶|ğŸ”›|ğŸ”¤|ğŸ˜œ|ğŸ™|ğŸ–¥|ğŸ¥Š|ğŸ”|â†˜|ğŸ•°|ğŸ’´|ğŸ˜¿|ğŸ’|ğŸ™‚|â¬›|ğŸ‘±|ğŸ™‹|ğŸ¥€|ğŸ‡¼|ğŸƒ|ğŸ©|ğŸ—½|ğŸ“¬|ğŸ’º|ğŸ€|ğŸ‰|ğŸŸ|ğŸ‡»|ğŸº|ğŸ¯|ğŸ¥|ğŸ“ƒ|ğŸ•¢|â›º|ğŸŒ®|ğŸ£|ğŸŒ|ğŸ˜ƒ|ğŸ‰‘|2|ğŸ†’|ğŸŒ¼|ğŸ’†|ğŸ¤œ|ğŸ‘|ğŸ•|â—¼|â™£|ğŸ‚|ğŸ˜´|ğŸŒº|â˜£|â†™|ğŸ«|ğŸš»|ğŸ€|ğŸ¥…|ğŸ|ğŸ¥„|ğŸ¤—|ğŸš¥|ğŸ”„|ğŸ€|â­|ğŸŒ¥|ğŸ“ |ğŸ˜¸|ğŸ–Œ|ğŸ“´|ğŸ’‰|ğŸ‘|ğŸ’|ğŸ¼|â˜€|ğŸŒ|ğŸ“|ğŸš¡|ğŸˆ|â˜¦|ğŸ‘|ğŸ¦ˆ|â˜|ğŸ…°|â¬‡|ğŸ‘¶|ğŸŒ…|7|ğŸ¢|â±|ğŸ˜”|ğŸˆš|ğŸˆ|ğŸ¤|ğŸš”|ğŸ“©|ğŸ“³|ğŸ‚|â˜•|ğŸ½|ğŸŒ|ğŸ‘´|ğŸµ|ğŸ®|ğŸ”¥|ğŸ|â™|ğŸ¾|ğŸ†•|ğŸ˜|ğŸ™|ğŸ¸|ğŸŒ²|ğŸ¦|ğŸ¥ƒ|ğŸ‡¸|â˜„|ğŸ”|ğŸ¤›|ğŸšŸ|ğŸ¡|ğŸš|â›”|â™Œ|ğŸ‘‡|ğŸ“|ğŸ¦„|ğŸ˜¢|ğŸ’·|ğŸ‘|ğŸ‘‰|â™|ğŸ”¢|ğŸ“‡|ğŸ‘‘|ğŸ’|ğŸ›³|ğŸš¬|ğŸšµ|ğŸ¤·|ğŸš—|ğŸ¿|ğŸ“Œ|ğŸ““|â›ª|â—¾|ğŸ|ğŸ•·|ğŸ’’|ğŸ‘š|ğŸ¨|ğŸŒµ|ğŸŒ´|ğŸ•§|ğŸ“¢|ğŸ’©|ğŸ‘£|ğŸ˜–|ğŸ®|ğŸ—“|âœŠ|ğŸ‡­|ğŸ‘œ|ğŸ–|ğŸ¥›|ğŸˆ´|ğŸ”¯|ğŸ“£|ğŸ§|ğŸ®|ğŸ”|ğŸˆº|ğŸ—¼|â˜‘|â¬…|â›|ğŸ—|ğŸ˜·|ğŸŒ™|ğŸš|6|â­|ğŸ|ğŸ•|ğŸŒ’|ğŸ—³|ğŸ—¾|ğŸ–|â™»|ğŸš¦|ğŸ˜½|ğŸ|ğŸ¤¹|ğŸš|ğŸ”¸|ğŸš™|ğŸ¨|ğŸ”°|ğŸ“Š|ğŸš³|ğŸ|ğŸ‘ˆ|ğŸ™|ğŸ¦|ğŸ—º|â­•|ğŸ‘²|â¹|ğŸ·|â˜¢|ğŸ“œ|ğŸ‡½|ğŸ¦ƒ|â›±|ğŸ›„|ğŸ‡´|ğŸŒ|ğŸ§€|ğŸ¬|ğŸ‘ |â†ª|ã€½|ğŸš€|ğŸ¤|ğŸ”Š|ğŸ•¶|ğŸ›|ğŸ¾|ğŸ†™|ğŸ²|ğŸƒ|ğŸ¦|ğŸ’¿|ğŸ‘¾|ğŸ”“|ğŸ•Ÿ|âš™|ğŸ˜«|ğŸ‡|ğŸ‡®|â|ğŸ†|ğŸ¤’|ğŸ´|ğŸ |ğŸ—»|ğŸ™|ğŸ´|â›‘|ğŸº|â˜|ğŸ˜|ğŸ”¹|ğŸ“¹|ğŸ”½|ğŸ«|ğŸ˜³|âš±|âšœ|âœ´|âœ‚|ğŸšŒ|ğŸ’¼|ğŸ±|ğŸˆ‚|ğŸ˜£|ğŸ™'

    text = re.sub(emo_regex, ' ', text)

    print("6")

    # remove stop_words last

    # IMPT: split by thick quotes " " to keep newlines \n

    text_split_keep_newline = text.split(" ")

    text_split_keep_newline = [w for w in text_split_keep_newline if w != ''] # IMPT: remove spaces '' when split

    stop_words = get_stop_words('en') # get_stop_words more comprehensive than nltk

    nltk_words = stopwords.words('english')

    stop_words.extend(nltk_words)

    stop_words = list(set(stop_words)) 

    text = ' '.join([w for w in text_split_keep_newline if w not in stop_words])

    print("7")

    # remove extra spacing behind '\n' caused by ' '.join above 

    text = re.sub('(?<=\S) \n', '\n', text) # IMPT: adding (?<=\S) positive look-behind (<) does two things: 

                                            # (i) matches end of sentence char before ' \n' but does not replace that char  

                                            # (ii) stops matching start of an empty space sentence

    print("8")

    

    # DONE!

    print("DONE!")

    return text
# fast clean one long string

start = time.time() 

text_one_cleaned = fast_clean_text_keep_newline(text_one_long)

end = time.time()

print("Fast clean took: %s seconds" % (end-start))
# split preprocessed text back into individual tweets by newline + spacing '\n '

text_cleaned_split = text_one_cleaned.split('\n ') 



# check

text_cleaned_split 
# join back with original df 

df_cleaned = df

df_cleaned['text_cleaned'] = text_cleaned_split 



# check, DONE!

df_cleaned
# write out as csv

df_cleaned.to_csv('twcs_text_cleaned.csv')
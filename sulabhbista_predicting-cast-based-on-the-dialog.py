# import all the required packages

from IPython.display import display, HTML

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import scipy

import sqlite3

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_selection import chi2

from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import Normalizer
from sklearn.pipeline import make_pipeline

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import MultinomialNB

from sklearn.model_selection import cross_val_score

import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
# connect to a sqlite database
con = sqlite3.connect("../input/fraiser.db")


# list all files (for figuring out the location of the sqlite file)

#import os, operator, sys
#dirpath = os.path.join(os.getcwd(), '..')
#all_files = [ os.path.join(basedir, filename) for basedir, dirs, files in os.walk(dirpath) for filename in files   ]
#print("\n".join(all_files))

# query the database to read dialog data
script = pd.read_sql_query("SELECT * FROM script;", con)

# We are only interested on the data of main characters. Also, the original data is not 100% clean. There are some irregularities in name of cast and etc.
cast_of_interest = ["Daphne", "Frasier", "Martin", "Niles", "Roz"]
script = script[script.cast.isin(cast_of_interest)]

# assign numerical values for casts
script['cast_id'] = script.cast.factorize()[0]
script.head()
# data structures to be used later to map cast to cast_id and vice versa
cast_id_df = script[['cast', 'cast_id']].drop_duplicates().sort_values('cast_id')
cast_to_id = dict(cast_id_df.values)
id_to_cast = dict(cast_id_df[['cast_id', 'cast']].values)
script.groupby('cast').cast.count().plot.bar()
%%time
# run tf-idf on our text data
tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', ngram_range=(1, 2), stop_words='english')
features = tfidf.fit_transform(script.dialog).toarray()
labels = script.cast_id

# number of features generated by tf-idf
print(len(features))
%%time
most_correlated = []
number_of_ngrams = 5
for cast, cast_id in sorted(cast_to_id.items()):
    # data structure to collect words for casts
    ngrams = {
       "cast": cast,
        1: [],
        2: []
    }
    
    # chi-square tests
    features_chi2 = chi2(features, labels == cast_id)
    indices = np.argsort(features_chi2[0])
    feature_names = np.array(tfidf.get_feature_names())[indices]
    
    # pick the features ie words/ngrams
    for v in feature_names:
        length = len(v.split(' '))
        if length in ngrams and len(ngrams[length]) <= number_of_ngrams:
            ngrams[length].append(v)
    
    most_correlated.append(ngrams)
    
most_correlated_df = pd.DataFrame(most_correlated)
display(most_correlated_df)
for index, cast in enumerate(cast_of_interest):
    print("{}:".format(cast))
    print("\t 1-gram: {}".format(", ".join(most_correlated_df.loc[:,1][index])))
    print("\t 2-gram: {}".format(", ".join(most_correlated_df.loc[:,2][index])))
%%time
from sklearn.manifold import TSNE

# we run TSNE only on a subset of features because TSNE is computationally expensive
SAMPLE_SIZE = 500

np.random.seed(0)
indices = np.random.choice(range(len(features)), size=SAMPLE_SIZE, replace=False)

# reduce to 2 features
projected_features_tsne = TSNE(
    n_components=2,
    random_state=0
).fit_transform(features[indices])
# function for ploptting the scatter-plot from projected features
def scatter_plot_features(projected_features):
    colors = ['pink', 'green', 'midnightblue', 'orange', 'darkgrey']
    for cast, cast_id in sorted(cast_to_id.items()):
        points = projected_features[(labels[indices] == cast_id).values]
        plt.scatter(
            points[:, 0],
            points[:, 1],
            s=30,
            c=colors[cast_id],
            label=cast
        )

    plt.title("tf-idf feature vector for each article, projected on 2 dimensions")
    plt.legend()
    
scatter_plot_features(projected_features_tsne)
%%time
svd = TruncatedSVD(
    n_components=2,
    random_state=0
)
normalizer = Normalizer(copy=False)
lsa = make_pipeline(svd, normalizer)
projected_features_svd = lsa.fit_transform(features[indices])

scatter_plot_features(projected_features_svd)
%%time
# list of models we are using
models = [
    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),
    MultinomialNB(),
    LogisticRegression(random_state=0),
]

# number of k-folds
number_of_folds = 5
cross_validations_df = pd.DataFrame(index=range(number_of_folds * len(models)))
entries = []

for model in models:
  model_name = model.__class__.__name__
  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=number_of_folds)
  for fold_idx, accuracy in enumerate(accuracies):
    entries.append((model_name, fold_idx, accuracy))
    
cross_validations_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])
display(cross_validations_df)
sns.boxplot(
    x='model_name',
    y='accuracy',
    data=cross_validations_df
)
cross_validations_df.groupby('model_name').accuracy.mean()
%%time
# our winning model
model = LogisticRegression(random_state=0)

# split our data on train/test set
X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(
    features,
    labels,
    script.index,
    test_size=0.33,
    random_state=0
)

# fit and predict using the model
model.fit(X_train, y_train)
y_pred_proba = model.predict_proba(X_test)
y_pred = model.predict(X_test)
# visualizing confusion matrix
conf_mat = confusion_matrix(y_test, y_pred)
sns.heatmap(
    conf_mat,
    annot=True,
    fmt='d',    
    xticklabels=cast_id_df.cast.values,
    yticklabels=cast_id_df.cast.values
)
plt.ylabel('Actual')
plt.xlabel('Predicted')
for predicted in cast_id_df.cast_id:
    count = 0
    for actual in cast_id_df.cast_id:
        if predicted != actual and conf_mat[actual, predicted] >= 2:        
            print(
                "'{}' predicted as '{}'".format(
                    id_to_cast[actual],
                    id_to_cast[predicted],
                )
            )
            print("-" * 10)
            print("\n".join(script.loc[indices_test[(y_test == actual) & (y_pred == predicted)]].head().dialog.tolist()))
            print("\n")
model.fit(features, labels)
number_of_words = 5
for cast, cast_id in sorted(cast_to_id.items()):
  indices = np.argsort(model.coef_[cast_id])
  feature_names = np.array(tfidf.get_feature_names())[indices]
  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:number_of_words]
  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:number_of_words]
  print("# '{}':".format(cast))
  print("  . Top unigrams:\n       . {}".format('\n       . '.join(unigrams)))
  print("  . Top bigrams:\n       . {}".format('\n       . '.join(bigrams)))
dialogs = [
    ("Frasier", """Yes, hello, Miss Paxton.  I'm so sorry to have kept you 
         waiting.  Well, thank you.  I'm very flattered that you listen 
         to my little show.  Yes, well, yes, I meant every word.  Yes, 
         that's lovely, I'd like to meet you, too sometime. [sudden 
         thought] As a matter of fact, I'm having a few friends over 
         for a little gathering this Friday night, for cocktails and 
         such.  Well, I suppose you're far too busy to... you would!  
         Oh, that's marvelous.  Alright, that's the Elliot Bay towers 
         on the Counterbalance.  Around seven is just fine.  And, well, 
         I'll see then then.  Ciao! [hangs up]"""),
    ("Roz", """I look like crap - I've got a spot on my dress, I over-plucked 
         one eyebrow and the crotch of my pantyhose is creeping down to 
         my knees."""),
    ("Daphne", """ Well, my theory on death is: first you're whisked down a long
        dark tunnel towards a beautiful white light; you suddenly get 
        all the jokes you never got before, you let out a little 
        chuckle, and then you die!"""),
    ("Martin", """[as he returns from the kitchen] For God's sake, Frasier,
         you're forty-one years old.  It's time you learned something:  
         the system ain't perfect.  Sometimes the bad guy wins.  And 
         all those things you thought would be around to help you, 
         the courts and the police department?  Well, sometimes 
         they're just not there when you need them.  So you can either 
         let it eat a hole in your stomach, or you can just file it 
         away under the heading, "Sometimes Life Sucks." """),
    ("Niles", """Remember that day in junior high school when somebody took
         all my clothes while I was in the shower, right after gym
         class?  They hung them from the goalpost on the football 
         field.  I had no choice but to get a ladder and climb up 
         there wearing nothing but a towel, wet and shivering.  Then 
         the towel fell off!  There I was - your little brother, 
         hanging naked from a goal post, and everyone was standing 
         around laughing, and all Coach Medwick would do was stand 
         there going- [holds his arms up to imitate the gesture] 
         whatever that means."""),
    ("Roz", """Well, to be frank, Frasier, I don't spend my idle hours 
         imagining how you live.  But I did expect lots of beige 
         and, look, I was right."""),
    ("Frasier", """"Live with my work" - I love that phrase.  If you would, right 
         this way, please. [leads her towards the centre of the room] 
         I think this is the perfect spot for an ideal viewing.  Oh 
         God, I've waited so long for this moment - I'm just going to 
         stand back and let you describe your work - "Elegy in Green"â€”
         in your own words.  The way you insinuate the palette but 
         never lean on it, you capture the zeitgeist of our generation.  
         It is the most perfect canvas it has ever my privilege to gaze 
         upon.  I mean, one can only imagine what inspired you to paint 
         it."""),
    ("Daphne", """You know, I may be just a girl from Manchester but, I have to
         tell you, even though it's not a Paxton, I really like that 
         picture.  I liked it the minute I saw it.  I liked it even 
         before I knew who Martha Paxton was.  And quite frankly,
         I don't think that woman bathes."""),
    ("Frasier","""So that's that, huh?  Hayson just gets away with it.  He's 
         sitting there now with his brie and his wine and his little 
         chuckle at my expense.  Gosh, you know, I finally understand 
         why people take matters into their own hands.  It would be so 
         satisfying right now to just... slash his tires, or... throw 
         a brick though his window or something.  Just so he'd learn """),
    ("Niles", """Remember that day in junior high school when somebody took
         all my clothes while I was in the shower, right after gym
         class?  They hung them from the goalpost on the football 
         field.  I had no choice but to get a ladder and climb up 
         there wearing nothing but a towel, wet and shivering.  Then 
         the towel fell off!  There I was - your little brother, 
         hanging naked from a goal post, and everyone was standing 
         around laughing, and all Coach Medwick would do was stand 
         there going- [holds his arms up to imitate the gesture] 
         whatever that means."""),
    ("Frasier", """Well, Niles, if you were strong enough to show restraint
         after so much humiliation, not to mention the nicknames.""")
]

dialog_casts = [dialog[0] for dialog in dialogs]
dialog_texts = [dialog[1] for dialog in dialogs]

text_features = tfidf.transform(dialog_texts)
predictions = model.predict(text_features)
for text, predicted in zip(dialog_casts, predictions):
  print('"{}"'.format(text))
  print("  - Predicted as: '{}'".format(id_to_cast[predicted]))
  print("")
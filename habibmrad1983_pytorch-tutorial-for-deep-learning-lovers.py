# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import matplotlib.pyplot as plt

# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



import os

print(os.listdir("../input"))



# Any results you write to the current directory are saved as output.
# import numpy library

import numpy as np



# numpy array

array = [[1,2,3],[4,5,6]]

first_array = np.array(array) # 2x3 array

print("Array Type: {}".format(type(first_array))) # type

print("Array Shape: {}".format(np.shape(first_array))) # shape

print(first_array)
# import pytorch library

import torch



# pytorch array

tensor = torch.Tensor(array)

print("Array Type: {}".format(tensor.type)) # type

print("Array Shape: {}".format(tensor.shape)) # shape

print(tensor)
# numpy ones

print("Numpy {}\n".format(np.ones((2,3))))



# pytorch ones

print(torch.ones((2,3)))
# numpy random

print("Numpy {}\n".format(np.random.rand(2,3)))



# pytorch random

print(torch.rand(2,3))
# random numpy array

array = np.random.rand(2,2)

print("{} {}\n".format(type(array),array))



# from numpy to tensor

from_numpy_to_tensor = torch.from_numpy(array)

print("{}\n".format(from_numpy_to_tensor))



# from tensor to numpy

tensor = from_numpy_to_tensor

from_tensor_to_numpy = tensor.numpy()

print("{} {}\n".format(type(from_tensor_to_numpy),from_tensor_to_numpy))
# create tensor 

tensor = torch.ones(3,3)

print("\n",tensor)



# Resize

print("{}{}\n".format(tensor.view(9).shape,tensor.view(9)))



# Addition

print("Addition: {}\n".format(torch.add(tensor,tensor)))



# Subtraction

print("Subtraction: {}\n".format(tensor.sub(tensor)))



# Element wise multiplication

print("Element wise multiplication: {}\n".format(torch.mul(tensor,tensor)))



# Element wise division

print("Element wise division: {}\n".format(torch.div(tensor,tensor)))



# Mean

tensor = torch.Tensor([1,2,3,4,5])

print("Mean: {}".format(tensor.mean()))



# Standart deviation (std)

print("std: {}".format(tensor.std()))
# import variable from pytorch library

from torch.autograd import Variable



# define variable

var = Variable(torch.ones(3), requires_grad = True)

var
# lets make basic backward propagation

# we have an equation that is y = x^2

array = [2,4]

tensor = torch.Tensor(array)

x = Variable(tensor, requires_grad = True)

y = x**2

print(" y =  ",y)



# recap o equation o = 1/2*sum(y)

o = (1/2)*sum(y)

print(" o =  ",o)



# backward

o.backward() # calculates gradients



# As I defined, variables accumulates gradients. In this part there is only one variable x.

# Therefore variable x should be have gradients

# Lets look at gradients with x.grad

print("gradients: ",x.grad)
# As a car company we collect this data from previous selling

# lets define car prices

car_prices_array = [3,4,5,6,7,8,9]

car_price_np = np.array(car_prices_array,dtype=np.float32)

car_price_np = car_price_np.reshape(-1,1)

car_price_tensor = Variable(torch.from_numpy(car_price_np))



# lets define number of car sell

number_of_car_sell_array = [ 7.5, 7, 6.5, 6.0, 5.5, 5.0, 4.5]

number_of_car_sell_np = np.array(number_of_car_sell_array,dtype=np.float32)

number_of_car_sell_np = number_of_car_sell_np.reshape(-1,1)

number_of_car_sell_tensor = Variable(torch.from_numpy(number_of_car_sell_np))



# lets visualize our data

import matplotlib.pyplot as plt

plt.scatter(car_prices_array,number_of_car_sell_array)

plt.xlabel("Car Price $")

plt.ylabel("Number of Car Sell")

plt.title("Car Price$ VS Number of Car Sell")

plt.show()
# Linear Regression with Pytorch



# libraries

import torch      

from torch.autograd import Variable     

import torch.nn as nn 

import warnings

warnings.filterwarnings("ignore")



# https://www.w3schools.com/python/python_classes.asp



# create class

class LinearRegression(nn.Module):

    def __init__(self,input_size,output_size):

        # super function. It inherits from nn.Module and we can access everythink in nn.Module

        super(LinearRegression,self).__init__()

        # Linear function.

        self.linear = nn.Linear(input_dim,output_dim)



    def forward(self,x):

        return self.linear(x)

    

# define model

input_dim = 1

output_dim = 1

model = LinearRegression(input_dim,output_dim) # input and output size are 1



# MSE

mse = nn.MSELoss()



# Optimization (find parameters that minimize error)

learning_rate = 0.02   # how fast we reach best parameters

optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)



# train model

loss_list = []

iteration_number = 1001

for iteration in range(iteration_number):

        

    # optimization

    optimizer.zero_grad() 

    

    # Forward to get output

    results = model(car_price_tensor)

    

    # Calculate Loss

    loss = mse(results, number_of_car_sell_tensor)

    

    # backward propagation

    loss.backward()

    

    # Updating parameters

    optimizer.step()

    

    # store loss

    loss_list.append(loss.data)

    

    # print loss

    if(iteration % 50 == 0):

        print('epoch {}, loss {}'.format(iteration, loss.data))



plt.plot(range(iteration_number),loss_list)

plt.xlabel("Number of Iterations")

plt.ylabel("Loss")

plt.show()
# predict our car price 

predicted = model(car_price_tensor).data.numpy()

plt.scatter(car_prices_array,number_of_car_sell_array,label = "original data",color ="red")

plt.scatter(car_prices_array,predicted,label = "predicted data",color ="blue")



# predict if car price is 10$, what will be the number of car sell

#predicted_10 = model(torch.from_numpy(np.array([10]))).data.numpy()

#plt.scatter(10,predicted_10.data,label = "car price 10$",color ="green")

plt.legend()

plt.xlabel("Car Price $")

plt.ylabel("Number of Car Sell")

plt.title("Original vs Predicted values")

plt.show()
# Import Libraries

import torch

import torch.nn as nn

from torch.autograd import Variable

from torch.utils.data import DataLoader

#         - DataLoader(): It combines dataset and samples. It also provides multi process iterators over the dataset.



import pandas as pd

from sklearn.model_selection import train_test_split
# Prepare Dataset (Data prearation)



# load data

train = pd.read_csv(r"../input/train.csv",dtype = np.float32)



# split data into features(pixels) and labels(numbers from 0 to 9)

targets_numpy = train.label.values

features_numpy = train.loc[:,train.columns != "label"].values/255 # normalization



# train test split. Size of train data is 80% and size of test data is 20%. 

features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,

                                                                             targets_numpy,

                                                                             test_size = 0.2,

                                                                             random_state = 42) 



# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable

featuresTrain = torch.from_numpy(features_train)

targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long



# create feature and targets tensor for test set.

featuresTest = torch.from_numpy(features_test)

targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long



# batch_size, epoch and iteration

batch_size = 100

n_iters = 10000

num_epochs = n_iters / (len(features_train) / batch_size)

num_epochs = int(num_epochs)



# Pytorch train and test sets

train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)

test = torch.utils.data.TensorDataset(featuresTest,targetsTest)



# data loader

# https://pytorch.org/docs/stable/data.html

train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)

test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)



# visualize one of the images in data set

plt.imshow(features_numpy[10].reshape(28,28))

plt.axis("off")

plt.title(str(targets_numpy[10]))

plt.savefig('graph.png')

plt.show()
type(train)
train.tensors.count
train.tensors
# targets_numpy = train.label.values

targets_numpy
features_numpy
# Create Logistic Regression Model

class LogisticRegressionModel(nn.Module):

    def __init__(self, input_dim, output_dim):

        super(LogisticRegressionModel, self).__init__()

        # Linear part

        self.linear = nn.Linear(input_dim, output_dim)

        # There should be logistic function right?

        # However logistic function in pytorch is in loss function

        # So actually we do not forget to put it, it is only at next parts

    

    def forward(self, x):

        out = self.linear(x)

        return out



# Instantiate Model Class

input_dim = 28*28 # size of image px*px

output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9



# create logistic regression model

model = LogisticRegressionModel(input_dim, output_dim)



# Cross Entropy Loss  

error = nn.CrossEntropyLoss()



# SGD Optimizer 

learning_rate = 0.001

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
print(train_loader)
# Traning the Model



count = 0

loss_list = []

iteration_list = []

for epoch in range(num_epochs):

    for i, (images, labels) in enumerate(train_loader):

        

        # Define variables

        train = Variable(images.view(-1, 28*28))

        labels = Variable(labels)

        

        # Clear gradients

        optimizer.zero_grad()

        

        # Forward propagation

        outputs = model(train)

        

        # Calculate softmax and cross entropy loss

        loss = error(outputs, labels)

        

        # Calculate gradients

        loss.backward()

        

        # Update parameters

        optimizer.step()

        

        count += 1

        

        # Prediction

        if count % 50 == 0:

            # Calculate Accuracy         

            correct = 0

            total = 0

            # Predict test dataset

            for images, labels in test_loader: 

                test = Variable(images.view(-1, 28*28))

                

                # Forward propagation

                outputs = model(test)

                

                # Get predictions from the maximum value

                predicted = torch.max(outputs.data, 1)[1]

                

                # Total number of labels

                total += len(labels)

                

                # Total correct predictions

                correct += (predicted == labels).sum()

            

            accuracy = 100 * correct / float(total)

            

            # store loss and iteration

            loss_list.append(loss.data)

            iteration_list.append(count)

        if count % 500 == 0:

            # Print Loss

            print('Iteration: {}  Loss: {}  Accuracy: {}%'.format(count, loss.data, accuracy))
# visualization

plt.plot(iteration_list,loss_list)

plt.xlabel("Number of iteration")

plt.ylabel("Loss")

plt.title("Logistic Regression: Loss vs Number of iteration")

plt.show()
# Import Libraries

import torch

import torch.nn as nn

from torch.autograd import Variable
# Create ANN Model

class ANNModel(nn.Module):

    

    def __init__(self, input_dim, hidden_dim, output_dim):

        super(ANNModel, self).__init__()

        

        # Linear function 1: 784 --> 150

        self.fc1 = nn.Linear(input_dim, hidden_dim) 

        # Non-linearity 1

        self.relu1 = nn.ReLU()

        

        # Linear function 2: 150 --> 150

        self.fc2 = nn.Linear(hidden_dim, hidden_dim)

        # Non-linearity 2

        self.tanh2 = nn.Tanh()

        

        # Linear function 3: 150 --> 150

        self.fc3 = nn.Linear(hidden_dim, hidden_dim)

        # Non-linearity 3

        self.elu3 = nn.ELU()

        

        # Linear function 4 (readout): 150 --> 10

        self.fc4 = nn.Linear(hidden_dim, output_dim)  

    

    def forward(self, x):

        # Linear function 1

        out = self.fc1(x)

        # Non-linearity 1

        out = self.relu1(out)

        

        # Linear function 2

        out = self.fc2(out)

        # Non-linearity 2

        out = self.tanh2(out)

        

        # Linear function 2

        out = self.fc3(out)

        # Non-linearity 2

        out = self.elu3(out)

        

        # Linear function 4 (readout)

        out = self.fc4(out)

        return out



# instantiate ANN

input_dim = 28*28

hidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 

# there is no reason.

output_dim = 10



# Create ANN

model = ANNModel(input_dim, hidden_dim, output_dim)



# Cross Entropy Loss 

error = nn.CrossEntropyLoss()



# SGD Optimizer

learning_rate = 0.02

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
# ANN model training

count = 0

loss_list = []

iteration_list = []

accuracy_list = []

for epoch in range(num_epochs):

    for i, (images, labels) in enumerate(train_loader):



        train = Variable(images.view(-1, 28*28))

        labels = Variable(labels)

        

        # Clear gradients

        optimizer.zero_grad()

        

        # Forward propagation

        outputs = model(train)

        

        # Calculate softmax and ross entropy loss

        loss = error(outputs, labels)

        

        # Calculating gradients

        loss.backward()

        

        # Update parameters

        optimizer.step()

        

        count += 1

        

        if count % 50 == 0:

            # Calculate Accuracy         

            correct = 0

            total = 0

            # Predict test dataset

            for images, labels in test_loader:



                test = Variable(images.view(-1, 28*28))

                

                # Forward propagation

                outputs = model(test)

                

                # Get predictions from the maximum value

                predicted = torch.max(outputs.data, 1)[1]

                

                # Total number of labels

                total += len(labels)



                # Total correct predictions

                correct += (predicted == labels).sum()

            

            accuracy = 100 * correct / float(total)

            

            # store loss and iteration

            loss_list.append(loss.data)

            iteration_list.append(count)

            accuracy_list.append(accuracy)

        if count % 500 == 0:

            # Print Loss

            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))
# visualization loss 

plt.plot(iteration_list,loss_list)

plt.xlabel("Number of iteration")

plt.ylabel("Loss")

plt.title("ANN: Loss vs Number of iteration")

plt.show()



# visualization accuracy 

plt.plot(iteration_list,accuracy_list,color = "red")

plt.xlabel("Number of iteration")

plt.ylabel("Accuracy")

plt.title("ANN: Accuracy vs Number of iteration")

plt.show()
# Import Libraries

import torch

import torch.nn as nn

from torch.autograd import Variable
# Create CNN Model

class CNNModel(nn.Module):

    def __init__(self):

        super(CNNModel, self).__init__()

        

        # Convolution 1

        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)

        self.relu1 = nn.ReLU()

        

        # Max pool 1

        self.maxpool1 = nn.MaxPool2d(kernel_size=2)

     

        # Convolution 2

        self.cnn2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)

        self.relu2 = nn.ReLU()

        

        # Max pool 2

        self.maxpool2 = nn.MaxPool2d(kernel_size=2)

        

        # Fully connected 1

        self.fc1 = nn.Linear(32 * 4 * 4, 10) 

    

    def forward(self, x):

        # Convolution 1

        out = self.cnn1(x)

        out = self.relu1(out)

        

        # Max pool 1

        out = self.maxpool1(out)

        

        # Convolution 2 

        out = self.cnn2(out)

        out = self.relu2(out)

        

        # Max pool 2 

        out = self.maxpool2(out)

        

        # flatten

        out = out.view(out.size(0), -1)



        # Linear function (readout)

        out = self.fc1(out)

        

        return out



# batch_size, epoch and iteration

batch_size = 100

n_iters = 2500

num_epochs = n_iters / (len(features_train) / batch_size)

num_epochs = int(num_epochs)



# Pytorch train and test sets

train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)

test = torch.utils.data.TensorDataset(featuresTest,targetsTest)



# data loader

train_loader = torch.utils.data.DataLoader(train, batch_size = batch_size, shuffle = False)

test_loader = torch.utils.data.DataLoader(test, batch_size = batch_size, shuffle = False)

    

# Create CNN

model = CNNModel()



# Cross Entropy Loss 

error = nn.CrossEntropyLoss()



# SGD Optimizer

learning_rate = 0.1

optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# CNN model training

count = 0

loss_list = []

iteration_list = []

accuracy_list = []

for epoch in range(num_epochs):

    for i, (images, labels) in enumerate(train_loader):

        

        train = Variable(images.view(100,1,28,28))

        labels = Variable(labels)

        

        # Clear gradients

        optimizer.zero_grad()

        

        # Forward propagation

        outputs = model(train)

        

        # Calculate softmax and ross entropy loss

        loss = error(outputs, labels)

        

        # Calculating gradients

        loss.backward()

        

        # Update parameters

        optimizer.step()

        

        count += 1

        

        if count % 50 == 0:

            # Calculate Accuracy         

            correct = 0

            total = 0

            # Iterate through test dataset

            for images, labels in test_loader:

                

                test = Variable(images.view(100,1,28,28))

                

                # Forward propagation

                outputs = model(test)

                

                # Get predictions from the maximum value

                predicted = torch.max(outputs.data, 1)[1]

                

                # Total number of labels

                total += len(labels)

                

                correct += (predicted == labels).sum()

            

            accuracy = 100 * correct / float(total)

            

            # store loss and iteration

            loss_list.append(loss.data)

            iteration_list.append(count)

            accuracy_list.append(accuracy)

        if count % 500 == 0:

            # Print Loss

            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))
# visualization loss 

plt.plot(iteration_list,loss_list)

plt.xlabel("Number of iteration")

plt.ylabel("Loss")

plt.title("CNN: Loss vs Number of iteration")

plt.show()



# visualization accuracy 

plt.plot(iteration_list,accuracy_list,color = "red")

plt.xlabel("Number of iteration")

plt.ylabel("Accuracy")

plt.title("CNN: Accuracy vs Number of iteration")

plt.show()
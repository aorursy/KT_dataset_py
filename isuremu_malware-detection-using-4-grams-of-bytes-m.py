#imports

import pandas as pd

import json

import numpy as np

import keras

from keras.layers import Dense, Input

from keras.models import Model

from keras import regularizers
#Read the data

legit_grams = pd.read_csv('../input/cmpgramslegit/compressed_legit_min_max_grams.csv')

mals_grams = pd.read_csv('../input/cmpgramsmals/compressed_mals_min_max_grams.csv')



#Generate labels

legit_labels = np.zeros(legit_grams.shape[0])

mals_labels = np.ones(mals_grams.shape[0])
#Delete unsused columns

del legit_grams['Unnamed: 0']

del legit_grams['path']

del mals_grams['Unnamed: 0']

del mals_grams['path']
#Concat data

dataset = pd.concat([legit_grams, mals_grams], sort = False)

dataset_labels = np.concatenate([legit_labels, mals_labels])
#HyperParameters

input_dim = dataset.shape[1]

output_dim = 1

layers_configs = [[1024, 1024]]

training_algorithms = ['adam']

losses = ['mean_squared_error']

dropouts = [1]

regularization = ['dropout', 'early']

regularizers_v = [regularizers.l2(0.)]

hidden_activations = ['relu']

output_activations = ['sigmoid']

callbacks = []

metrics = ['accuracy']

initializers = ['random_normal']

epochs = 100

batch_size = 512
#return configuration for keras network

def configs_builder(input_dim, output_dim, layers_configs, training_algorithms, 

                    losses, dropouts, regularizers, 

                    hidden_activations, output_activations, 

                    callbacks, metrics, initializers):

    configs = []

    for layers_config in layers_configs:

        for training_algorithm in training_algorithms:

            for loss in losses:

                for dropout in dropouts:

                    for regularizer in regularizers_v:

                        for hidden_activation in hidden_activations:

                            for output_activation in output_activations:

                                for initializer in initializers:

                                    config = {

                                        'layers': {

                                            'dtype' : 'float32',

                                            'input dim': input_dim,

                                            'number': len(layers_config),

                                            'dims': layers_config,

                                            'names': ['dense'+str(x) for x in range(1, len(layers_config)+1, 1)],

                                            'initializers': [initializer for x in range(len(layers_config))],

                                            'bias initializers': [initializer for x in range(len(layers_config))],

                                            'activations': [hidden_activation for x in range(len(layers_config))],

                                            'kernel regulizers': [regularizer for x in range(len(layers_config))],

                                            'output dim': output_dim,

                                            'output activation' : output_activation,

                                            'ouput initializer' : initializer,

                                            'output bias initializer': initializer,

                                            'output regulizer': regularizer,

                                        },



                                        'compile': {

                                            'optimizer': training_algorithm,

                                            'metrics': metrics,

                                            'loss': loss,

                                            'callbacks': callbacks,

                                        },

                                    }

                                    configs.append(config)

    return configs
#build a feedforward network using the configuration

def feed_forward_builder(config):

    input_layer = Input(shape=(config['layers']['input dim'],), dtype = config['layers']['dtype'], name = 'InputLayer')

    if config['layers']['number'] == 0:

        print('No Hidden Layers')

    else:

        layer = Dense(config['layers']['dims'][0], 

                      dtype = config['layers']['dtype'], 

                      name = config['layers']['names'][0], 

                      activation = config['layers']['activations'][0], 

                      kernel_initializer = config['layers']['initializers'][0], 

                      bias_initializer = config['layers']['bias initializers'][0], 

                      kernel_regularizer = config['layers']['kernel regulizers'][0])(input_layer)

        for n in range(1,config['layers']['number'],1):

            layer =   Dense(config['layers']['dims'][n], 

                      dtype = config['layers']['dtype'], 

                      name = config['layers']['names'][n], 

                      activation = config['layers']['activations'][n], 

                      kernel_initializer = config['layers']['initializers'][n], 

                      bias_initializer = config['layers']['bias initializers'][n], 

                      kernel_regularizer = config['layers']['kernel regulizers'][n])(layer)

    output_layer = Dense(config['layers']['output dim'], 

                         dtype =  config['layers']['dtype'], 

                         name = 'OutputLayer', 

                         activation = config['layers']['output activation'], 

                         kernel_initializer = config['layers']['ouput initializer'], 

                         bias_initializer = config['layers']['output bias initializer'], 

                         kernel_regularizer = config['layers']['output regulizer'])(layer)

    fnn = Model(inputs = input_layer, outputs = output_layer)

    fnn.compile(optimizer = config['compile']['optimizer'], 

                metrics = config['compile']['metrics'], 

                loss = config['compile']['loss'])

    return fnn
#train

def fnn_trainer(model, params, data, labels):

    return model.fit(data, labels, validation_split = params['validation'], 

                        batch_size = params['batch_size'], epochs = params['epochs'])
#start trainer and return history of error

def fnn_evaluate(configs, data, labels):

    params = {

        'epochs' : 10,

        'batch_size' : 512,

        'validation': 0.15,

    }

    histories = []

    for config in configs:

        model = feed_forward_builder(config)

        model.summary()

        histories.append(fnn_trainer(model, params, data, labels))

    return histories, model
#Get the configuration for the network

configs = configs_builder(input_dim, output_dim, layers_configs, training_algorithms, 

                    losses, dropouts, regularizers, 

                    hidden_activations, output_activations, 

                    callbacks, metrics, initializers)
#Launch training 

histories, model = fnn_evaluate(configs, dataset, dataset_labels)
#print history

for h in histories:

    print(h.history)
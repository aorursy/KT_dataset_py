import numpy as np 

import pandas as pd 



import seaborn as sns

from matplotlib import pyplot as plt

sns.set_style("whitegrid")

%matplotlib inline



import warnings

warnings.filterwarnings("ignore")



import os 

print(os.listdir("../input"))
training = pd.read_csv("../input/train.csv")

testing = pd.read_csv("../input/test.csv")
training.head()
testing.head()
print(training.keys())

print(testing.keys())
types_train = training.dtypes

num_values = types_train[(types_train == float)]



print("These are the numerical features:")

print(num_values)
training.describe()
def null_table(training, testing):

    print("Training Data Frame")

    print(pd.isnull(training).sum()) 

    print(" ")

    print("Testing Data Frame")

    print(pd.isnull(testing).sum())



null_table(training, testing)
training.drop(labels = ["Cabin", "Ticket"], axis = 1, inplace = True)

testing.drop(labels = ["Cabin", "Ticket"], axis = 1, inplace = True)



null_table(training, testing)
copy = training.copy()

copy.dropna(inplace = True)

sns.distplot(copy["Age"])
#the median will be an acceptable value to place in the NaN cells

training["Age"].fillna(training["Age"].median(), inplace = True)

testing["Age"].fillna(testing["Age"].median(), inplace = True) 

training["Embarked"].fillna("S", inplace = True)

testing["Fare"].fillna(testing["Fare"].median(), inplace = True)



null_table(training, testing)
training.head()
testing.head()
#can ignore the testing set for now

sns.barplot(x="Sex", y="Survived", data=training)

plt.title("Distribution of Survival based on Gender")

plt.show()



total_survived_females = training[training.Sex == "female"]["Survived"].sum()

total_survived_males = training[training.Sex == "male"]["Survived"].sum()



print("Total people survived is: " + str((total_survived_females + total_survived_males)))

print("Proportion of Females who survived:") 

print(total_survived_females/(total_survived_females + total_survived_males))

print("Proportion of Males who survived:")

print(total_survived_males/(total_survived_females + total_survived_males))
sns.barplot(x="Pclass", y="Survived", data=training)

plt.ylabel("Survival Rate")

plt.title("Distribution of Survival Based on Class")

plt.show()



total_survived_one = training[training.Pclass == 1]["Survived"].sum()

total_survived_two = training[training.Pclass == 2]["Survived"].sum()

total_survived_three = training[training.Pclass == 3]["Survived"].sum()

total_survived_class = total_survived_one + total_survived_two + total_survived_three



print("Total people survived is: " + str(total_survived_class))

print("Proportion of Class 1 Passengers who survived:") 

print(total_survived_one/total_survived_class)

print("Proportion of Class 2 Passengers who survived:")

print(total_survived_two/total_survived_class)

print("Proportion of Class 3 Passengers who survived:")

print(total_survived_three/total_survived_class)
sns.barplot(x="Pclass", y="Survived", hue="Sex", data=training)

plt.ylabel("Survival Rate")

plt.title("Survival Rates Based on Gender and Class")
sns.barplot(x="Sex", y="Survived", hue="Pclass", data=training)

plt.ylabel("Survival Rate")

plt.title("Survival Rates Based on Gender and Class")
survived_ages = training[training.Survived == 1]["Age"]

not_survived_ages = training[training.Survived == 0]["Age"]

plt.subplot(1, 2, 1)

sns.distplot(survived_ages, kde=False)

plt.axis([0, 100, 0, 100])

plt.title("Survived")

plt.ylabel("Proportion")

plt.subplot(1, 2, 2)

sns.distplot(not_survived_ages, kde=False)

plt.axis([0, 100, 0, 100])

plt.title("Didn't Survive")

plt.subplots_adjust(right=1.7)

plt.show()
sns.stripplot(x="Survived", y="Age", data=training, jitter=True)
sns.pairplot(training)
training.sample(5)
testing.sample(5)
set(training["Embarked"])
from sklearn.preprocessing import LabelEncoder



le_sex = LabelEncoder()

le_sex.fit(training["Sex"])



encoded_sex_training = le_sex.transform(training["Sex"])

training["Sex"] = encoded_sex_training

encoded_sex_testing = le_sex.transform(testing["Sex"])

testing["Sex"] = encoded_sex_testing



le_embarked = LabelEncoder()

le_embarked.fit(training["Embarked"])



encoded_embarked_training = le_embarked.transform(training["Embarked"])

training["Embarked"] = encoded_embarked_training

encoded_embarked_testing = le_embarked.transform(testing["Embarked"])

testing["Embarked"] = encoded_embarked_testing



#Here's how to do it manually in Python without packages

"""

training.loc[training["Sex"] == "male", "Sex"] = 0

training.loc[training["Sex"] == "female", "Sex"] = 1



training.loc[training["Embarked"] == "S", "Embarked"] = 0

training.loc[training["Embarked"] == "C", "Embarked"] = 1

training.loc[training["Embarked"] == "Q", "Embarked"] = 2



testing.loc[testing["Sex"] == "male", "Sex"] = 0

testing.loc[testing["Sex"] == "female", "Sex"] = 1



testing.loc[testing["Embarked"] == "S", "Embarked"] = 0

testing.loc[testing["Embarked"] == "C", "Embarked"] = 1

testing.loc[testing["Embarked"] == "Q", "Embarked"] = 2

"""
training.sample(5)
testing.sample(5)
training["FamSize"] = training["SibSp"] + training["Parch"] + 1

testing["FamSize"] = testing["SibSp"] + testing["Parch"] + 1
training["IsAlone"] = training.FamSize.apply(lambda x: 1 if x == 1 else 0)

testing["IsAlone"] = testing.FamSize.apply(lambda x: 1 if x == 1 else 0)
for name in training["Name"]:

    training["Title"] = training["Name"].str.extract("([A-Za-z]+)\.",expand=True)

    

for name in testing["Name"]:

    testing["Title"] = testing["Name"].str.extract("([A-Za-z]+)\.",expand=True)
training.head() #Title column added
titles = set(training["Title"]) #making it a set gets rid of all duplicates

print(titles)
title_list = list(training["Title"])

frequency_titles = []



for i in titles:

    frequency_titles.append(title_list.count(i))

    

print(frequency_titles)
titles = list(titles)



title_dataframe = pd.DataFrame({

    "Titles" : titles,

    "Frequency" : frequency_titles

})



print(title_dataframe)
title_replacements = {"Mlle": "Other", "Major": "Other", "Col": "Other", "Sir": "Other", "Don": "Other", "Mme": "Other",

          "Jonkheer": "Other", "Lady": "Other", "Capt": "Other", "Countess": "Other", "Ms": "Other", "Dona": "Other"}



training.replace({"Title": title_replacements}, inplace=True)

testing.replace({"Title": title_replacements}, inplace=True)



le_title = LabelEncoder()

le_title.fit(training["Title"])



encoded_title_training = le_title.transform(training["Title"])

training["Title"] = encoded_title_training

encoded_title_testing = le_title.transform(testing["Title"])

testing["Title"] = encoded_title_testing



#Again, here's how to do it manually

"""

training.loc[training["Title"] == "Miss", "Title"] = 0

training.loc[training["Title"] == "Mr", "Title"] = 1

training.loc[training["Title"] == "Mrs", "Title"] = 2

training.loc[training["Title"] == "Master", "Title"] = 3

training.loc[training["Title"] == "Dr", "Title"] = 4

training.loc[training["Title"] == "Rev", "Title"] = 5

training.loc[training["Title"] == "Other", "Title"] = 6



testing.loc[testing["Title"] == "Miss", "Title"] = 0

testing.loc[testing["Title"] == "Mr", "Title"] = 1

testing.loc[testing["Title"] == "Mrs", "Title"] = 2

testing.loc[testing["Title"] == "Master", "Title"] = 3

testing.loc[testing["Title"] == "Dr", "Title"] = 4

testing.loc[testing["Title"] == "Rev", "Title"] = 5

testing.loc[testing["Title"] == "Other", "Title"] = 6

"""
training.drop("Name", axis = 1, inplace = True)

testing.drop("Name", axis = 1, inplace = True)
training.sample(5)
testing.sample(5)
from sklearn.preprocessing import StandardScaler



scaler = StandardScaler()



#We need to reshape our data since the Scaler takes in arrays

ages_train = np.array(training["Age"]).reshape(-1, 1)

fares_train = np.array(training["Fare"]).reshape(-1, 1)

ages_test = np.array(testing["Age"]).reshape(-1, 1)

fares_test = np.array(testing["Fare"]).reshape(-1, 1)



training["Age"] = scaler.fit_transform(ages_train)

training["Fare"] = scaler.fit_transform(fares_train)

testing["Age"] = scaler.fit_transform(ages_test)

testing["Fare"] = scaler.fit_transform(fares_test)



#You can try with MinMaxScaler as well to see how it performs in comparison, just replace StandardScaler with MinMaxScaler
training.head()
testing.head()
from sklearn.svm import SVC, LinearSVC

from sklearn.ensemble import RandomForestClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.neighbors import KNeighborsClassifier

from sklearn.naive_bayes import GaussianNB

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import make_scorer, accuracy_score 
from sklearn.model_selection import GridSearchCV
X_train = training.drop(labels=["PassengerId", "Survived"], axis=1) #define training features set

y_train = training["Survived"] #define training label set

X_test = testing.drop("PassengerId", axis=1) #define testing features set

#we don't have y_test, that is what we're trying to predict with our model
X_train.head()
from sklearn.model_selection import train_test_split #to create validation data set



X_training, X_valid, y_training, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0) #X_valid and y_valid are the validation sets
svc_clf = SVC() 



parameters_svc = {"kernel": ["rbf", "linear"], "probability": [True, False], "verbose": [True, False]}



grid_svc = GridSearchCV(svc_clf, parameters_svc, scoring=make_scorer(accuracy_score))

grid_svc.fit(X_training, y_training)



svc_clf = grid_svc.best_estimator_



svc_clf.fit(X_training, y_training)

pred_svc = svc_clf.predict(X_valid)

acc_svc = accuracy_score(y_valid, pred_svc)
print("The Score for SVC is: " + str(acc_svc))
linsvc_clf = LinearSVC()



parameters_linsvc = {"multi_class": ["ovr", "crammer_singer"], "fit_intercept": [True, False], "max_iter": [100, 500, 1000, 1500]}



grid_linsvc = GridSearchCV(linsvc_clf, parameters_linsvc, scoring=make_scorer(accuracy_score))

grid_linsvc.fit(X_training, y_training)



linsvc_clf = grid_linsvc.best_estimator_



linsvc_clf.fit(X_training, y_training)

pred_linsvc = linsvc_clf.predict(X_valid)

acc_linsvc = accuracy_score(y_valid, pred_linsvc)



print("The Score for LinearSVC is: " + str(acc_linsvc))
rf_clf = RandomForestClassifier()



parameters_rf = {"n_estimators": [4, 5, 6, 7, 8, 9, 10, 15], "criterion": ["gini", "entropy"], "max_features": ["auto", "sqrt", "log2"], 

                 "max_depth": [2, 3, 5, 10], "min_samples_split": [2, 3, 5, 10]}



grid_rf = GridSearchCV(rf_clf, parameters_rf, scoring=make_scorer(accuracy_score))

grid_rf.fit(X_training, y_training)



rf_clf = grid_rf.best_estimator_



rf_clf.fit(X_training, y_training)

pred_rf = rf_clf.predict(X_valid)

acc_rf = accuracy_score(y_valid, pred_rf)



print("The Score for Random Forest is: " + str(acc_rf))
logreg_clf = LogisticRegression()



parameters_logreg = {"penalty": ["l2"], "fit_intercept": [True, False], "solver": ["newton-cg", "lbfgs", "liblinear", "sag", "saga"],

                     "max_iter": [50, 100, 200], "warm_start": [True, False]}



grid_logreg = GridSearchCV(logreg_clf, parameters_logreg, scoring=make_scorer(accuracy_score))

grid_logreg.fit(X_training, y_training)



logreg_clf = grid_logreg.best_estimator_



logreg_clf.fit(X_training, y_training)

pred_logreg = logreg_clf.predict(X_valid)

acc_logreg = accuracy_score(y_valid, pred_logreg)



print("The Score for Logistic Regression is: " + str(acc_logreg))
knn_clf = KNeighborsClassifier()



parameters_knn = {"n_neighbors": [3, 5, 10, 15], "weights": ["uniform", "distance"], "algorithm": ["auto", "ball_tree", "kd_tree"],

                  "leaf_size": [20, 30, 50]}



grid_knn = GridSearchCV(knn_clf, parameters_knn, scoring=make_scorer(accuracy_score))

grid_knn.fit(X_training, y_training)



knn_clf = grid_knn.best_estimator_



knn_clf.fit(X_training, y_training)

pred_knn = knn_clf.predict(X_valid)

acc_knn = accuracy_score(y_valid, pred_knn)



print("The Score for KNeighbors is: " + str(acc_knn))
gnb_clf = GaussianNB()



parameters_gnb = {}



grid_gnb = GridSearchCV(gnb_clf, parameters_gnb, scoring=make_scorer(accuracy_score))

grid_gnb.fit(X_training, y_training)



gnb_clf = grid_gnb.best_estimator_



gnb_clf.fit(X_training, y_training)

pred_gnb = gnb_clf.predict(X_valid)

acc_gnb = accuracy_score(y_valid, pred_gnb)



print("The Score for Gaussian NB is: " + str(acc_gnb))
dt_clf = DecisionTreeClassifier()



parameters_dt = {"criterion": ["gini", "entropy"], "splitter": ["best", "random"], "max_features": ["auto", "sqrt", "log2"]}



grid_dt = GridSearchCV(dt_clf, parameters_dt, scoring=make_scorer(accuracy_score))

grid_dt.fit(X_training, y_training)



dt_clf = grid_dt.best_estimator_



dt_clf.fit(X_training, y_training)

pred_dt = dt_clf.predict(X_valid)

acc_dt = accuracy_score(y_valid, pred_dt)



print("The Score for Decision Tree is: " + str(acc_dt))
from xgboost import XGBClassifier



xg_clf = XGBClassifier()



parameters_xg = {"objective" : ["reg:linear"], "n_estimators" : [5, 10, 15, 20]}



grid_xg = GridSearchCV(xg_clf, parameters_xg, scoring=make_scorer(accuracy_score))

grid_xg.fit(X_training, y_training)



xg_clf = grid_xg.best_estimator_



xg_clf.fit(X_training, y_training)

pred_xg = xg_clf.predict(X_valid)

acc_xg = accuracy_score(y_valid, pred_xg)



print("The Score for XGBoost is: " + str(acc_xg))
model_performance = pd.DataFrame({

    "Model": ["SVC", "Linear SVC", "Random Forest", 

              "Logistic Regression", "K Nearest Neighbors", "Gaussian Naive Bayes",  

              "Decision Tree", "XGBClassifier"],

    "Accuracy": [acc_svc, acc_linsvc, acc_rf, 

              acc_logreg, acc_knn, acc_gnb, acc_dt, acc_xg]

})



model_performance.sort_values(by="Accuracy", ascending=False)
rf_clf.fit(X_train, y_train)
submission_predictions = rf_clf.predict(X_test)
submission = pd.DataFrame({

        "PassengerId": testing["PassengerId"],

        "Survived": submission_predictions

    })



submission.to_csv("titanic.csv", index=False)

print(submission.shape)
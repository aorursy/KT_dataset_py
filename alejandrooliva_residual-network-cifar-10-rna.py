#import needed classes
import keras
from keras.datasets import cifar10
from keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,AveragePooling2D,Dropout,BatchNormalization,Activation
from keras.models import Model,Input
from keras.optimizers import Adam
from keras.callbacks import LearningRateScheduler
from keras.callbacks import ModelCheckpoint
from math import ceil
import os
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler as LRS

import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
batch_size= 150
clases= 10
epochs= 50
#Función encargada de sumar la salida x a la salida de la última capa Conv2D
def Residual(x,filters,pool=False):
    #alamcenamos la salida de la anterior capa con la finalidad de conectarla
    #con la salida de la última capa Conv2D de la función
    res = x
    
    #Comprobar si se ha de realizar pooling, se realiza al principio de cada
    #bloque. Hay que tener en cuenta que el tamaño de la entrada y la salida ha
    #de ser el mismo.
    if pool:
        x = MaxPooling2D(pool_size=(2, 2))(x)
        res = Conv2D(filters=filters,kernel_size=[1,1],strides=(2,2),padding="same")(res)
    
    salida = BatchNormalization()(x)
    salida = Activation("relu")(salida)
    
    
    salida = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding="same")(salida)
    salida = BatchNormalization()(salida)
    salida = Activation("relu")(salida)
    
    salida = Conv2D(filters=filters, kernel_size=[3, 3], strides=[1, 1], padding="same")(salida)

    salida = keras.layers.add([res,salida])

    return salida


#Función para defenir el modelo
def Modelo(input_shape):
    #input shape
    images = Input(input_shape)
    #Bloque 1
    modelo = Conv2D(filters=32, kernel_size=[3, 3], strides=[1, 1], padding="same")(images)
    modelo = Residual(modelo,32)
    modelo = Residual(modelo,32)
    modelo = Residual(modelo,32)
    #Bloque 2
    modelo = Residual(modelo,64,pool=True)
    modelo = Residual(modelo,64)
    modelo = Residual(modelo,64)
    #Bloque 3
    modelo = Residual(modelo,128,pool=True)
    modelo = Residual(modelo,128)
    modelo = Residual(modelo,128)
    #Bloque 4
    modelo = Residual(modelo, 256,pool=True)
    modelo = Residual(modelo, 256)
    modelo = Residual(modelo, 256)
    modelo = BatchNormalization()(modelo)
    modelo = Activation("relu")(modelo)
    modelo = Dropout(0.25)(modelo)
    modelo = MaxPooling2D(pool_size=(4,4))(modelo)
    
    #Aplanar
    modelo = Flatten()(modelo)
    
    #Salida
    modelo = Dense(units=10,activation="softmax")(modelo)
    
    model = Model(images,modelo)

    return model

input_shape = (32,32,3)
model = Modelo(input_shape)


#load the cifar10 dataset
(train_x, train_y) , (test_x, test_y) = cifar10.load_data()

train_x = train_x.astype('float32') / 255
test_x = test_x.astype('float32') / 255

train_x = train_x - train_x.mean()
test_x = test_x - test_x.mean()

train_x = train_x / train_x.std(axis=0)
test_x = test_x / test_x.std(axis=0)

# Labels of classes
train_y = keras.utils.to_categorical(train_y, clases)
test_y = keras.utils.to_categorical(test_y,clases)

datagen = ImageDataGenerator(rotation_range=10,
                             width_shift_range=5. / 32,
                             height_shift_range=5. / 32,
                             horizontal_flip=True)

testdatagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True
)

datagen.fit(train_x)



model.summary()
lr=5e-4
opt = Adam(lr=lr, beta_1=0.9, beta_2=0.999)
model.compile(optimizer=opt ,loss="categorical_crossentropy",metrics=["accuracy"])
# DEFINE A LEARNING RATE SCHEDULER
def scheduler(epoch):
    if epoch < 40:
        return lr
    elif epoch < 80:
        return lr/2
    else:
        return lr/10

set_lr = LRS(scheduler)

# Fit the model on the batches generated by datagen.flow().
history=model.fit_generator(datagen.flow(train_x, train_y, batch_size=128),
                    validation_data=testdatagen.flow(test_x, test_y),
                    validation_steps=10000/128,
                    epochs=epochs,
                    steps_per_epoch=50000/128,
                    callbacks=[set_lr],
                    verbose=1,
                    workers=4)
accuracy = model.evaluate(x=test_x,y=test_y,batch_size=128)
scores = model.evaluate(test_x, test_y, verbose=1)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])

print(history.history.keys())
plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.savefig('val.png')
plt.show()

# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'validation'], loc='upper left')
plt.savefig('err.png')
plt.show()

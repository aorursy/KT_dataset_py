# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data= pd.read_csv('/kaggle/input/malware-analysis-datasets-top1000-pe-imports/top_1000_pe_imports.csv')
data.info()
data.columns.tolist()
pd.set_option('display.max_columns', None)
data.head(10)
innocent= data.loc[data['malware']==0]
innocent.head(10)

counts= data['malware'].value_counts()

print("Malicious Samples: ", counts[1])
print("Benign Samples: ", counts[0])
%matplotlib inline

labels=['Malicious', 'Benign']
sizes=[data['malware'].value_counts()[1],
     data['malware'].value_counts()[0]]
fig1, ax1 = plt.subplots()
ax1.pie(sizes, labels=labels,)
ax1.axis('equal')
plt.show()
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.feature_selection import SelectFromModel
y=data['malware']
x=data.drop(['malware','hash'],axis=1)
#dropping hash as it is a (mosty unique) character value, cannot be mapped into feature space.

#x.head()
#y.head()
model=ExtraTreesClassifier(n_estimators=50, random_state=15372)
#Setting random state, else predictions vary everytime
model.fit(x,y)
selections= SelectFromModel(model,prefit=True)
x_new=selections.transform(x)
print('Number of features: %d' %(x_new.shape[1]))
nb_features= x_new.shape[1]
indices = np.argsort(model.feature_importances_)[::-1][:nb_features]
for f in range(nb_features):
    number = f+1
    feature_name = ''.join(x.columns[indices[f]])
    feature_importance = model.feature_importances_[indices[f]]
    print('   %d.\t%s \t%f' % (number, feature_name, (feature_importance * 100)))
#Testing Purposes 
#print(indices)
#print(x.iloc[[1],[957]])
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
data= pd.read_csv('/kaggle/input/malware-analysis-datasets-top1000-pe-imports/top_1000_pe_imports.csv')
data=data.drop(['hash'],axis=1)
data.info() #dropping hash; to get all integer values
flag=0
for column in data.columns.tolist():
    if data[column].nunique()!=2:
        print(column+ " has more than 2 unique values")
        flag=1

if flag==0:
    print("All columns have 2 values only")
mal= data.loc[data['malware']==1]
benign= data.loc[data['malware']==0]
index= data.columns.tolist()
columns=['Benign=0', 'Benign=1', 'Malicious=0', 'Malicious=1', 'Difference']
df = pd.DataFrame(index=index, columns=columns)
df = df.fillna(0)
#df.head()
#Creating a dataframe with the percentage of benign and malicious samples corresponding to 0 and 1 respectively for each column

for column in data.columns.tolist():
    df.loc[[column],['Benign=1']]= np.round((benign[column].sum()/benign.shape[0]*100),2)
    df.loc[[column],['Benign=0']]= np.round(((benign.shape[0]-benign[column].sum())/benign.shape[0]*100),2)
    df.loc[[column],['Malicious=1']]= np.round((mal[column].sum()/mal.shape[0]*100),2)
    df.loc[[column],['Malicious=0']]= np.round(((mal.shape[0]-mal[column].sum())/mal.shape[0]*100),2)
    df.loc[[column],['Difference']]= abs(np.round((((benign[column].sum()/benign.shape[0])- (mal[column].sum()/mal.shape[0]))*100 ),2))
df.head()
# Sorting with respect to differences, to get the most different values

df=df.sort_values('Difference', ascending=False)
df=df.drop(['malware']) #Obviously, this column will be different
#df.head(20)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
data= pd.read_csv('/kaggle/input/malware-analysis-datasets-top1000-pe-imports/top_1000_pe_imports.csv')
def create_data(dataframe):
    y= dataframe['malware']
    X= dataframe.drop(['malware'], axis=1)
    print("X Shape: ", X.shape)
    print("Y Shape: ", y.shape)
    X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)
    
    return (X, y, X_train, X_test, y_train, y_test)
def evaluation(model, X_val, y_val, predictions):
    print("Confusion Matrix: \n", confusion_matrix(y_val, predictions))
    print("Precision:", precision_score(y_val, predictions))
    print("Recall:",recall_score(y_val, predictions))
    print("F1 Score:", f1_score(y_val, predictions))
    return
df= data
y= df['malware']
X= df.drop(['hash','malware'], axis=1)
#Remove hash, it is always a unique character string- MODEL WILL NOT TRAIN
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)
rf = RandomForestClassifier(n_estimators=50, random_state=3)
rf.fit(X_train,y_train)
#Find Important features
importances= pd.DataFrame({'Feature': X_train.columns, 'Importance': np.round((rf.feature_importances_ * 100),3)})
importances= importances.sort_values('Importance', ascending=False).set_index('Feature')

importances.loc[importances['Importance']> 0.2]
features= importances.loc[importances['Importance']> 0.2].index.to_list()
mal= ['malware']
columns= features + mal

columns
#Keep only important features

df=data[columns]

#df.info()
X, y, X_train, X_test, y_train, y_test= create_data(df)

rf = RandomForestClassifier(n_estimators=100, 
                            random_state=3, 
                            bootstrap= True,
                            max_depth= 110,
                            max_features= 3,
                            min_samples_leaf= 2,
                            min_samples_split= 3)
rf.fit(X_train,y_train)
y_pred= rf.predict(X_test)

print("Train Accuracy:",rf.score(X_train, y_train) )#accuracy
print( "Test Accuracy:", accuracy_score(y_test, y_pred, normalize=True))
scores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold

print("Scores: ", scores)
print("Mean: ", scores.mean())
print("Standard Deviation: ", scores.std())
#Cross Validation Evaluation

print("Cross Validation Evaluation:\n")
predictions = cross_val_predict(rf, X_train, y_train, cv=3)
evaluation(rf, X_train, y_train, predictions)
#Test Set Evaulation

print("\nTest Set Evaluation:\n")
y_pred = rf.predict(X_test)
evaluation(rf, X_test, y_test, y_pred)
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
data= pd.read_csv('/kaggle/input/malware-analysis-datasets-top1000-pe-imports/top_1000_pe_imports.csv')
df= data[['OpenProcess', 'GetCurrentProcess', 'GetProcessHeap','ReadFile', 'CreateFileW', 'WriteFile', 'FindFirstFileW', 'FindNextFileW', 'SetWindowsHookExW', 'GetAsyncKeyState', 'GetForegroundWindow', 'GetKeyState', 'MapVirtualKeyW', 'VirtualAlloc', 'VirtualProtect', 'GetModuleHandleA', 'ExitProcess', 'RegCloseKey','GetCurrentProcessId', 'malware']]
#df.columns.to_list()

y= df['malware']
X= df.drop(['malware'], axis=1)
print(X.shape, y.shape)
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)
#print(X_train.shape, X_test.shape)
#print(y_train.shape, y_test.shape)
#Random Forest Classifier

rf = RandomForestClassifier(n_estimators=50, random_state=3)
rf.fit(X_train,y_train)

from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_val_score, cross_val_predict
y_pred= rf.predict(X_test)

print("Train Accuracy:",rf.score(X_train, y_train) )
print( "Test Accuracy:", accuracy_score(y_test, y_pred, normalize=True))
#since accuracy is pretty high (97.8%), will check on cross validation

scores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold

print("Scores: ", scores)
print("Mean: ", scores.mean())
print("Standard Deviation: ", scores.std())
#Still pretty good accuracy, so checking Out of Bag score

rf = RandomForestClassifier(n_estimators=50, oob_score = True, random_state=3)
rf.fit(X_train, y_train)
y_pred = rf.predict(X_test)

print("OOB score:", round(rf.oob_score_, 4)*100, "%")
def evaluation(model, X_val, y_val, predictions):
    print("Confusion Matrix: \n", confusion_matrix(y_val, predictions))
    print("Precision:", precision_score(y_val, predictions))
    print("Recall:",recall_score(y_val, predictions))
    print("F1 Score:", f1_score(y_val, predictions))
    return
#Cross Val Evaluation

print("Cross Validation:\n")
predictions = cross_val_predict(rf, X_train, y_train, cv=3)
evaluation(rf, X_train, y_train, predictions)


#Confusion matrix: [[TN, FP], [FN,TP]]
#Testing Evaluation

print("Test Set:\n")
y_pred = rf.predict(X_test)
evaluation(rf, X_test, y_test, y_pred)
#Checking what Features are deamed important by the classifier; cross check with how it relates to Bhat's Stuff

importances= pd.DataFrame({'Feature': X_train.columns, 'Importance': np.round(rf.feature_importances_,3)})
importances= importances.sort_values('Importance', ascending=False).set_index('Feature')

importances

# Current parameters in use
from pprint import pprint
# Look at parameters used by our current forest
print('Parameters currently in use:\n')
pprint(rf.get_params())
#Testing diff values: each iteration, algo chooses a different combination of features

from sklearn.model_selection import RandomizedSearchCV

n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
max_features = ['auto', 'sqrt']
max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]
max_depth.append(None)
min_samples_split = [2, 5, 10]
min_samples_leaf = [1, 2, 4]
bootstrap = [True, False]
random_grid = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}
pprint(random_grid)
rf = RandomForestClassifier() #Base Model
rf_random = RandomizedSearchCV(estimator = rf, 
                               param_distributions = random_grid, 
                               n_iter = 100, 
                               cv = 3, 
                               verbose=2, 
                               random_state=42,
                               n_jobs = -1)

# 3 folds of cross validation, more cv reduces overfitting but increases run time.
# n_jobs=-1 uses all processors in parallel
# Verbosity helps log the output. https://stats.stackexchange.com/questions/153823/what-is-verbose-in-scikit-learn-package-of-python
#Will take a while to run
rf_random.fit(X_train, y_train)
#Show best parameters
rf_random.best_params_

from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search 
param_grid = {
    'bootstrap': [True],
    'max_depth': [100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [1, 2, 3],
    'min_samples_split': [8,10],
    'n_estimators': [1400, 1600, 1800]
}
# Create a based model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)
grid_search.fit(X_train, y_train)
grid_search.best_params_
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict
from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score
data= pd.read_csv('/kaggle/input/malware-analysis-datasets-top1000-pe-imports/top_1000_pe_imports.csv')
df= data[['OpenProcess', 'LoadLibraryA', 'GetProcessHeap', 'ShellExecuteW', 'VirtualFree', 'GetDC', 'IsDebuggerPresent', 'malloc', 'FindNextFileA', 'free', 'GetAsyncKeyState', 'GetTickCount', 'exit', '_cexit', 'VirtualAlloc', 'VirtualProtect', 'GetModuleHandleA', 'ExitProcess', 'RegCloseKey','GetCurrentProcessId', 'malware']]
y= df['malware']
X= df.drop(['malware'], axis=1)
print(X.shape, y.shape)
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=2)
#print(X_train.shape, X_test.shape)
#print(y_train.shape, y_test.shape)
rf = RandomForestClassifier(n_estimators=1600, 
                            random_state=3, 
                            bootstrap= True,
                            max_depth= 110,
                            max_features= 3,
                            min_samples_leaf= 2,
                            min_samples_split= 10)
rf.fit(X_train,y_train)

#If Bootstrap is False then OOB is not available
y_pred= rf.predict(X_test)

print("Train Accuracy:",rf.score(X_train, y_train) )#accuracy
print( "Test Accuracy:", accuracy_score(y_test, y_pred, normalize=True))
scores= cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy') #5 fold

print("Scores: ", scores)
print("Mean: ", scores.mean())
print("Standard Deviation: ", scores.std())
def evaluation(model, X_val, y_val, predictions):
    print("Confusion Matrix: \n", confusion_matrix(y_val, predictions))
    print("Precision:", precision_score(y_val, predictions))
    print("Recall:",recall_score(y_val, predictions))
    print("F1 Score:", f1_score(y_val, predictions))
    return
#Cross Validation Evaluation

print("Cross Validation Evaluation:\n")
predictions = cross_val_predict(rf, X_train, y_train, cv=3)
evaluation(rf, X_train, y_train, predictions)
#Test Set Evaulation

print("\nTest Set Evaluation:\n")
y_pred = rf.predict(X_test)
evaluation(rf, X_test, y_test, y_pred)

import numpy as np

import pandas as pd



import matplotlib.pyplot as plt

import seaborn as sns



from datetime import datetime



from sklearn.preprocessing import StandardScaler

from sklearn.preprocessing import Imputer

from sklearn.model_selection import GridSearchCV,train_test_split,cross_val_score

from sklearn.metrics import classification_report,confusion_matrix

from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import GaussianNB

from sklearn.neighbors import KNeighborsClassifier

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import GradientBoostingClassifier

from sklearn.svm import SVC

from sklearn.metrics import roc_curve, auc

import os

import warnings

warnings.filterwarnings('ignore')

print(os.listdir("../input"))

data=pd.read_csv('../input/heart.csv')
#Now, our data is loaded. We're writing the following snippet to see the loaded data. The purpose here is to see the top five of the loaded data.



print('Data First 5 Rows Show\n')

data.head()

print('Data Last 5 Rows Show\n')

data.tail()
print('Data Show Describe\n')

data.describe()
print('Data Show Info\n')

data.info()
#We will list all the columns for all data. We check all columns. Is there any spelling mistake?

print('Data Show Columns:\n')

data.columns
data.sample(frac=0.01)
#sample; random rows in dataset

data.sample(5)
data=data.rename(columns={'age':'Age','sex':'Sex','cp':'Cp','trestbps':'Trestbps','chol':'Chol','fbs':'Fbs','restecg':'Restecg','thalach':'Thalach','exang':'Exang','oldpeak':'Oldpeak','slope':'Slope','ca':'Ca','thal':'Thal','target':'Target'})
#New show columns

data.columns
#And, how many rows and columns are there for all data?

print('Data Shape Show\n')

data.shape  #first one is rows, other is columns
#Now,I will check null on all data and If data has null, I will sum of null data's. In this way, how many missing data is in the data.

print('Data Sum of Null Values \n')

data.isnull().sum()
#all rows control for null values

data.isnull().values.any()
plt.figure(figsize=(10,10))

sns.heatmap(data.corr(),annot=True,fmt='.1f')

plt.show()
plt.figure(figsize=(10,10))

sns.heatmap(data.corr(),vmax=.3, center=0,

            square=True, linewidths=.5, cbar_kws={"shrink": .5},annot=True)

plt.tight_layout()

plt.show()
sns.pairplot(data)

plt.show()
data.Age.value_counts()[:10]

#data age show value counts for age least 10
sns.barplot(x=data.Age.value_counts()[:10].index,y=data.Age.value_counts()[:10].values)

plt.xlabel('Age')

plt.ylabel('Age Counter')

plt.title('Age Analysis System')

plt.show()
#firstly find min and max ages

minAge=min(data.Age)

maxAge=max(data.Age)

meanAge=data.Age.mean()

print('Min Age :',minAge)

print('Max Age :',maxAge)

print('Mean Age :',meanAge)
young_ages=data[(data.Age>=29)&(data.Age<40)]

middle_ages=data[(data.Age>=40)&(data.Age<55)]

elderly_ages=data[(data.Age>55)]

print('Young Ages :',len(young_ages))

print('Middle Ages :',len(middle_ages))

print('Elderly Ages :',len(elderly_ages))
sns.barplot(x=['young ages','middle ages','elderly ages'],y=[len(young_ages),len(middle_ages),len(elderly_ages)])

plt.xlabel('Age Range')

plt.ylabel('Age Counts')

plt.title('Ages State in Dataset')

plt.show()
data['AgeRange']=0

youngAge_index=data[(data.Age>=29)&(data.Age<40)].index

middleAge_index=data[(data.Age>=40)&(data.Age<55)].index

elderlyAge_index=data[(data.Age>55)].index
for index in elderlyAge_index:

    data.loc[index,'AgeRange']=2

    

for index in middleAge_index:

    data.loc[index,'AgeRange']=1



for index in youngAge_index:

    data.loc[index,'AgeRange']=0
# Draw a categorical scatterplot to show each observation

sns.swarmplot(x="AgeRange", y="Age",hue='Sex',

              palette=["r", "c", "y"], data=data)

plt.show()
# Plot the total crashes

sns.set_color_codes("pastel")

sns.barplot(y="AgeRange", x="Sex", data=data,

            label="Total", color="b")

plt.show()
sns.countplot(elderly_ages.Sex)

plt.title("Elderly Sex Operations")

plt.show()
elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum')
sns.barplot(x=elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum').index,y=elderly_ages.groupby(elderly_ages['Sex'])['Thalach'].agg('sum').values)

plt.title("Gender Group Thalach Show Sum Time")

plt.show()
sns.violinplot(data.Age, palette="Set3", bw=.2, cut=1, linewidth=1)

plt.xticks(rotation=90)

plt.title("Age Rates")

plt.show()
plt.figure(figsize=(15,7))

sns.violinplot(x=data.Age,y=data.Target)

plt.xticks(rotation=90)

plt.legend()

plt.title("Age & Target System")

plt.show()
colors = ['blue','green','yellow']

explode = [0,0,0.1]

plt.figure(figsize = (5,5))

#plt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')

plt.pie([len(young_ages),len(middle_ages),len(elderly_ages)],labels=['young ages','middle ages','elderly ages'],explode=explode,colors=colors, autopct='%1.1f%%')

plt.title('Age States',color = 'blue',fontsize = 15)

plt.show()
data.Sex.value_counts()
#Sex (1 = male; 0 = female)

sns.countplot(data.Sex)

plt.show()
sns.countplot(data.Sex,hue=data.Slope)

plt.title('Slope & Sex Rates Show')

plt.show()
total_genders_count=len(data.Sex)

male_count=len(data[data['Sex']==1])

female_count=len(data[data['Sex']==0])

print('Total Genders :',total_genders_count)

print('Male Count    :',male_count)

print('Female Count  :',female_count)
#Percentage ratios

print("Male State: {:.2f}%".format((male_count / (total_genders_count)*100)))

print("Female State: {:.2f}%".format((female_count / (total_genders_count)*100)))
#Male State & target 1 & 0

male_andtarget_on=len(data[(data.Sex==1)&(data['Target']==1)])

male_andtarget_off=len(data[(data.Sex==1)&(data['Target']==0)])

####

sns.barplot(x=['Male Target On','Male Target Off'],y=[male_andtarget_on,male_andtarget_off])

plt.xlabel('Male and Target State')

plt.ylabel('Count')

plt.title('State of the Gender')

plt.show()
#Female State & target 1 & 0

female_andtarget_on=len(data[(data.Sex==0)&(data['Target']==1)])

female_andtarget_off=len(data[(data.Sex==0)&(data['Target']==0)])

####

sns.barplot(x=['Female Target On','Female Target Off'],y=[female_andtarget_on,female_andtarget_off])

plt.xlabel('Female and Target State')

plt.ylabel('Count')

plt.title('State of the Gender')

plt.show()


# Plot miles per gallon against horsepower with other semantics

sns.relplot(x="Trestbps", y="Age",

            sizes=(40, 400), alpha=.5, palette="muted",

            height=6, data=data)
data.head()
#As seen, there are 4 types of chest pain.

data.Cp.value_counts()
sns.countplot(data.Cp)

plt.xlabel('Chest Type')

plt.ylabel('Count')

plt.title('Chest Type vs Count State')

plt.show()

#0 status at least

#1 condition slightly distressed

#2 condition medium problem

#3 condition too bad
cp_zero_target_zero=len(data[(data.Cp==0)&(data.Target==0)])

cp_zero_target_one=len(data[(data.Cp==0)&(data.Target==1)])
sns.barplot(x=['cp_zero_target_zero','cp_zero_target_one'],y=[cp_zero_target_zero,cp_zero_target_one])

plt.show()
cp_one_target_zero=len(data[(data.Cp==1)&(data.Target==0)])

cp_one_target_one=len(data[(data.Cp==1)&(data.Target==1)])
sns.barplot(x=['cp_one_target_zero','cp_one_target_one'],y=[cp_one_target_zero,cp_one_target_one])

plt.show()
cp_two_target_zero=len(data[(data.Cp==2)&(data.Target==0)])

cp_two_target_one=len(data[(data.Cp==2)&(data.Target==1)])
sns.barplot(x=['cp_two_target_zero','cp_two_target_one'],y=[cp_two_target_zero,cp_two_target_one])

plt.show()
cp_three_target_zero=len(data[(data.Cp==3)&(data.Target==0)])

cp_three_target_one=len(data[(data.Cp==3)&(data.Target==1)])
sns.barplot(x=['cp_three_target_zero','cp_three_target_one'],y=[cp_three_target_zero,cp_three_target_one])

plt.show()
data.head(1)
# Show the results of a linear regression within each dataset

sns.lmplot(x="Trestbps", y="Chol",data=data,hue="Cp")

plt.show()
target_0_agerang_0=len(data[(data.Target==0)&(data.AgeRange==0)])

target_1_agerang_0=len(data[(data.Target==1)&(data.AgeRange==0)])
colors = ['blue','green']

explode = [0,0.1]

plt.figure(figsize = (5,5))

plt.pie([target_0_agerang_0,target_1_agerang_0], explode=explode, labels=['Target 0 Age Range 0','Target 1 Age Range 0'], colors=colors, autopct='%1.1f%%')

plt.title('Target vs Age Range Young Age ',color = 'blue',fontsize = 15)

plt.show()
target_0_agerang_1=len(data[(data.Target==0)&(data.AgeRange==1)])

target_1_agerang_1=len(data[(data.Target==1)&(data.AgeRange==1)])
colors = ['blue','green']

explode = [0.1,0]

plt.figure(figsize = (5,5))

plt.pie([target_0_agerang_1,target_1_agerang_1], explode=explode, labels=['Target 0 Age Range 1','Target 1 Age Range 1'], colors=colors, autopct='%1.1f%%')

plt.title('Target vs Age Range Middle Age',color = 'blue',fontsize = 15)

plt.show()
target_0_agerang_2=len(data[(data.Target==0)&(data.AgeRange==2)])

target_1_agerang_2=len(data[(data.Target==1)&(data.AgeRange==2)])
colors = ['blue','green']

explode = [0,0.1]

plt.figure(figsize = (5,5))

plt.pie([target_0_agerang_2,target_1_agerang_2], explode=explode, labels=['Target 0 Age Range 2','Target 1 Age Range 2'], colors=colors, autopct='%1.1f%%')

plt.title('Target vs Age Range Elderly Age ',color = 'blue',fontsize = 15)

plt.show()
data.Thalach.value_counts()[:20]

#First show 20 rows
sns.barplot(x=data.Thalach.value_counts()[:20].index,y=data.Thalach.value_counts()[:20].values)

plt.xlabel('Thalach')

plt.ylabel('Count')

plt.title('Thalach Counts')

plt.xticks(rotation=45)

plt.show()
sns.swarmplot(x=data.Age)

plt.title('Age Rates')

plt.show()
age_unique=sorted(data.Age.unique())

age_thalach_values=data.groupby('Age')['Thalach'].count().values

mean_thalach=[]

for i,age in enumerate(age_unique):

    mean_thalach.append(sum(data[data['Age']==age].Thalach)/age_thalach_values[i])
#data_sorted=data.sort_values(by='Age',ascending=True)

plt.figure(figsize=(10,5))

sns.pointplot(x=age_unique,y=mean_thalach,color='red',alpha=0.8)

plt.xlabel('Age',fontsize = 15,color='blue')

plt.xticks(rotation=45)

plt.ylabel('Thalach',fontsize = 15,color='blue')

plt.title('Age vs Thalach',fontsize = 15,color='blue')

plt.grid()

plt.show()
age_range_thalach=data.groupby('AgeRange')['Thalach'].mean()
sns.barplot(x=age_range_thalach.index,y=age_range_thalach.values)

plt.xlabel('Age Range Values')

plt.ylabel('Maximum Thalach By Age Range')

plt.title('illustration of the thalach to the age range')

plt.show()

#As shown in this graph, this rate decreases as the heart rate 

#is faster and in old age areas.
cp_thalach=data.groupby('Cp')['Thalach'].mean()
sns.barplot(x=cp_thalach.index,y=cp_thalach.values)

plt.xlabel('Degree of Chest Pain (Cp)')

plt.ylabel('Maximum Thalach By Cp Values')

plt.title('Illustration of thalach to degree of chest pain')

plt.show()

#As seen in this graph, it is seen that the heart rate is less 

#when the chest pain is low. But in cases where chest pain is 

#1, it is observed that the area is more. 2 and 3 were found to 

#be of the same degree.
data.Thal.value_counts()
sns.countplot(data.Thal)

plt.show()
data[(data.Thal==0)]

#as seen, only 50% was understood to be 50% target.
data[(data['Thal']==1)].Target.value_counts()

sns.barplot(x=data[(data['Thal']==1)].Target.value_counts().index,y=data[(data['Thal']==1)].Target.value_counts().values)

plt.xlabel('Thal Value')

plt.ylabel('Count')

plt.title('Counter for Thal')

plt.show()
#Target 1

a=len(data[(data['Target']==1)&(data['Thal']==0)])

b=len(data[(data['Target']==1)&(data['Thal']==1)])

c=len(data[(data['Target']==1)&(data['Thal']==2)])

d=len(data[(data['Target']==1)&(data['Thal']==3)])

print('Target 1 Thal 0: ',a)

print('Target 1 Thal 1: ',b)

print('Target 1 Thal 2: ',c)

print('Target 1 Thal 3: ',d)



#so,Apparently, there is a rate at Thal 2.Now, draw graph

print('*'*50)

#Target 0

e=len(data[(data['Target']==0)&(data['Thal']==0)])

f=len(data[(data['Target']==0)&(data['Thal']==1)])

g=len(data[(data['Target']==0)&(data['Thal']==2)])

h=len(data[(data['Target']==0)&(data['Thal']==3)])

print('Target 0 Thal 0: ',e)

print('Target 0 Thal 1: ',f)

print('Target 0 Thal 2: ',g)

print('Target 0 Thal 3: ',h)
f,ax=plt.subplots(figsize=(7,7))

sns.barplot(y=['T 1&0 Th 0','T 1&0 Th 1','T 1&0 Th 2','Ta 1&0 Th 3'],x=[1,6,130,28],color='green',alpha=0.5,label='Target 1 Thal State')

sns.barplot(y=['T 1&0 Th 0','T 1&0 Th 1','T 1&0 Th 2','Ta 1&0 Th 3'],x=[1,12,36,89],color='red',alpha=0.7,label='Target 0 Thal State')

ax.legend(loc='lower right',frameon=True)

ax.set(xlabel='Target State and Thal Counter',ylabel='Target State and Thal State',title='Target VS Thal')

plt.xticks(rotation=90)

plt.show()

#so, there has been a very nice graphic display. This is the situation that best describes the situation.
data.Target.unique()

#only two values are shown.

#A value of 1 is the value of patient 0.
sns.countplot(data.Target)

plt.xlabel('Target')

plt.ylabel('Count')

plt.title('Target Counter 1 & 0')

plt.show()
sns.countplot(data.Target,hue=data.Sex)

plt.xlabel('Target')

plt.ylabel('Count')

plt.title('Target & Sex Counter 1 & 0')

plt.show()
#determine the age ranges of patients with and without sickness and make analyzes about them

age_counter_target_1=[]

age_counter_target_0=[]

for age in data.Age.unique():

    age_counter_target_1.append(len(data[(data['Age']==age)&(data.Target==1)]))

    age_counter_target_0.append(len(data[(data['Age']==age)&(data.Target==0)]))



#now, draw show on graph    
#Target 1 & 0 show graph on scatter

plt.scatter(x=data.Age.unique(),y=age_counter_target_1,color='blue',label='Target 1')

plt.scatter(x=data.Age.unique(),y=age_counter_target_0,color='red',label='Target 0')

plt.legend(loc='upper right',frameon=True)

plt.xlabel('Age')

plt.ylabel('Count')

plt.title('Target 0 & Target 1 State')

plt.show()
sns.lineplot(x="Sex", y="Oldpeak",

             hue="Target",data=data)

plt.show()
data.head()
data.head()
g = sns.catplot(x="AgeRange", y="Chol",

                 hue="Sex",

                 data=data, kind="bar")

plt.show()
ax = sns.barplot("Sex", "Chol", data=data,

                  linewidth=2.5, facecolor=(1, 1, 1, 0),

                  errcolor=".2", edgecolor=".2")

plt.show()
male_young_t_1=data[(data['Sex']==1)&(data['AgeRange']==0)&(data['Target']==1)]

male_middle_t_1=data[(data['Sex']==1)&(data['AgeRange']==1)&(data['Target']==1)]

male_elderly_t_1=data[(data['Sex']==1)&(data['AgeRange']==2)&(data['Target']==1)]

print(len(male_young_t_1))

print(len(male_middle_t_1))

print(len(male_elderly_t_1))
f,ax1=plt.subplots(figsize=(20,10))

sns.pointplot(x=np.arange(len(male_young_t_1)),y=male_young_t_1.Trestbps,color='lime',alpha=0.8,label='Young')

sns.pointplot(x=np.arange(len(male_middle_t_1)),y=male_middle_t_1.Trestbps,color='black',alpha=0.8,label='Middle')

sns.pointplot(x=np.arange(len(male_elderly_t_1)),y=male_elderly_t_1.Trestbps,color='red',alpha=0.8,label='Elderly')

plt.xlabel('Range',fontsize = 15,color='blue')

plt.xticks(rotation=90)

plt.legend(loc='upper right',frameon=True)

plt.ylabel('Trestbps',fontsize = 15,color='blue')

plt.title('Age Range Values vs Trestbps',fontsize = 20,color='blue')

plt.grid()

plt.show()
data.head()
data_filter_mean=data[(data['Target']==1)&(data['Age']>50)].groupby('Sex')[['Trestbps','Chol','Thalach']].mean()
data_filter_mean.unstack()
for i,col in enumerate(data.columns.values):

    plt.subplot(5,3,i+1)

    plt.scatter([i for i in range(303)],data[col].values.tolist())

    plt.title(col)

    fig,ax=plt.gcf(),plt.gca()

    fig.set_size_inches(10,10)

    plt.tight_layout()

plt.show()
#Let's see how the correlation values between them

data.corr()
dataX=data.drop('Target',axis=1)

dataY=data['Target']
X_train,X_test,y_train,y_test=train_test_split(dataX,dataY,test_size=0.2,random_state=42)
print('X_train',X_train.shape)

print('X_test',X_test.shape)

print('y_train',y_train.shape)

print('y_test',y_test.shape)
#Normalization as the first process

# Normalize

X_train=(X_train-np.min(X_train))/(np.max(X_train)-np.min(X_train)).values

X_test=(X_test-np.min(X_test))/(np.max(X_test)-np.min(X_test)).values
from sklearn.decomposition import PCA

pca=PCA().fit(X_train)

print(pca.explained_variance_ratio_)

print()

print(X_train.columns.values.tolist())

print(pca.components_)
cumulative=np.cumsum(pca.explained_variance_ratio_)

plt.step([i for i in range(len(cumulative))],cumulative)

plt.show()
pca = PCA(n_components=8)

pca.fit(X_train)

reduced_data_train = pca.transform(X_train)

#inverse_data = pca.inverse_transform(reduced_data)

plt.scatter(reduced_data_train[:, 0], reduced_data_train[:, 1], label='reduced')

plt.xlabel('First Principal Component')

plt.ylabel('Second Principal Component')

plt.show()
pca = PCA(n_components=8)

pca.fit(X_test)

reduced_data_test = pca.transform(X_test)

#inverse_data = pca.inverse_transform(reduced_data)

plt.scatter(reduced_data_test[:, 0], reduced_data_test[:, 1], label='reduced')

plt.xlabel('First Principal Component')

plt.ylabel('Second Principal Component')

plt.show()
reduced_data_train = pd.DataFrame(reduced_data_train, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8'])

reduced_data_test = pd.DataFrame(reduced_data_test, columns=['Dim1', 'Dim2','Dim3','Dim4','Dim5','Dim6','Dim7','Dim8'])

X_train=reduced_data_train

X_test=reduced_data_test
def plot_roc_(false_positive_rate,true_positive_rate,roc_auc):

    plt.figure(figsize=(5,5))

    plt.title('Receiver Operating Characteristic')

    plt.plot(false_positive_rate,true_positive_rate, color='red',label = 'AUC = %0.2f' % roc_auc)

    plt.legend(loc = 'lower right')

    plt.plot([0, 1], [0, 1],linestyle='--')

    plt.axis('tight')

    plt.ylabel('True Positive Rate')

    plt.xlabel('False Positive Rate')

    plt.show()

    

def plot_feature_importances(gbm):

    n_features = X_train.shape[1]

    plt.barh(range(n_features), gbm.feature_importances_, align='center')

    plt.yticks(np.arange(n_features), X_train.columns)

    plt.xlabel("Feature importance")

    plt.ylabel("Feature")

    plt.ylim(-1, n_features)
combine_features_list=[

    ('Dim1','Dim2','Dim3'),

    ('Dim4','Dim5','Dim5','Dim6'),

    ('Dim7','Dim8','Dim1'),

    ('Dim4','Dim8','Dim5')

]
parameters=[

{

    'penalty':['l1','l2'],

    'C':[0.1,0.4,0.5],

    'random_state':[0]

    },

]



for features in combine_features_list:

    print(features)

    print("*"*50)

    

    X_train_set=X_train.loc[:,features]

    X_test_set=X_test.loc[:,features]

    

    gslog=GridSearchCV(LogisticRegression(),parameters,scoring='accuracy')

    gslog.fit(X_train_set,y_train)

    print('Best parameters set:')

    print(gslog.best_params_)

    print()

    predictions=[

    (gslog.predict(X_train_set),y_train,'Train'),

    (gslog.predict(X_test_set),y_test,'Test'),

    ]

    

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1],pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)



    print("*"*50)    

    basari=cross_val_score(estimator=LogisticRegression(),X=X_train,y=y_train,cv=12)

    print(basari.mean())

    print(basari.std())

    print("*"*50) 

   
from sklearn.linear_model import LogisticRegression



lr=LogisticRegression(C=0.1,penalty='l1',random_state=0)

lr.fit(X_train,y_train)



y_pred=lr.predict(X_test)





y_proba=lr.predict_proba(X_test)



false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])

roc_auc = auc(false_positive_rate, true_positive_rate)

plot_roc_(false_positive_rate,true_positive_rate,roc_auc)





from sklearn.metrics import r2_score,accuracy_score



#print('Hata Oranı :',r2_score(y_test,y_pred))

print('Accurancy Oranı :',accuracy_score(y_test, y_pred))

print("Logistic TRAIN score with ",format(lr.score(X_train, y_train)))

print("Logistic TEST score with ",format(lr.score(X_test, y_test)))

print()



cm=confusion_matrix(y_test,y_pred)

print(cm)

sns.heatmap(cm,annot=True)

plt.show()
print('CoEf:\n')

print(lr.coef_)

print('Intercept_\n')

print(lr.intercept_)

print('Proba:\n')

print(lr.predict_log_proba)
parameters=[

{

    'n_neighbors':np.arange(2,33),

    'n_jobs':[2,6]

    },

]

print("*"*50)

for features in combine_features_list:

    print("*"*50)

    

    X_train_set=X_train.loc[:,features]

    X_test_set=X_test.loc[:,features]

   

    gsknn=GridSearchCV(KNeighborsClassifier(),parameters,scoring='accuracy')

    gsknn.fit(X_train_set,y_train)

    print('Best parameters set:')

    print(gsknn.best_params_)

    print("*"*50)

    predictions = [

    (gsknn.predict(X_train_set), y_train, 'Train'),

    (gsknn.predict(X_test_set), y_test, 'Test1')

    ]

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

        

    print("*"*50)    

    basari=cross_val_score(estimator=KNeighborsClassifier(),X=X_train,y=y_train,cv=12)

    print(basari.mean())

    print(basari.std())

    print("*"*50)
knn=KNeighborsClassifier(n_jobs=2, n_neighbors=22)

knn.fit(X_train,y_train)



y_pred=knn.predict(X_test)



y_proba=knn.predict_proba(X_test)

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])

roc_auc = auc(false_positive_rate, true_positive_rate)

plot_roc_(false_positive_rate,true_positive_rate,roc_auc)



from sklearn.metrics import r2_score,accuracy_score



print('Accurancy Oranı :',accuracy_score(y_test, y_pred))

print("KNN TRAIN score with ",format(knn.score(X_train, y_train)))

print("KNN TEST score with ",format(knn.score(X_test, y_test)))

print()



cm=confusion_matrix(y_test,y_pred)

print(cm)

sns.heatmap(cm,annot=True)

plt.show()
n_neighbors = range(1, 17)

train_data_accuracy = []

test1_data_accuracy = []

for n_neigh in n_neighbors:

    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)

    knn.fit(X_train, y_train)

    train_data_accuracy.append(knn.score(X_train, y_train))

    test1_data_accuracy.append(knn.score(X_test, y_test))

plt.plot(n_neighbors, train_data_accuracy, label="Train Data Set")

plt.plot(n_neighbors, test1_data_accuracy, label="Test1 Data Set")

plt.ylabel("Accuracy")

plt.xlabel("Neighbors")

plt.legend()

plt.show()
n_neighbors = range(1, 17)

k_scores=[]

for n_neigh in n_neighbors:

    knn = KNeighborsClassifier(n_neighbors=n_neigh,n_jobs=5)

    scores=cross_val_score(estimator=knn,X=X_train,y=y_train,cv=12)

    k_scores.append(scores.mean())

print(k_scores)
plt.plot(n_neighbors,k_scores)

plt.xlabel('Value of k for KNN')

plt.ylabel("Cross-Validated Accurancy")

plt.show()
print('Leaf Size :',knn.leaf_size)

print('Metric :',knn.metric_params)

print('Radius :',knn.radius)

print('Weights :',knn.weights)

print('Algorithms :',knn.algorithm)
parameters = [

    {

        'kernel': ['linear'],

        'random_state': [2]

    },

    {

        'kernel': ['rbf'],

        'gamma':[0.9,0.06,0.3],

        'random_state': [0],

        'C':[1,2,3,4,5,6],

        'degree':[2],

        'probability':[True]

    },

]



for features in combine_features_list:

    print("*"*50)

    X_train_set=X_train.loc[:,features]

    X_test_set=X_test.loc[:,features]

  

    svc = GridSearchCV(SVC(), parameters,

    scoring='accuracy')

    svc.fit(X_train_set, y_train)

    print('Best parameters set:')

    print(svc.best_params_)

    print("*"*50)

    predictions = [

    (svc.predict(X_train_set), y_train, 'Train'),

    (svc.predict(X_test_set), y_test, 'Test1')

    ]

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

        

    print("*"*50)    

    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)

    print(basari.mean())

    print(basari.std())

    print("*"*50)
svc=SVC(C=5,degree=2,gamma=0.06,kernel='rbf',probability=True,random_state=0)

svc.fit(X_train,y_train)



y_pred=svc.predict(X_test)



y_proba=svc.predict_proba(X_test)

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])

roc_auc = auc(false_positive_rate, true_positive_rate)

plot_roc_(false_positive_rate,true_positive_rate,roc_auc)



from sklearn.metrics import r2_score,accuracy_score



print('Accurancy Oranı :',accuracy_score(y_test, y_pred))

print("SVC TRAIN score with ",format(svc.score(X_train, y_train)))

print("SVC TEST score with ",format(svc.score(X_test, y_test)))

print()



cm=confusion_matrix(y_test,y_pred)

print(cm)

sns.heatmap(cm,annot=True)

plt.show()
print('Coef :',svc.coef0)

print('Dual Coef_',svc.dual_coef_)

print('Fit Status :',svc.fit_status_)

print('Gamma :',svc.gamma)

print('Kernel :',svc.kernel)

print('SV :',svc.support_vectors_)

print('Probablity :',svc.probability)

parameters = [

{

    'learning_rate': [0.01, 0.02, 0.002],

    'random_state': [0],

    'n_estimators': np.arange(3, 20)

    },

]

for features in combine_features_list:

    print("*"*50)

    X_train_set=X_train.loc[:,features]

    X_test1_set=X_test.loc[:,features]

   

    gbc = GridSearchCV(GradientBoostingClassifier(), parameters, scoring='accuracy')

    gbc.fit(X_train_set, y_train)

    print('Best parameters set:')

    print(gbc.best_params_)

    print("*"*50)

    predictions = [

    (gbc.predict(X_train_set), y_train, 'Train'),

    (gbc.predict(X_test1_set), y_test, 'Test1')

    ]

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

        

    print("*"*50)    

    basari=cross_val_score(estimator=GradientBoostingClassifier(),X=X_train,y=y_train,cv=4)

    print(basari.mean())

    print(basari.std())

    print("*"*50)
gbc=GradientBoostingClassifier(learning_rate=0.02,n_estimators=18,random_state=0)

gbc.fit(X_train,y_train)



y_pred=gbc.predict(X_test)



y_proba=gbc.predict_proba(X_test)

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])

roc_auc = auc(false_positive_rate, true_positive_rate)

plot_roc_(false_positive_rate,true_positive_rate,roc_auc)



from sklearn.metrics import r2_score,accuracy_score



print('Accurancy Oranı :',accuracy_score(y_test, y_pred))

print("GradientBoostingClassifier TRAIN score with ",format(gbc.score(X_train, y_train)))

print("GradientBoostingClassifier TEST score with ",format(gbc.score(X_test, y_test)))

print()



cm=confusion_matrix(y_test,y_pred)

print(cm)

sns.heatmap(cm,annot=True)

plt.show()
plot_feature_importances(gbc)

plt.show()
parameters = [

    {

        'max_depth': np.arange(1, 10),

        'min_samples_split': np.arange(2, 5),

        'random_state': [3],

        'n_estimators': np.arange(10, 20)

    },

]



for features in combine_features_list:

    print("*"*50)

    

    X_train_set=X_train.loc[:,features]

    X_test1_set=X_test.loc[:,features]

    

    tree=GridSearchCV(RandomForestClassifier(),parameters,scoring='accuracy')

    tree.fit(X_train_set, y_train)

    

    print('Best parameters set:')

    print(tree.best_params_)

    print("*"*50)

    predictions = [

        (tree.predict(X_train_set), y_train, 'Train'),

        (tree.predict(X_test1_set), y_test, 'Test1')

    ]

    

    for pred in predictions:

        

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

    

    print("*"*50)    

    basari=cross_val_score(estimator=RandomForestClassifier(),X=X_train,y=y_train,cv=4)

    print(basari.mean())

    print(basari.std())

    print("*"*50)
rfc=RandomForestClassifier(max_depth=7,min_samples_split=4,n_estimators=19,random_state=3)

rfc.fit(X_train,y_train)



y_pred=rfc.predict(X_test)



y_proba=rfc.predict_proba(X_test)

false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test,y_proba[:,1])

roc_auc = auc(false_positive_rate, true_positive_rate)

plot_roc_(false_positive_rate,true_positive_rate,roc_auc)



from sklearn.metrics import r2_score,accuracy_score

print('Accurancy Oranı :',accuracy_score(y_test, y_pred))

print("RandomForestClassifier TRAIN score with ",format(rfc.score(X_train, y_train)))

print("RandomForestClassifier TEST score with ",format(rfc.score(X_test, y_test)))

print()



cm=confusion_matrix(y_test,y_pred)

print(cm)

sns.heatmap(cm,annot=True)

plt.show()
for i in range(1,11):

    rf = RandomForestClassifier(n_estimators=i, random_state = 3, max_depth=7)

    rf.fit(X_train, y_train)

    print("TEST set score w/ " +str(i)+" estimators: {:.5}".format(rf.score(X_test, y_test)))
plot_feature_importances(rf)

plt.show()
parameters = [

{

    'random_state': [42],

    },

]

for features in combine_features_list:

    print("*"*50)

    X_train_set=X_train.loc[:,features]

    X_test1_set=X_test.loc[:,features]

    

    dtr = GridSearchCV(DecisionTreeClassifier(), parameters, scoring='accuracy')

    

    dtr.fit(X_train_set, y_train)

    print('Best parameters set:')

    print(dtr.best_params_)

    print("*"*50)

    predictions = [

    (dtr.predict(X_train_set), y_train, 'Train'),

    (dtr.predict(X_test1_set), y_test, 'Test1')

    ]

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

        

    print("*"*50)    

    basari=cross_val_score(estimator=DecisionTreeClassifier(),X=X_train,y=y_train,cv=4)

    print(basari.mean())

    print(basari.std())

    print("*"*50)  
parameters = [

{

    'random_state': [42],

    },

]

for features in combine_features_list:

    print("*"*50)

    X_train_set=X_train.loc[:,features]

    X_test1_set=X_test.loc[:,features]

    

    dtr = GridSearchCV(SVC(), parameters, scoring='accuracy')

    

    dtr.fit(X_train_set, y_train)

    print('Best parameters set:')

    print(dtr.best_params_)

    print("*"*50)

    predictions = [

    (dtr.predict(X_train_set), y_train, 'Train'),

    (dtr.predict(X_test1_set), y_test, 'Test1')

    ]

    for pred in predictions:

        print(pred[2] + ' Classification Report:')

        print("*"*50)

        print(classification_report(pred[1], pred[0]))

        print("*"*50)

        print(pred[2] + ' Confusion Matrix:')

        print(confusion_matrix(pred[1], pred[0]))

        print("*"*50)

        

    print("*"*50)    

    basari=cross_val_score(estimator=SVC(),X=X_train,y=y_train,cv=4)

    print(basari.mean())

    print(basari.std())

    print("*"*50)  
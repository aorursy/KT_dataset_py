# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory



import os

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))



# Any results you write to the current directory are saved as output.
# Read in the file into a pandas DataFrame

train = pd.read_csv('../input/solar-generation-forecasting-challenge/train_v3.csv')



# Sneak peek at the actual DataFrame itself

train.head()
train.info()
train.describe()
# Convert the Date Time column to a DateTime64 data type

train['Date.Time'] = pd.to_datetime(train['Date.Time'])



# Then let's create some new features such as hour / month / year

train['Hour'] = pd.DatetimeIndex(train['Date.Time']).hour

train['Month'] = pd.DatetimeIndex(train['Date.Time']).month

train['Year'] = pd.DatetimeIndex(train['Date.Time']).year
# import some visualisation libraries

import seaborn as sns

import matplotlib.pyplot as plt



# for DateTime conversion into plots

from pandas.plotting import register_matplotlib_converters

register_matplotlib_converters() 
# Let's only take a single PV.Id to visualise and only in the year 2010

sns.lineplot(x="Date.Time", y="Value", data=train[(train['PV.Id'] == 1) & (train['Year'] == 2010)]).set_title("Solar Generated for PV.Id = 1 in Year 2010")
# Let's summarise and take the mean Value across all PV.Ids for a specific hour

mean_value_per_hour = train.groupby(['Hour'])['Value'].mean()

mean_value_per_hour = mean_value_per_hour.reset_index()

# Let's visualise this

sns.lineplot(x="Hour", y="Value", data=mean_value_per_hour).set_title("Average Solar Generated across all PV panels on an hourly basis")
# Let's summarise and take the mean Value across all PV.Ids by month

mean_value_month = train.groupby(['Month'])['Value'].mean()

mean_value_month = mean_value_month.reset_index()

# Let's visualise this

sns.lineplot(x="Month", y="Value", data=mean_value_month).set_title("Average Solar Generated by Month")
# Read in the CSV that contains the PV attributes

solar_pv_attribute = pd.read_csv("../input/solar-generation-forecasting-challenge/solar_pv_attributes.csv")
solar_pv_attribute
# Now let's join them together with our training dataset

train_df = pd.merge(train, solar_pv_attribute, on = 'PV.Id', how = 'left')

train_df.head()
def process_data(file_name, solar_pv_attr_file_name = None):

    # read in the file

    file_path = os.path.join('../input/solar-generation-forecasting-challenge', file_name)

    data = pd.read_csv(file_path)

    

    # Convert the Date Time column to a DateTime64 data type

    data['Date.Time'] = pd.to_datetime(data['Date.Time'])



    # Then let's create some new features such as hour / month / year

    data['Hour'] = pd.DatetimeIndex(data['Date.Time']).hour

    data['Month'] = pd.DatetimeIndex(data['Date.Time']).month

    data['Year'] = pd.DatetimeIndex(data['Date.Time']).year

    

    # Merge it with the solar PV attribute (if present)

    if solar_pv_attr_file_name is not None:

        solar_pv_file_path = os.path.join('../input/solar-generation-forecasting-challenge', solar_pv_attr_file_name)

        solar_pv_attribute = pd.read_csv(solar_pv_file_path)

        data = pd.merge(data, solar_pv_attribute, on = 'PV.Id', how = 'left')

    

    return data



train_df = process_data('train_v3.csv', solar_pv_attr_file_name = 'solar_pv_attributes.csv')

test_df = process_data('test_v3.csv', solar_pv_attr_file_name = 'solar_pv_attributes.csv')
# Check to make sure all is good

train_df.head()
# Note that the test data frame does NOT have the Value column (this is what we are trying to predict)

test_df.head()
from sklearn.linear_model import LinearRegression



# Let's split the training data into train / validation based on description above

X_train = train_df[(train_df['Year'] < 2012) | ((train_df['Year'] == 2012) & (train_df['Month'] <= 6))].drop(['Value', 'Date.Time', 'Postcode'], axis = 1)

y_train = train_df[(train_df['Year'] < 2012) | ((train_df['Year'] == 2012) & (train_df['Month'] <= 6))]['Value']

X_valid = train_df[(train_df['Year'] == 2012) & (train_df['Month'] > 6)].drop(['Value', 'Date.Time', 'Postcode'], axis = 1)

y_valid = train_df[(train_df['Year'] == 2012) & (train_df['Month'] > 6)]['Value']
X_train
y_train
lm_model = LinearRegression().fit(X = X_train, y = y_train)
# Let's get a summary on how the model fitted

from statsmodels.api import OLS



OLS(y_train,X_train).fit().summary()
# Reduce to piecewise linear features

def calculate_piecewise_features(data):

    data['Month_Piecewise_1_6'] = data['Month'].apply(lambda x: x if x >= 1 and x <= 6 else 0)

    data['Month_Piecewise_7_12'] = data['Month'].apply(lambda x: x if x > 7 else 0)

    data['Hour_Piecewise_1_12'] = data['Hour'].apply(lambda x: x if x < 13 else 0)

    data['Hour_Piecewise_13_24'] = data['Hour'].apply(lambda x: x if x >= 13 else 0)

    return data



X_train = calculate_piecewise_features(X_train)

X_valid = calculate_piecewise_features(X_valid)

X_test  = calculate_piecewise_features(test_df)
# Try again and observe the R^2

from statsmodels.api import OLS



OLS(y_train,X_train.drop(['Hour','Month', 'PV.Id'], axis = 1)).fit().summary()
from sklearn.metrics import mean_squared_error

import math



def calculate_rmse(y_true, y_pred):

    return math.sqrt(mean_squared_error(y_true, y_pred))



cols_excluded = ['PV.Id', 'Hour', 'Month']

test_cols_excluded = ['Id', 'PV.Id', 'Hour', 'Month', 'Date.Time', 'Postcode']



# need to re-build our linear model to incorporate the new features

lm_model = LinearRegression().fit(X = X_train.drop(cols_excluded, axis = 1), y = y_train)



# calculate for the training & validation set

y_train_pred = lm_model.predict(X_train.drop(cols_excluded, axis = 1))

y_valid_pred = lm_model.predict(X_valid.drop(cols_excluded, axis = 1))



# calculate for the test set (if required)

y_test_pred = lm_model.predict(X_test.drop(test_cols_excluded, axis = 1))



# evaluate

print('[Linear] Train RMSE: {}'.format(calculate_rmse(y_train, y_train_pred)))

print('[Linear] Validation RMSE: {}'.format(calculate_rmse(y_valid, y_valid_pred)))
from sklearn.ensemble import RandomForestRegressor



# build a random forest model

rf_model = RandomForestRegressor(n_estimators = 10) # can increase the n_estimators to improve the performance, at the expense of increasing training time!

rf_model.fit(X = X_train.drop(cols_excluded, axis = 1), y = y_train)
# now let's evaluate to see if this is better...

# calculate for the training & validation set

y_train_pred = rf_model.predict(X_train.drop(cols_excluded, axis = 1))

y_valid_pred = rf_model.predict(X_valid.drop(cols_excluded, axis = 1))



# calculate for the test set (if required)

y_test_pred = rf_model.predict(X_test.drop(test_cols_excluded, axis = 1))



# evaluate

print('[RF] Train RMSE: {}'.format(calculate_rmse(y_train, y_train_pred)))

print('[RF] Validation RMSE: {}'.format(calculate_rmse(y_valid, y_valid_pred)))
def random_forest_varimp(rf_model, features):

    importances = rf_model.feature_importances_

    indices = np.argsort(importances)

    

    plt.title('Feature Importance')

    plt.barh(range(len(indices)), importances[indices], color='g', align='center')

    plt.yticks(range(len(indices)), [features[i] for i in indices])

    plt.xlabel('Relative Importance')

    plt.ylabel('Features')

    plt.show()

    

random_forest_varimp(rf_model, X_train.drop(cols_excluded, axis = 1).columns.values)
# Take the sample submission format and replace the Values with the ones we predicted

# Take care of the ordering (by default both sample submission and the test CSV are ordered the same way, so this shouldn't be an issue unless you change the order of test predictions)

submission = pd.read_csv('../input/solar-generation-forecasting-challenge/sample_submission_v3.csv')



submission['Value'] = y_test_pred



submission.to_csv('submission.csv', index=False)
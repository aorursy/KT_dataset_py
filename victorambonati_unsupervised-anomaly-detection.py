# libraries

#%matplotlib notebook



import pandas as pd

import numpy as np



import matplotlib

import seaborn

import matplotlib.dates as md

from matplotlib import pyplot as plt



from sklearn import preprocessing

from sklearn.decomposition import PCA

from sklearn.cluster import KMeans

from sklearn.covariance import EllipticEnvelope

#from pyemma import msm # not available on Kaggle Kernel

from sklearn.ensemble import IsolationForest

from sklearn.svm import OneClassSVM
# some function for later



# return Series of distance between each point and his distance with the closest centroid

def getDistanceByPoint(data, model):

    distance = pd.Series()

    for i in range(0,len(data)):

        Xa = np.array(data.loc[i])

        Xb = model.cluster_centers_[model.labels_[i]-1]

        distance.set_value(i, np.linalg.norm(Xa-Xb))

    return distance



# train markov model to get transition matrix

def getTransitionMatrix (df):

	df = np.array(df)

	model = msm.estimate_markov_model(df, 1)

	return model.transition_matrix



def markovAnomaly(df, windows_size, threshold):

    transition_matrix = getTransitionMatrix(df)

    real_threshold = threshold**windows_size

    df_anomaly = []

    for j in range(0, len(df)):

        if (j < windows_size):

            df_anomaly.append(0)

        else:

            sequence = df[j-windows_size:j]

            sequence = sequence.reset_index(drop=True)

            df_anomaly.append(anomalyElement(sequence, real_threshold, transition_matrix))

    return df_anomaly
df = pd.read_csv("../input/realKnownCause/realKnownCause/ambient_temperature_system_failure.csv")
print(df.info())
# check the timestamp format and frequence 

print(df['timestamp'].head(10))
# check the temperature mean

print(df['value'].mean())
# change the type of timestamp column for plotting

df['timestamp'] = pd.to_datetime(df['timestamp'])

# change fahrenheit to Â°C (temperature mean= 71 -> fahrenheit)

df['value'] = (df['value'] - 32) * 5/9

# plot the data

df.plot(x='timestamp', y='value')
# the hours and if it's night or day (7:00-22:00)

df['hours'] = df['timestamp'].dt.hour

df['daylight'] = ((df['hours'] >= 7) & (df['hours'] <= 22)).astype(int)
# the day of the week (Monday=0, Sunday=6) and if it's a week end day or week day.

df['DayOfTheWeek'] = df['timestamp'].dt.dayofweek

df['WeekDay'] = (df['DayOfTheWeek'] < 5).astype(int)

# An estimation of anomly population of the dataset (necessary for several algorithm)

outliers_fraction = 0.01
# time with int to plot easily

df['time_epoch'] = (df['timestamp'].astype(np.int64)/100000000000).astype(np.int64)
# creation of 4 distinct categories that seem useful (week end/day week & night/day)

df['categories'] = df['WeekDay']*2 + df['daylight']



a = df.loc[df['categories'] == 0, 'value']

b = df.loc[df['categories'] == 1, 'value']

c = df.loc[df['categories'] == 2, 'value']

d = df.loc[df['categories'] == 3, 'value']



fig, ax = plt.subplots()

a_heights, a_bins = np.histogram(a)

b_heights, b_bins = np.histogram(b, bins=a_bins)

c_heights, c_bins = np.histogram(c, bins=a_bins)

d_heights, d_bins = np.histogram(d, bins=a_bins)



width = (a_bins[1] - a_bins[0])/6



ax.bar(a_bins[:-1], a_heights*100/a.count(), width=width, facecolor='blue', label='WeekEndNight')

ax.bar(b_bins[:-1]+width, (b_heights*100/b.count()), width=width, facecolor='green', label ='WeekEndLight')

ax.bar(c_bins[:-1]+width*2, (c_heights*100/c.count()), width=width, facecolor='red', label ='WeekDayNight')

ax.bar(d_bins[:-1]+width*3, (d_heights*100/d.count()), width=width, facecolor='black', label ='WeekDayLight')



plt.legend()

plt.show()
# Take useful feature and standardize them

data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]

min_max_scaler = preprocessing.StandardScaler()

np_scaled = min_max_scaler.fit_transform(data)

data = pd.DataFrame(np_scaled)

# reduce to 2 importants features

pca = PCA(n_components=2)

data = pca.fit_transform(data)

# standardize these 2 new features

min_max_scaler = preprocessing.StandardScaler()

np_scaled = min_max_scaler.fit_transform(data)

data = pd.DataFrame(np_scaled)
# calculate with different number of centroids to see the loss plot (elbow method)

n_cluster = range(1, 20)

kmeans = [KMeans(n_clusters=i).fit(data) for i in n_cluster]

scores = [kmeans[i].score(data) for i in range(len(kmeans))]

fig, ax = plt.subplots()

ax.plot(n_cluster, scores)

plt.show()
# Not clear for me, I choose 15 centroids arbitrarily and add these data to the central dataframe

df['cluster'] = kmeans[14].predict(data)

df['principal_feature1'] = data[0]

df['principal_feature2'] = data[1]

df['cluster'].value_counts()
#plot the different clusters with the 2 main features

fig, ax = plt.subplots()

colors = {0:'red', 1:'blue', 2:'green', 3:'pink', 4:'black', 5:'orange', 6:'cyan', 7:'yellow', 8:'brown', 9:'purple', 10:'white', 11: 'grey', 12:'lightblue', 13:'lightgreen', 14: 'darkgrey'}

ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df["cluster"].apply(lambda x: colors[x]))

plt.show()
# get the distance between each point and its nearest centroid. The biggest distances are considered as anomaly

distance = getDistanceByPoint(data, kmeans[14])

number_of_outliers = int(outliers_fraction*len(distance))

threshold = distance.nlargest(number_of_outliers).min()

# anomaly21 contain the anomaly result of method 2.1 Cluster (0:normal, 1:anomaly) 

df['anomaly21'] = (distance >= threshold).astype(int)
# visualisation of anomaly with cluster view

fig, ax = plt.subplots()

colors = {0:'blue', 1:'red'}

ax.scatter(df['principal_feature1'], df['principal_feature2'], c=df["anomaly21"].apply(lambda x: colors[x]))

plt.show()
# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly21'] == 1, ['time_epoch', 'value']] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.show()
# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly21'] == 0, 'value']

b = df.loc[df['anomaly21'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

plt.legend()

plt.show()
# creation of 4 differents data set based on categories defined before

df_class0 = df.loc[df['categories'] == 0, 'value']

df_class1 = df.loc[df['categories'] == 1, 'value']

df_class2 = df.loc[df['categories'] == 2, 'value']

df_class3 = df.loc[df['categories'] == 3, 'value']
# plot the temperature repartition by categories

fig, axs = plt.subplots(2,2)

df_class0.hist(ax=axs[0,0],bins=32)

df_class1.hist(ax=axs[0,1],bins=32)

df_class2.hist(ax=axs[1,0],bins=32)

df_class3.hist(ax=axs[1,1],bins=32)
# apply ellipticEnvelope(gaussian distribution) at each categories

envelope =  EllipticEnvelope(contamination = outliers_fraction) 

X_train = df_class0.values.reshape(-1,1)

envelope.fit(X_train)

df_class0 = pd.DataFrame(df_class0)

df_class0['deviation'] = envelope.decision_function(X_train)

df_class0['anomaly'] = envelope.predict(X_train)



envelope =  EllipticEnvelope(contamination = outliers_fraction) 

X_train = df_class1.values.reshape(-1,1)

envelope.fit(X_train)

df_class1 = pd.DataFrame(df_class1)

df_class1['deviation'] = envelope.decision_function(X_train)

df_class1['anomaly'] = envelope.predict(X_train)



envelope =  EllipticEnvelope(contamination = outliers_fraction) 

X_train = df_class2.values.reshape(-1,1)

envelope.fit(X_train)

df_class2 = pd.DataFrame(df_class2)

df_class2['deviation'] = envelope.decision_function(X_train)

df_class2['anomaly'] = envelope.predict(X_train)



envelope =  EllipticEnvelope(contamination = outliers_fraction) 

X_train = df_class3.values.reshape(-1,1)

envelope.fit(X_train)

df_class3 = pd.DataFrame(df_class3)

df_class3['deviation'] = envelope.decision_function(X_train)

df_class3['anomaly'] = envelope.predict(X_train)
# plot the temperature repartition by categories with anomalies

a0 = df_class0.loc[df_class0['anomaly'] == 1, 'value']

b0 = df_class0.loc[df_class0['anomaly'] == -1, 'value']



a1 = df_class1.loc[df_class1['anomaly'] == 1, 'value']

b1 = df_class1.loc[df_class1['anomaly'] == -1, 'value']



a2 = df_class2.loc[df_class2['anomaly'] == 1, 'value']

b2 = df_class2.loc[df_class2['anomaly'] == -1, 'value']



a3 = df_class3.loc[df_class3['anomaly'] == 1, 'value']

b3 = df_class3.loc[df_class3['anomaly'] == -1, 'value']



fig, axs = plt.subplots(2,2)

axs[0,0].hist([a0,b0], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

axs[0,1].hist([a1,b1], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

axs[1,0].hist([a2,b2], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

axs[1,1].hist([a3,b3], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

axs[0,0].set_title("WeekEndNight")

axs[0,1].set_title("WeekEndLight")

axs[1,0].set_title("WeekDayNight")

axs[1,1].set_title("WeekDayLight")

plt.legend()

plt.show()
# add the data to the main 

df_class = pd.concat([df_class0, df_class1, df_class2, df_class3])

df['anomaly22'] = df_class['anomaly']

df['anomaly22'] = np.array(df['anomaly22'] == -1).astype(int) 
# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly22'] == 1, ('time_epoch', 'value')] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.show()
# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly22'] == 0, 'value']

b = df.loc[df['anomaly22'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

plt.legend()

plt.show()
# definition of the different state

x1 = (df['value'] <=18).astype(int)

x2= ((df['value'] > 18) & (df['value']<=21)).astype(int)

x3 = ((df['value'] > 21) & (df['value']<=24)).astype(int)

x4 = ((df['value'] > 24) & (df['value']<=27)).astype(int)

x5 = (df['value'] >27).astype(int)

df_mm = x1 + 2*x2 + 3*x3 + 4*x4 + 5*x5



# getting the anomaly labels for our dataset (evaluating sequence of 5 values and anomaly = less than 20% probable)

# I USE pyemma NOT AVAILABLE IN KAGGLE KERNEL

#df_anomaly = markovAnomaly(df_mm, 5, 0.20)

#df_anomaly = pd.Series(df_anomaly)

#print(df_anomaly.value_counts())
"""

# add the data to the main 

df['anomaly24'] = df_anomaly



# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly24'] == 1, ('time_epoch', 'value')] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.show()

"""
"""

# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly24'] == 0, 'value']

b = df.loc[df['anomaly24'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])

plt.legend()

plt.show()

"""
# Take useful feature and standardize them 

data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]

min_max_scaler = preprocessing.StandardScaler()

np_scaled = min_max_scaler.fit_transform(data)

data = pd.DataFrame(np_scaled)

# train isolation forest 

model =  IsolationForest(contamination = outliers_fraction)

model.fit(data)

# add the data to the main  

df['anomaly25'] = pd.Series(model.predict(data))

df['anomaly25'] = df['anomaly25'].map( {1: 0, -1: 1} )

print(df['anomaly25'].value_counts())
# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly25'] == 1, ['time_epoch', 'value']] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.show()
# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly25'] == 0, 'value']

b = df.loc[df['anomaly25'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label = ['normal', 'anomaly'])

plt.legend()

plt.show()
# Take useful feature and standardize them 

data = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]

min_max_scaler = preprocessing.StandardScaler()

np_scaled = min_max_scaler.fit_transform(data)

# train one class SVM 

model =  OneClassSVM(nu=0.95 * outliers_fraction) #nu=0.95 * outliers_fraction  + 0.05

data = pd.DataFrame(np_scaled)

model.fit(data)

# add the data to the main  

df['anomaly26'] = pd.Series(model.predict(data))

df['anomaly26'] = df['anomaly26'].map( {1: 0, -1: 1} )

print(df['anomaly26'].value_counts())
# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly26'] == 1, ['time_epoch', 'value']] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.show()
# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly26'] == 0, 'value']

b = df.loc[df['anomaly26'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'], label=['normal', 'anomaly'])

plt.legend()

plt.show()
#select and standardize data

data_n = df[['value', 'hours', 'daylight', 'DayOfTheWeek', 'WeekDay']]

min_max_scaler = preprocessing.StandardScaler()

np_scaled = min_max_scaler.fit_transform(data_n)

data_n = pd.DataFrame(np_scaled)



# important parameters and train/test size

prediction_time = 1 

testdatasize = 1000

unroll_length = 50

testdatacut = testdatasize + unroll_length  + 1



#train data

x_train = data_n[0:-prediction_time-testdatacut].as_matrix()

y_train = data_n[prediction_time:-testdatacut  ][0].as_matrix()



# test data

x_test = data_n[0-testdatacut:-prediction_time].as_matrix()

y_test = data_n[prediction_time-testdatacut:  ][0].as_matrix()
#unroll: create sequence of 50 previous data points for each data points

def unroll(data,sequence_length=24):

    result = []

    for index in range(len(data) - sequence_length):

        result.append(data[index: index + sequence_length])

    return np.asarray(result)



# adapt the datasets for the sequence data shape

x_train = unroll(x_train,unroll_length)

x_test  = unroll(x_test,unroll_length)

y_train = y_train[-x_train.shape[0]:]

y_test  = y_test[-x_test.shape[0]:]



# see the shape

print("x_train", x_train.shape)

print("y_train", y_train.shape)

print("x_test", x_test.shape)

print("y_test", y_test.shape)
# specific libraries for RNN

# keras is a high layer build on Tensorflow layer to stay in high level/easy implementation

from keras.layers.core import Dense, Activation, Dropout

from keras.layers.recurrent import LSTM

from keras.models import Sequential

import time #helper libraries

from keras.models import model_from_json

import sys
# Build the model

model = Sequential()



model.add(LSTM(

    input_dim=x_train.shape[-1],

    output_dim=50,

    return_sequences=True))

model.add(Dropout(0.2))



model.add(LSTM(

    100,

    return_sequences=False))

model.add(Dropout(0.2))



model.add(Dense(

    units=1))

model.add(Activation('linear'))



start = time.time()

model.compile(loss='mse', optimizer='rmsprop')

print('compilation time : {}'.format(time.time() - start))
# Train the model

#nb_epoch = 350



model.fit(

    x_train,

    y_train,

    batch_size=3028,

    nb_epoch=30,

    validation_split=0.1)

# save the model because the training is long (1h30) and we don't want to do it every time

"""

# serialize model to JSON

model_json = model.to_json()

with open("model2.json", "w") as json_file:

    json_file.write(model_json)

# serialize weights to HDF5

model.save_weights("model2.h5")

print("Saved model to disk")

"""
# load json and create model

"""

json_file = open('model.json', 'r')

loaded_model_json = json_file.read()

json_file.close()

loaded_model = model_from_json(loaded_model_json)

# load weights into new model

loaded_model.load_weights("model.h5")

print("Loaded model from disk")

"""
# create the list of difference between prediction and test data

loaded_model = model

diff=[]

ratio=[]

p = loaded_model.predict(x_test)

# predictions = lstm.predict_sequences_multiple(loaded_model, x_test, 50, 50)

for u in range(len(y_test)):

    pr = p[u][0]

    ratio.append((y_test[u]/pr)-1)

    diff.append(abs(y_test[u]- pr))
# plot the prediction and the reality (for the test data)

fig, axs = plt.subplots()

axs.plot(p,color='red', label='prediction')

axs.plot(y_test,color='blue', label='y_test')

plt.legend(loc='upper left')

plt.show()
# select the most distant prediction/reality data points as anomalies

diff = pd.Series(diff)

number_of_outliers = int(outliers_fraction*len(diff))

threshold = diff.nlargest(number_of_outliers).min()

# data with anomaly label (test data part)

test = (diff >= threshold).astype(int)

# the training data part where we didn't predict anything (overfitting possible): no anomaly

complement = pd.Series(0, index=np.arange(len(data_n)-testdatasize))

# # add the data to the main

df['anomaly27'] = complement.append(test, ignore_index='True')

print(df['anomaly27'].value_counts())
# visualisation of anomaly throughout time (viz 1)

fig, ax = plt.subplots()



a = df.loc[df['anomaly27'] == 1, ['time_epoch', 'value']] #anomaly



ax.plot(df['time_epoch'], df['value'], color='blue')

ax.scatter(a['time_epoch'],a['value'], color='red')

plt.axis([1.370*1e7, 1.405*1e7, 15,30])

plt.show()
# visualisation of anomaly with temperature repartition (viz 2)

a = df.loc[df['anomaly27'] == 0, 'value']

b = df.loc[df['anomaly27'] == 1, 'value']



fig, axs = plt.subplots()

axs.hist([a,b], bins=32, stacked=True, color=['blue', 'red'])

plt.legend()

plt.show()
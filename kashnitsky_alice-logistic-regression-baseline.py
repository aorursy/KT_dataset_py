# Import libraries and set desired options

%matplotlib inline

from matplotlib import pyplot as plt

import seaborn as sns



import pickle

import numpy as np

import pandas as pd

from scipy.sparse import csr_matrix

from scipy.sparse import hstack

from sklearn.preprocessing import StandardScaler

from sklearn.feature_extraction.text import CountVectorizer

from sklearn.metrics import roc_auc_score

from sklearn.linear_model import LogisticRegression
# Read the training and test data sets

train_df = pd.read_csv('../input/train_sessions.csv',

                       index_col='session_id', parse_dates=['time1'])

test_df = pd.read_csv('../input/test_sessions.csv',

                      index_col='session_id', parse_dates=['time1'])



# Sort the data by time

train_df = train_df.sort_values(by='time1')



# Look at the first rows of the training set

train_df.head()
# Change site1, ..., site10 columns type to integer and fill NA-values with zeros

sites = ['site%s' % i for i in range(1, 11)]

train_df[sites] = train_df[sites].fillna(0).astype('int')

test_df[sites] = test_df[sites].fillna(0).astype('int')



# Load websites dictionary

with open(r"../input/site_dic.pkl", "rb") as input_file:

    site_dict = pickle.load(input_file)



# Create dataframe for the dictionary

sites_dict = pd.DataFrame(list(site_dict.keys()), index=list(site_dict.values()), columns=['site'])

print(u'Websites total:', sites_dict.shape[0])

sites_dict.head()
# Answer

print(test_df.shape, train_df.shape)
# Our target variable

y_train = train_df['target'].values



# United dataframe of the initial data 

full_df = pd.concat([train_df.drop('target', axis=1), test_df])



# Index to split the training and test data sets

idx_split = train_df.shape[0]
# small

train_df[sites].fillna(0).to_csv('train_sessions_text.txt', 

                                 sep=' ', index=None, header=None)

test_df[sites].fillna(0).to_csv('test_sessions_text.txt', 

                                sep=' ', index=None, header=None)
!head -3 train_sessions_text.txt
%%time

cv = CountVectorizer(ngram_range=(1, 3), max_features=50000)

with open('train_sessions_text.txt') as inp_train_file:

    X_train = cv.fit_transform(inp_train_file)

with open('test_sessions_text.txt') as inp_test_file:

    X_test = cv.transform(inp_test_file)

print(X_train.shape, X_test.shape)
def get_auc_lr_valid(X, y, C=1.0, seed=17, ratio = 0.9):

    # Split the data into the training and validation sets

    idx = int(round(X.shape[0] * ratio))

    # Classifier training

    lr = LogisticRegression(C=C, random_state=seed, solver='lbfgs', max_iter=500).fit(X[:idx, :], y[:idx])

    # Prediction for validation set

    y_pred = lr.predict_proba(X[idx:, :])[:, 1]

    # Calculate the quality

    score = roc_auc_score(y[idx:], y_pred)

    

    return score
X_train.shape, y_train.shape
%%time

# Calculate metric on the validation set

print(get_auc_lr_valid(X_train, y_train))
# Function for writing predictions to a file

def write_to_submission_file(predicted_labels, out_file,

                             target='target', index_label="session_id"):

    predicted_df = pd.DataFrame(predicted_labels,

                                index = np.arange(1, predicted_labels.shape[0] + 1),

                                columns=[target])

    predicted_df.to_csv(out_file, index_label=index_label)
# Train the model on the whole training data set

# Use random_state=17 for reproducibility

# Parameter C=1 by default, but here we set it explicitly

lr = LogisticRegression(C=1.0, random_state=17, solver='lbfgs', max_iter=500).fit(X_train, y_train)



# Make a prediction for test data set

y_test = lr.predict_proba(X_test)[:, 1]



# Write it to the file which could be submitted

write_to_submission_file(y_test, 'baseline_1.csv')


import numpy as np 

import pandas as pd 



import os

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))
import pandas as pd

#loading dataset

malware = pd.read_csv("../input/data.csv",sep="|")

#dataset description

malware.describe()

#malware.head()
# dropping useless data + selection of predective features 

X = malware.drop(['Name', 'md5', 'legitimate'], axis=1)

X.head()
# selection of prediction column

y = malware.legitimate

y.head()
number_of_features = X.shape[1]

#X.shape

print(number_of_features)
# Important features selection using ExtraTreeslassifier

from sklearn.ensemble import ExtraTreesClassifier

feature_selector = ExtraTreesClassifier(random_state=0)

# model fitting

from sklearn.feature_selection import SelectFromModel

fsel = ExtraTreesClassifier().fit(X, y)

model = SelectFromModel(fsel, prefit=True) # trimming
# new features version

X_new = model.transform(X)

new_number_of_features = X_new.shape[1]

print(f"reduction rate of {number_of_features - new_number_of_features} features")
# splitting training and validation data 

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X_new, y ,test_size=0.2)
# indices of important features 

indices = np.argsort(fsel.feature_importances_)[::-1][:new_number_of_features]

for feature in range(new_number_of_features):

    print("%d. feature %s (%f)" % (feature + 1, malware.columns[2+indices[feature]], fsel.feature_importances_[indices[feature]]))
# odering features

features = []

for feature in sorted(np.argsort(fsel.feature_importances_)[::-1][:new_number_of_features]):

    features.append(malware.columns[2+feature])

print(features)
# algorithms

from sklearn.tree import DecisionTreeClassifier, export_graphviz

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier

from sklearn.naive_bayes import GaussianNB



algorithms = {

        "DecisionTree": DecisionTreeClassifier(max_depth=10),

        "RandomForest": RandomForestClassifier(n_estimators=50),

        "GradientBoosting": GradientBoostingClassifier(n_estimators=50),

        "AdaBoost": AdaBoostClassifier(n_estimators=100),

        "GNB": GaussianNB()

    }
# benchmarking best algorithm

results = {}

print("Benchmark of algorithms:")

for algo in algorithms:

    classifier = algorithms[algo]

    classifier.fit(X_train, y_train)

    score = classifier.score(X_test, y_test)

    print("%s : %f %%" % (algo, score*100))

    results[algo] = score
# chosen classifier

chosen_classifier = max(results, key=results.get)

print(f"Chosen classifier is {chosen_classifier}, running 50 estimators")

classifier = algorithms[chosen_classifier]

classifier.fit(X_train, y_train)

score = classifier.score(X_test, y_test)

print("%s : %f %%" % (chosen_classifier, score*100))
# model improvment using Hyperparameter Tuning 

# tweaking n_estimators, max_depth

# n_estimators = number of trees in the foreset

# max_depth = max number of levels in each decision tree

# extra because of high accuracy
# visualization of the random forest

estimator = algorithms[chosen_classifier].estimators_[5]

from sklearn.tree import export_graphviz

# Export as dot file

export_graphviz(estimator, out_file='tree.dot', 

                feature_names = features,

                class_names = "legitimate",

                rounded = True, proportion = False, 

                precision = 2, filled = True)



# Convert to png using system command (requires Graphviz)

from subprocess import call

call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])



# Display in jupyter notebook

from IPython.display import Image

Image(filename = 'tree.png')
# Save the classifier: algorithm+ features using joblib

from sklearn.externals import joblib

import pickle



joblib.dump(results[chosen_classifier], 'classifier.pkl')

open('features.pkl', 'wb').write(pickle.dumps(features))

print('Saved')
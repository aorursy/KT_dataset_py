#suppress warnings generated by ipython for cleaner working
import warnings
warnings.simplefilter('ignore')
import twint #twitter scrapping tool
import nest_asyncio #twint has dependency on this
import os #to measure file size
import time #to process time data
import pandas as pd
import seaborn as sns #plotting tool
import matplotlib.pyplot as plt #plotting tool
nest_asyncio.apply() #so that multiple requests can be made at the same time
tweet_client = twint.Config() # configure a client

#search configuration
tweet_client.Search = "delhiriots" #serach querry
tweet_client.Limit = 10000 #max number of results
tweet_client.Store_csv = True #store data to a csv file by appending it
tweet_client.Output = "tweets.csv" #file name
tweet_client.Hide_output = True #do not print output in the notebook
tweet_client.Resume = 'last_qerry.txt'#last checkpoint to continue search incase of error
old_size = os.path.getsize('./tweets.csv') #track file size
twint.run.Search(tweet_client) #run search
new_size = os.path.getsize('./tweets.csv') #track file size
while(old_size<new_size):
    twint.run.Search(tweet_client) #run search
    old_size = new_size
    new_size = os.path.getsize('./tweets.csv') #track file size
    time.sleep(30)
# reading twitter handles from tweets.csv
participant_handles_df = pd.read_csv('tweets.csv', usecols=['username'])

# remove duplicates
participant_handles_df.drop_duplicates(inplace=True)
participant_handles_df.to_csv("unique_users.csv") # store users for future use
print("number of unique users: ",participant_handles_df.shape[0])
import threading #multiprocessing
import asyncio

def search_query(username):
    asyncio.set_event_loop(asyncio.new_event_loop())
    
    #client configuration
    user_client = twint.Config()
    user_client.Store_csv = True
    user_client.Hide_output = True
    user_client.Output = "users_parallel.csv"
    for user in username:
        user_client.Username = user
        twint.run.Lookup(user_client)

max_queries=100 #number of simultanous querries
user_thread=[]

#generating and starting threads
for i in range(max_queries):
    user_thread.append(threading.Thread(target=search_query, args=(participant_handles_df.username[580*i:580*i+579],)))
    user_thread[i].start()
tweets_df = pd.read_csv("tweets.csv") # read tweets data and store in dataframe
users_df = pd.read_csv("users.csv") # read users data and store in dataframe
tweets_df.head()
users_df.head()
tweets_df.columns
users_df.columns
#drop columns that are not required in tweets dataframe
tweets_df.drop(columns=['id', 'conversation_id', 'user_id', 'name', 'place', 'urls', 'photos',
       'cashtags', 'link', 'retweet', 'quote_url', 'video', 'near', 'geo',
       'source', 'user_rt_id', 'user_rt', 'retweet_id', 'reply_to',
       'retweet_date', 'translate', 'trans_src', 'trans_dest'], axis=1, inplace=True)
#verify if drop of columns successful in tweets dataframe
tweets_df.columns
#drop columns that are not required in users dataframe
users_df.drop(columns=['id', 'url', 'media', 'profile_image_url', 'background_image'], axis=1, inplace=True)
#verify if drop of columns successfil in users dataframe
users_df.columns
tweets_df['date'] = pd.to_datetime(tweets_df.date)
users_df['join_date'] = pd.to_datetime(users_df.join_date)
#verification of successful conversion
tweets_df['date'].head()
print("oldest tweet date:",min(tweets_df.date))
tweets_bft = tweets_df[(tweets_df.date<"2020-02-22")] #filter tweets older than 22nd February 2020
#verify successful filtering
max(tweets_bft.date)
tweets_df = tweets_df[(tweets_df.date>"2020-02-21")] #filter tweets latest than 22nd February 2020
#verify successful filtering
min(tweets_df.date)
tweet_user_df = tweets_df.set_index('username').join(users_df.set_index('username')) #join with username as key
tweet_user_df.columns
# the below filter has been set as participating users were too large (58k+) even with multiprocessing it is taking more time. So data is scrapped using a seperate script
tweet_user_df[~(tweet_user_df.join_date.isna())].head(2)
date_df=tweets_df.date.groupby(tweets_df["date"]).count() #group by using date to generate histogram
date_df.plot(kind="bar", title="histogram of tweets", figsize=(16,6))
tweets_df[['retweets_count','likes_count','replies_count']].boxplot()
tweets_df[((tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count)>0)][['retweets_count','likes_count','replies_count']].boxplot()
plt.figure(figsize=(16, 6))
sns.distplot(tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count, hist=False, bins=1).set_title("distribution of engagement")
#tweet with most likes
best_tweet_likes = tweets_df[(tweets_df.likes_count==max(tweets_df.likes_count))]
print("Best tweet based on likes: \"%s\" by %s"%(best_tweet_likes.tweet.values[0], best_tweet_likes.username.values[0]))
#tweet with most likes
best_tweet_retweets = tweets_df[(tweets_df.retweets_count==max(tweets_df.retweets_count))]
print("Best tweet based on retweets: \"%s\" by %s"%(best_tweet_retweets.tweet.values[0], best_tweet_retweets.username.values[0]))
#tweet with most replies
best_tweet_replies = tweets_df[(tweets_df.replies_count==max(tweets_df.replies_count))]
print("Best tweet based on replies: \"%s\" by %s"%(best_tweet_replies.tweet.values[0], best_tweet_replies.username.values[0]))
#tweet with most engagements
best_tweet_engagements = tweets_df[((tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count)==max(tweets_df.retweets_count+tweets_df.likes_count+tweets_df.replies_count))]
print("Best tweet based on engagement: \"%s\" by %s"%(best_tweet_engagements.tweet.values[0], best_tweet_engagements.username.values[0]))
plt.figure(figsize=(7, 7))
plt.title("verified vs unverified participants")
plt.pie(users_df.username.groupby(users_df.verified).count(), labels=["unverified","verified"],autopct='%1.1f%%', explode=[0.2,0],startangle=90)
plt.show()
users_df["twitter_age"]=max(tweets_df['date'])-users_df['join_date'] #max is the latest tweet that is on 30th March 2020
sns.distplot(users_df["twitter_age"].dt.days, hist=False)
plt.title("distribution of twitter age of participants")
plt.show()
#statisitcal description of users with less than 50 days age
users_df[(users_df.twitter_age.dt.days<50)][['tweets',"followers","following","likes"]].describe()
#statisitcal description of users with less than 50 days age
users_df[(users_df.twitter_age.dt.days>50)][['tweets',"followers","following","likes"]].describe()
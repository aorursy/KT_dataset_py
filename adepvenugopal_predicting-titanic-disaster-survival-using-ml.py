import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

import seaborn as sns

%matplotlib inline

sns.set()



import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

warnings.filterwarnings("ignore", category=FutureWarning)

#warnings.filterwarnings("ignore")

#warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)

#warnings.filterwarnings(action='once')



from sklearn.utils.testing import ignore_warnings



from subprocess import check_output

print(check_output(["ls", "../input"]).decode("utf8"))
# This is used to avoid any warnings.

def warn(*args, **kwargs):

    pass

import warnings

warnings.warn = warn
#We are defining some functions which help in plotting and training

def get_best_score(model):

    

    print(model.best_score_)    

    print(model.best_params_)

    print(model.best_estimator_)

    

    return model.best_score_





def plot_feature_importances(model, columns):

    nr_f = 10

    imp = pd.Series(data = model.best_estimator_.feature_importances_, 

                    index=columns).sort_values(ascending=False)

    plt.figure(figsize=(7,5))

    plt.title("Feature importance")

    ax = sns.barplot(y=imp.index[:nr_f], x=imp.values[:nr_f], orient='h')
df_train = pd.read_csv("../input/train.csv")

df_test = pd.read_csv("../input/test.csv")
df_train.head()
df_test.head()
df_train.info()
df_test.info()
fig, ax = plt.subplots(figsize=(9,5))

sns.heatmap(df_train.isnull(), cbar=False, cmap="YlGnBu_r")

plt.show()
fig, ax = plt.subplots(figsize=(9,5))

sns.heatmap(df_test.isnull(), cbar=False, cmap="YlGnBu_r")

plt.show()
# We are assigning a variable for the columns.

cols = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']
nr_rows = 2

nr_cols = 3



fig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))



for r in range(0,nr_rows):

    for c in range(0,nr_cols):  

        

        i = r*nr_cols+c       

        ax = axs[r][c]

        sns.countplot(df_train[cols[i]], hue=df_train["Survived"], ax=ax)

        ax.set_title(cols[i], fontsize=14, fontweight='bold')

        ax.legend(title="survived", loc='upper center') 

        

plt.tight_layout()   
bins = np.arange(0, 80, 5)

g = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)

g.map(sns.distplot, 'Age', kde=False, bins=bins, hist_kws=dict(alpha=0.6))

g.add_legend()  

plt.show()  
df_train['Fare'].max()
bins = np.arange(0, 550, 50)

g = sns.FacetGrid(df_train, row='Sex', col='Pclass', hue='Survived', margin_titles=True, size=3, aspect=1.1)

g.map(sns.distplot, 'Fare', kde=False, bins=bins, hist_kws=dict(alpha=0.6))

g.add_legend()  

plt.show()  
sns.barplot(x='Pclass', y='Survived', data=df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of Pclass")

plt.show()
sns.barplot(x='Sex', y='Survived', hue='Pclass', data=df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of Pclass and Sex")

plt.show()
sns.barplot(x='Embarked', y='Survived', data=df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of Embarked Port")

plt.show()
sns.barplot(x='Embarked', y='Survived', hue='Pclass', data=df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of Embarked Port")

plt.show()
sns.countplot(x='Embarked', hue='Pclass', data=df_train)

plt.title("Count of Passengers as function of Embarked Port")

plt.show()
sns.boxplot(x='Embarked', y='Age', data=df_train)

plt.title("Age distribution as function of Embarked Port")

plt.show()
sns.boxplot(x='Embarked', y='Fare', data=df_train)

plt.title("Fare distribution as function of Embarked Port")

plt.show()
cm_surv = ["darkgrey" , "lightgreen"]
fig, ax = plt.subplots(figsize=(13,7))

sns.swarmplot(x='Pclass', y='Age', hue='Survived', split=True, data=df_train , palette=cm_surv, size=7, ax=ax)

plt.title('Survivals for Age and Pclass ')

plt.show()
fig, ax = plt.subplots(figsize=(13,7))

sns.violinplot(x="Pclass", y="Age", hue='Survived', data=df_train, split=True, bw=0.05 , palette=cm_surv, ax=ax)

plt.title('Survivals for Age and Pclass ')

plt.show()
g = sns.factorplot(x="Pclass", y="Age", hue="Survived", col="Sex", data=df_train, kind="swarm", split=True, palette=cm_surv, size=7, aspect=.9, s=7)
g = sns.factorplot(x="Pclass", y="Age", hue="Survived", col="Sex", data=df_train, kind="violin", split=True, bw=0.05, palette=cm_surv, size=7, aspect=.9, s=7)
for df in [df_train, df_test] :

    

    df['FamilySize'] = df['SibSp'] + df['Parch'] +1

    

    df['Alone']=0

    df.loc[(df.FamilySize==1),'Alone'] = 1

    

    df['NameLen'] = df.Name.apply(lambda x : len(x)) 

    df['NameLenBin']=np.nan

    for i in range(20,0,-1):

        df.loc[ df['NameLen'] <= i*5, 'NameLenBin'] = i

    

    

    df['Title']=0

    df['Title']=df.Name.str.extract(r'([A-Za-z]+)\.') #lets extract the Salutations

    df['Title'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],

                    ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr'],inplace=True)
print(df_train[['NameLen' , 'NameLenBin']].head(10))
grps_namelenbin_survrate = df_train.groupby(['NameLenBin'])['Survived'].mean().to_frame()

grps_namelenbin_survrate
plt.subplots(figsize=(10,6))

sns.barplot(x='NameLenBin' , y='Survived' , data = df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of NameLenBin")

plt.show()
fig, ax = plt.subplots(figsize=(9,7))

sns.violinplot(x="NameLenBin", y="Pclass", data=df_train, hue='Survived', split=True, 

               orient="h", bw=0.2 , palette=cm_surv, ax=ax)

plt.show()
g = sns.factorplot(x="NameLenBin", y="Survived", col="Sex", data=df_train, kind="bar", size=5, aspect=1.2)
grps_title_survrate = df_train.groupby(['Title'])['Survived'].mean().to_frame()

grps_title_survrate
plt.subplots(figsize=(10,6))

sns.barplot(x='Title' , y='Survived' , data = df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of Title")

plt.show()
pd.crosstab(df_train.FamilySize,df_train.Survived).apply(lambda r: r/r.sum(), axis=1).style.background_gradient(cmap='summer_r')
plt.subplots(figsize=(10,6))

sns.barplot(x='FamilySize' , y='Survived' , data = df_train)

plt.ylabel("Survival Rate")

plt.title("Survival as function of FamilySize")

plt.show()
for df in [df_train, df_test]:



    # Title

    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])



    # Age: use Title to fill missing values

    df.loc[(df.Age.isnull())&(df.Title=='Mr'),'Age']= df.Age[df.Title=="Mr"].mean()

    df.loc[(df.Age.isnull())&(df.Title=='Mrs'),'Age']= df.Age[df.Title=="Mrs"].mean()

    df.loc[(df.Age.isnull())&(df.Title=='Master'),'Age']= df.Age[df.Title=="Master"].mean()

    df.loc[(df.Age.isnull())&(df.Title=='Miss'),'Age']= df.Age[df.Title=="Miss"].mean()

    df.loc[(df.Age.isnull())&(df.Title=='Other'),'Age']= df.Age[df.Title=="Other"].mean()

    df = df.drop('Name', axis=1)





# Embarked

df_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode().iloc[0])

df_test['Embarked'] = df_test['Embarked'].fillna(df_test['Embarked'].mode().iloc[0])



# Fare

df_train['Fare'] = df_train['Fare'].fillna(df_train['Fare'].mean())

df_test['Fare'] = df_test['Fare'].fillna(df_test['Fare'].mean())
for df in [df_train, df_test]:

    

    df['Age_bin']=np.nan

    for i in range(8,0,-1):

        df.loc[ df['Age'] <= i*10, 'Age_bin'] = i

        

    df['Fare_bin']=np.nan

    for i in range(12,0,-1):

        df.loc[ df['Fare'] <= i*50, 'Fare_bin'] = i        

    

    # convert Title to numerical

    df['Title'] = df['Title'].map( {'Other':0, 'Mr': 1, 'Master':2, 'Miss': 3, 'Mrs': 4 } )

    # fill na with maximum frequency mode

    df['Title'] = df['Title'].fillna(df['Title'].mode().iloc[0])

    df['Title'] = df['Title'].astype(int)        
df_train_ml = df_train.copy()

df_test_ml = df_test.copy()



passenger_id = df_test_ml['PassengerId']
df_train_ml.info()
df_test_ml.info()
df_train_ml = pd.get_dummies(df_train_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)

df_test_ml = pd.get_dummies(df_test_ml, columns=['Sex', 'Embarked', 'Pclass'], drop_first=True)



df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)

df_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age', 'Fare_bin'],axis=1,inplace=True)



#df_train_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)

#df_test_ml.drop(['PassengerId','Name','Ticket', 'Cabin', 'Age_bin', 'Fare_bin'],axis=1,inplace=True)

df_train_ml.dropna(inplace=True)
for df in [df_train_ml, df_test_ml]:

    df.drop(['NameLen'], axis=1, inplace=True)



    df.drop(['SibSp'], axis=1, inplace=True)

    df.drop(['Parch'], axis=1, inplace=True)

    df.drop(['Alone'], axis=1, inplace=True)
df_train_ml.head()
df_test_ml.fillna(df_test_ml.mean(), inplace=True)

df_test_ml.head()
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()



# for df_train_ml

scaler.fit(df_train_ml.drop(['Survived'],axis=1))

scaled_features = scaler.transform(df_train_ml.drop(['Survived'],axis=1))

df_train_ml_sc = pd.DataFrame(scaled_features) # columns=df_train_ml.columns[1::])



# for df_test_ml

df_test_ml.fillna(df_test_ml.mean(), inplace=True)

#scaler.fit(df_test_ml)

scaled_features = scaler.transform(df_test_ml)

df_test_ml_sc = pd.DataFrame(scaled_features) # , columns=df_test_ml.columns)
df_train_ml_sc.head()
df_test_ml_sc.head()
df_train_ml.head()
X = df_train_ml.drop('Survived', axis=1)

y = df_train_ml['Survived']

X_test = df_test_ml



X_sc = df_train_ml_sc

y_sc = df_train_ml['Survived']

X_test_sc = df_test_ml_sc
from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.svm import SVC

from sklearn import tree





from sklearn.metrics import accuracy_score

from sklearn.model_selection import cross_val_score
svc = SVC(gamma = 0.01, C = 100)

scores_svc = cross_val_score(svc, X, y, cv=10, scoring='accuracy')

print(scores_svc)

print(scores_svc.mean())
svc = SVC(gamma = 0.01, C = 100)

scores_svc_sc = cross_val_score(svc, X_sc, y_sc, cv=10, scoring='accuracy')

print(scores_svc_sc)

print(scores_svc_sc.mean())
rfc = RandomForestClassifier(max_depth=5, max_features=6)

scores_rfc = cross_val_score(rfc, X, y, cv=10, scoring='accuracy')

print(scores_rfc)

print(scores_rfc.mean())
from sklearn.model_selection import RandomizedSearchCV

from sklearn.model_selection import GridSearchCV

from scipy.stats import uniform
model = SVC()

param_grid = {'C':uniform(0.1, 5000), 'gamma':uniform(0.0001, 1) }

rand_SVC = RandomizedSearchCV(model, param_distributions=param_grid, n_iter=100)

rand_SVC.fit(X_sc,y_sc)

score_rand_SVC = get_best_score(rand_SVC)
param_grid = {'C': [0.1,10, 100, 1000,5000], 'gamma': [1,0.1,0.01,0.001,0.0001], 'kernel': ['rbf']}

svc_grid = GridSearchCV(SVC(), param_grid, cv=10, refit=True, verbose=1)

svc_grid.fit(X_sc,y_sc)

sc_svc = get_best_score(svc_grid)
pred_all_svc = svc_grid.predict(X_test_sc)



sub_svc = pd.DataFrame()

sub_svc['PassengerId'] = df_test['PassengerId']

sub_svc['Survived'] = pred_all_svc

sub_svc.to_csv('svc.csv',index=False)
knn = KNeighborsClassifier()

leaf_range = list(range(3, 15, 1))

k_range = list(range(1, 15, 1))

weight_options = ['uniform', 'distance']

param_grid = dict(leaf_size=leaf_range, n_neighbors=k_range, weights=weight_options)

print(param_grid)



knn_grid = GridSearchCV(knn, param_grid, cv=10, verbose=1, scoring='accuracy')

knn_grid.fit(X, y)



sc_knn = get_best_score(knn_grid)
pred_all_knn = knn_grid.predict(X_test)



sub_knn = pd.DataFrame()

sub_knn['PassengerId'] = df_test['PassengerId']

sub_knn['Survived'] = pred_all_knn

sub_knn.to_csv('knn.csv',index=False)
from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()



param_grid = {'min_samples_split': [4,7,10,12]}

dtree_grid = GridSearchCV(dtree, param_grid, cv=10, refit=True, verbose=1)

dtree_grid.fit(X_sc,y_sc)



print(dtree_grid.best_score_)

print(dtree_grid.best_params_)

print(dtree_grid.best_estimator_)
pred_all_dtree = dtree_grid.predict(X_test_sc)



sub_dtree = pd.DataFrame()

sub_dtree['PassengerId'] = df_test['PassengerId']

sub_dtree['Survived'] = pred_all_dtree

sub_dtree.to_csv('dtree.csv',index=False)
from sklearn.ensemble import RandomForestClassifier

rfc = RandomForestClassifier()



param_grid = {'max_depth': [3, 5, 6, 7, 8], 'max_features': [6,7,8,9,10],  

              'min_samples_split': [5, 6, 7, 8]}



rf_grid = GridSearchCV(rfc, param_grid, cv=10, refit=True, verbose=1)

rf_grid.fit(X_sc,y_sc)

sc_rf = get_best_score(rf_grid)
plot_feature_importances(rf_grid, X.columns)
pred_all_rf = rf_grid.predict(X_test_sc)



sub_rf = pd.DataFrame()

sub_rf['PassengerId'] = df_test['PassengerId']

sub_rf['Survived'] = pred_all_rf

sub_rf.to_csv('rf.csv',index=False)
from sklearn.ensemble import ExtraTreesClassifier

extr = ExtraTreesClassifier()



param_grid = {'max_depth': [6,7,8,9], 'max_features': [7,8,9,10],  

              'n_estimators': [50, 100, 200]}



extr_grid = GridSearchCV(extr, param_grid, cv=10, refit=True, verbose=1)

extr_grid.fit(X_sc,y_sc)

sc_extr = get_best_score(extr_grid)
pred_all_extr = extr_grid.predict(X_test_sc)



sub_extr = pd.DataFrame()

sub_extr['PassengerId'] = df_test['PassengerId']

sub_extr['Survived'] = pred_all_extr

sub_extr.to_csv('extr.csv',index=False)
from sklearn.ensemble import GradientBoostingClassifier

gbdt = GradientBoostingClassifier()



param_grid = {'n_estimators': [50, 100], 

              'min_samples_split': [3, 4, 5, 6, 7],

              'max_depth': [3, 4, 5, 6]}

gbdt_grid = GridSearchCV(gbdt, param_grid, cv=10, refit=True, verbose=1)

gbdt_grid.fit(X_sc,y_sc)

sc_gbdt = get_best_score(gbdt_grid)
plot_feature_importances(gbdt_grid, X.columns)
pred_all_gbdt = gbdt_grid.predict(X_test_sc)



sub_gbdt = pd.DataFrame()

sub_gbdt['PassengerId'] = df_test['PassengerId']

sub_gbdt['Survived'] = pred_all_gbdt

sub_gbdt.to_csv('gbdt.csv',index=False)
from xgboost import XGBClassifier

xgb = XGBClassifier()

param_grid = {'max_depth': [5,6,7,8], 'gamma': [1, 2, 4], 'learning_rate': [0.1, 0.2, 0.3, 0.5]}



with ignore_warnings(category=DeprecationWarning):

    xgb_grid = GridSearchCV(xgb, param_grid, cv=10, refit=True, verbose=1)

    xgb_grid.fit(X_sc,y_sc)

    sc_xgb = get_best_score(xgb_grid)
plot_feature_importances(xgb_grid, X.columns)
with ignore_warnings(category=DeprecationWarning):

    pred_all_xgb = xgb_grid.predict(X_test_sc)



sub_xgb = pd.DataFrame()

sub_xgb['PassengerId'] = df_test['PassengerId']

sub_xgb['Survived'] = pred_all_xgb

sub_xgb.to_csv('xgb.csv',index=False)
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier()



param_grid = {'n_estimators': [30, 50, 100], 'learning_rate': [0.08, 0.1, 0.2]}

ada_grid = GridSearchCV(ada, param_grid, cv=10, refit=True, verbose=1)

ada_grid.fit(X_sc,y_sc)

sc_ada = get_best_score(ada_grid)



pred_all_ada = ada_grid.predict(X_test_sc)
sub_ada = pd.DataFrame()

sub_ada['PassengerId'] = df_test['PassengerId']

sub_ada['Survived'] = pred_all_ada

sub_ada.to_csv('ada.csv',index=False)
from catboost import CatBoostClassifier

cat=CatBoostClassifier()



param_grid = {'iterations': [100, 150], 'learning_rate': [0.3, 0.4, 0.5], 'loss_function' : ['Logloss']}



cat_grid = GridSearchCV(cat, param_grid, cv=10, refit=True, verbose=1)

cat_grid.fit(X_sc,y_sc, verbose=False)

sc_cat = get_best_score(cat_grid)



pred_all_cat = cat_grid.predict(X_test_sc)
sub_cat = pd.DataFrame()

sub_cat['PassengerId'] = df_test['PassengerId']

sub_cat['Survived'] = pred_all_cat

sub_cat.to_csv('cat.csv',index=False)
import lightgbm as lgb

lgbm = lgb.LGBMClassifier(silent=False)

param_grid = {"max_depth": [8,10,15], "learning_rate" : [0.008,0.01,0.012], 

              "num_leaves": [80,100,120], "n_estimators": [200,250]  }

lgbm_grid = GridSearchCV(lgbm, param_grid, cv=10, refit=True, verbose=1)

lgbm_grid.fit(X_sc,y_sc, verbose=True)

sc_lgbm = get_best_score(lgbm_grid)



pred_all_lgbm = lgbm_grid.predict(X_test_sc)
sub_lgbm = pd.DataFrame()

sub_lgbm['PassengerId'] = df_test['PassengerId']

sub_lgbm['Survived'] = pred_all_lgbm

sub_lgbm.to_csv('lgbm.csv',index=False)
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import GaussianNB



clf1 = LogisticRegression(random_state=1)

clf2 = RandomForestClassifier(random_state=1)

clf3 = GaussianNB()



eclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='soft')



params = {'lr__C': [1.0, 100.0], 'rf__n_estimators': [20, 200],}



with ignore_warnings(category=DeprecationWarning):

    votingclf_grid = GridSearchCV(estimator=eclf, param_grid=params, cv=10)

    votingclf_grid.fit(X_sc,y_sc)

    sc_vot1 = get_best_score(votingclf_grid)
clf4 = GradientBoostingClassifier()

clf5 = SVC()

clf6 = RandomForestClassifier()



eclf_2 = VotingClassifier(estimators=[('gbdt', clf4), 

                                      ('svc', clf5), 

                                      ('rf', clf6)], voting='soft')



params = {'gbdt__n_estimators': [50], 'gbdt__min_samples_split': [3],

          'svc__C': [10, 100] , 'svc__gamma': [0.1,0.01,0.001] , 'svc__kernel': ['rbf'] , 'svc__probability': [True],  

          'rf__max_depth': [7], 'rf__max_features': [2,3], 'rf__min_samples_split': [3] } 



with ignore_warnings(category=DeprecationWarning):

    votingclf_grid_2 = GridSearchCV(estimator=eclf_2, param_grid=params, cv=10)

    votingclf_grid_2.fit(X_sc,y_sc)

    sc_vot2_cv = get_best_score(votingclf_grid_2)
with ignore_warnings(category=DeprecationWarning):    

    pred_all_vot2 = votingclf_grid_2.predict(X_test_sc)



sub_vot2 = pd.DataFrame()

sub_vot2['PassengerId'] = df_test['PassengerId']

sub_vot2['Survived'] = pred_all_vot2

sub_vot2.to_csv('vot2.csv',index=False)
from mlxtend.classifier import StackingClassifier
# Initializing models

clf1 = xgb_grid.best_estimator_

clf2 = gbdt_grid.best_estimator_

clf3 = rf_grid.best_estimator_

clf4 = svc_grid.best_estimator_



lr = LogisticRegression()

st_clf = StackingClassifier(classifiers=[clf1, clf1, clf2, clf3, clf4], meta_classifier=lr)



params = {'meta-logisticregression__C': [0.1,1.0,5.0,10.0] ,

          #'use_probas': [True] ,

          #'average_probas': [True] ,

          'use_features_in_secondary' : [True, False]

         }



with ignore_warnings(category=DeprecationWarning):

    st_clf_grid = GridSearchCV(estimator=st_clf, param_grid=params, cv=5, refit=True)

    st_clf_grid.fit(X_sc, y_sc)

    sc_st_clf = get_best_score(st_clf_grid)

    pred_all_stack = st_clf_grid.predict(X_test_sc)
list_scores = [sc_knn, sc_rf, sc_extr, sc_svc, sc_gbdt, sc_xgb, 

               sc_ada, sc_cat, sc_lgbm, sc_vot2_cv, sc_st_clf]



list_classifiers = ['KNN','RF','EXTR','SVC','GBDT','XGB',

                    'ADA','CAT','LGBM','VOT2','STACK']
score_subm_svc   = 0.80861

score_subm_vot2  = 0.78947

score_subm_ada   = 0.78468

score_subm_lgbm  = 0.78468

score_subm_rf    = 0.77990

score_subm_xgb   = 0.77033

score_subm_dtree = 0.76076

score_subm_extr  = 0.76076

score_subm_gbdt  = 0.74641

score_subm_knn   = 0.69856

             



score_subm_cat = 0.5                #TODO: int

fig, ax = plt.subplots()

fig.set_size_inches(10,7)

sns.barplot(x=list_scores, y=list_classifiers, ax=ax)

plt.xlabel('Score: Accuracy')

plt.show()
predictions = {'KNN': pred_all_knn, 'RF': pred_all_rf, 'EXTR': pred_all_extr, 

               'SVC': pred_all_svc, 'GBDT': pred_all_gbdt, 'XGB': pred_all_xgb, 

               'ADA': pred_all_ada, 'CAT': pred_all_cat, 'LGBM': pred_all_lgbm, 

               'VOT2': pred_all_vot2, 'STACK': pred_all_stack}

df_predictions = pd.DataFrame(data=predictions) 

df_predictions.corr()
plt.figure(figsize=(9, 9))

sns.set(font_scale=1.25)

sns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, 

                fmt='.2f', annot_kws={'size': 10}, 

                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns

            )

plt.yticks(rotation=0)

plt.show()
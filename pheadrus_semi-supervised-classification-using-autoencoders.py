from keras.layers import Input, Dense

from keras.models import Model, Sequential

from keras import regularizers

from sklearn.model_selection import train_test_split 

from sklearn.linear_model import LogisticRegression

from sklearn.metrics import classification_report, accuracy_score

from sklearn.manifold import TSNE

from sklearn import preprocessing 

import matplotlib.pyplot as plt

import pandas as pd 

import numpy as np

import seaborn as sns

sns.set(style="whitegrid")

np.random.seed(203)



data = pd.read_csv("../input/creditcardfraud/creditcard.csv")

data["Time"] = data["Time"].apply(lambda x : x / 3600 % 24)

data.head()
vc = data['Class'].value_counts().to_frame().reset_index()

vc['percent'] = vc["Class"].apply(lambda x : round(100*float(x) / len(data), 2))

vc = vc.rename(columns = {"index" : "Target", "Class" : "Count"})

vc
non_fraud = data[data['Class'] == 0].sample(1000)

fraud = data[data['Class'] == 1]



df = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)

X = df.drop(['Class'], axis = 1).values

Y = df["Class"].values
def tsne_plot(x1, y1, name="graph.png"):

    tsne = TSNE(n_components=2, random_state=0)

    X_t = tsne.fit_transform(x1)



    plt.figure(figsize=(12, 8))

    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud')

    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud')



    plt.legend(loc='best');

    plt.savefig(name);

    plt.show();

    

tsne_plot(X, Y, "original.png")
## input layer 

input_layer = Input(shape=(X.shape[1],))



## encoding part

encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)

encoded = Dense(50, activation='relu')(encoded)



## decoding part

decoded = Dense(50, activation='tanh')(encoded)

decoded = Dense(100, activation='tanh')(decoded)



## output layer

output_layer = Dense(X.shape[1], activation='relu')(decoded)
autoencoder = Model(input_layer, output_layer)

autoencoder.compile(optimizer="adadelta", loss="mse")
x = data.drop(["Class"], axis=1)

y = data["Class"].values



x_scale = preprocessing.MinMaxScaler().fit_transform(x.values)

x_norm, x_fraud = x_scale[y == 0], x_scale[y == 1]
autoencoder.fit(x_norm[0:2000], x_norm[0:2000], 

                batch_size = 256, epochs = 10, 

                shuffle = True, validation_split = 0.20);
hidden_representation = Sequential()

hidden_representation.add(autoencoder.layers[0])

hidden_representation.add(autoencoder.layers[1])

hidden_representation.add(autoencoder.layers[2])
norm_hid_rep = hidden_representation.predict(x_norm[:3000])

fraud_hid_rep = hidden_representation.predict(x_fraud)
rep_x = np.append(norm_hid_rep, fraud_hid_rep, axis = 0)

y_n = np.zeros(norm_hid_rep.shape[0])

y_f = np.ones(fraud_hid_rep.shape[0])

rep_y = np.append(y_n, y_f)

tsne_plot(rep_x, rep_y, "latent_representation.png")
from IPython.display import display, Image, HTML

display(HTML("""<table align="center">

<tr ><td><b>Actual Representation (Before) </b></td><td><b>Latent Representation (Actual)</b></td></tr>

<tr><td><img src='original.png'></td><td>

             <img src='latent_representation.png'></td></tr></table>"""))
train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)

clf = LogisticRegression(solver="lbfgs").fit(train_x, train_y)

pred_y = clf.predict(val_x)



print ("")

print ("Classification Report: ")

print (classification_report(val_y, pred_y))



print ("")

print ("Accuracy Score: ", accuracy_score(val_y, pred_y))
train = pd.read_csv("../input/titanic/train.csv")

test = pd.read_csv("../input/titanic/test.csv")
import re 

full_data = [train, test]



train['Name_length'] = train['Name'].apply(len)

test['Name_length'] = test['Name'].apply(len)

train['Has_Cabin'] = train["Cabin"].apply(lambda x: 0 if type(x) == float else 1)

test['Has_Cabin'] = test["Cabin"].apply(lambda x: 0 if type(x) == float else 1)



for dataset in full_data:

    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1



for dataset in full_data:

    dataset['IsAlone'] = 0

    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1



for dataset in full_data:

    dataset['Embarked'] = dataset['Embarked'].fillna('S')



for dataset in full_data:

    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())

train['CategoricalFare'] = pd.qcut(train['Fare'], 4)



for dataset in full_data:

    age_avg = dataset['Age'].mean()

    age_std = dataset['Age'].std()

    age_null_count = dataset['Age'].isnull().sum()

    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)

    dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list

    dataset['Age'] = dataset['Age'].astype(int)

train['CategoricalAge'] = pd.cut(train['Age'], 5)



def get_title(name):

    title_search = re.search(' ([A-Za-z]+)\.', name)

    if title_search:

        return title_search.group(1)

    return ""



for dataset in full_data:

    dataset['Title'] = dataset['Name'].apply(get_title)

for dataset in full_data:

    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')

    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')

    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')

    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')



for dataset in full_data:

    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)    

    title_mapping = {"Mr": 1, "Miss": 2, "Mrs": 3, "Master": 4, "Rare": 5}

    dataset['Title'] = dataset['Title'].map(title_mapping)

    dataset['Title'] = dataset['Title'].fillna(0)

    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)

    

    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare']         = 0

    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1

    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2

    dataset.loc[ dataset['Fare'] > 31, 'Fare']         = 3

    dataset['Fare'] = dataset['Fare'].astype(int)

    

    dataset.loc[ dataset['Age'] <= 16, 'Age']        = 0

    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1

    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2

    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3

    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4 ;



drop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']

train = train.drop(drop_elements, axis = 1)

train = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)

test  = test.drop(drop_elements, axis = 1)
X = train.drop(["Survived"], axis=1)

y = train["Survived"]

y = y.values



## define the model

input_layer = Input(shape=(X.shape[1],))

encoded = Dense(100, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)

encoded = Dense(50, activation='relu')(encoded)

decoded = Dense(50, activation='tanh')(encoded)

decoded = Dense(100, activation='tanh')(decoded)

output_layer = Dense(X.shape[1], activation='relu')(decoded)



autoencoder = Model(input_layer, output_layer)

autoencoder.compile(optimizer="adadelta", loss="mse")
scaler = preprocessing.MinMaxScaler()

scaler.fit(X.values)

X_scale = scaler.transform(X.values)

test_x_scale = scaler.transform(test.values)



x_perished, x_survived = X_scale[y == 0], X_scale[y == 1]

autoencoder.fit(x_perished, x_perished, epochs = 20, shuffle = True, validation_split = 0.25)
hidden_representation = Sequential()

hidden_representation.add(autoencoder.layers[0])

hidden_representation.add(autoencoder.layers[1])

hidden_representation.add(autoencoder.layers[2])
perished_hid_rep = hidden_representation.predict(x_perished)

survived_hid_rep = hidden_representation.predict(x_survived)



rep_x = np.append(perished_hid_rep, survived_hid_rep, axis = 0)

y_n = np.zeros(perished_hid_rep.shape[0])

y_f = np.ones(survived_hid_rep.shape[0])

rep_y = np.append(y_n, y_f)
train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)

clf = LogisticRegression().fit(train_x, train_y)

pred_y = clf.predict(val_x)



print (classification_report(val_y, pred_y))

print (accuracy_score(val_y, pred_y))
temp = pd.DataFrame(pd.read_csv("../input/titanic/test.csv")['PassengerId'])

test_rep_x = hidden_representation.predict(test_x_scale)

temp['Survived'] = [int(x) for x in clf.predict(test_rep_x)]

temp.to_csv("submission.csv", index = False)

temp.head()
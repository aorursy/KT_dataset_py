# In debug mode, only 10000 lines of data are loaded

Debug = True



# IMPORT LIBRARIES

import pandas as pd, numpy as np, os, gc



# LOAD THESE BUT DONT ENCODE

MM = ['AvSigVersion','Census_OSVersion','Census_OSBuildRevision','AppVersion','EngineVersion']



# LOAD AND NUMERIC ENCODE

NE = ['Census_SystemVolumeTotalCapacity','Census_PrimaryDiskTotalCapacity']



# LOAD AND STATISTICAL ONE-HOT-ENCODE

OHE = [ 'RtpStateBitfield','DefaultBrowsersIdentifier', 'AVProductStatesIdentifier',

        'AVProductsInstalled', 'AVProductsEnabled', 'CountryIdentifier', 'CityIdentifier', 

        'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'Processor', 'OsBuild', 'OsSuite',

        'SmartScreen','Census_MDC2FormFactor', 'Census_OEMNameIdentifier', 

        'Census_ProcessorCoreCount', 'Census_ProcessorModelIdentifier', 

        'Census_OSUILocaleIdentifier', 'Census_PrimaryDiskTypeName',

        'Census_HasOpticalDiskDrive', 'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',

        'Census_InternalPrimaryDiagonalDisplaySizeInInches',

        'Census_InternalPrimaryDisplayResolutionHorizontal',

        'Census_InternalPrimaryDisplayResolutionVertical',

        'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',

        'Census_InternalBatteryNumberOfCharges', 'Census_OSEdition', 'Census_GenuineStateName',

        'Census_ActivationChannel', 'Census_FirmwareManufacturerIdentifier', 'Census_IsTouchEnabled', 

        'Census_IsPenCapable', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 

        'Wdft_RegionIdentifier', 'OsBuildLab', 'OrganizationIdentifier','Platform',

        'Census_OEMModelIdentifier', 'IsProtected', 'IeVerIdentifier','Firewall', 

        'Census_ProcessorManufacturerIdentifier','Census_OSInstallTypeName',

        'Census_OSWUAutoUpdateOptionsName','Census_IsFlightingInternal',

        'Census_FlightRing','Census_ThresholdOptIn','Census_FirmwareVersionIdentifier',

        'Census_IsSecureBootEnabled','Census_IsWIMBootEnabled']



# DONT LOAD THESE

XX = ['SMode','IsBeta', 'OsVer', 'OsPlatformSubRelease', 'SkuEdition', 'AutoSampleOptIn', 'PuaMode',

     'UacLuaenable', 'Census_ProcessorClass', 'Census_OSArchitecture', 'Census_OSBranch',

     'Census_OSBuildNumber', 'Census_OSSkuName', 'Census_OSInstallLanguageIdentifier',

     'Census_IsPortableOperatingSystem', 'Census_IsFlightsDisabled', 'Census_IsVirtualDevice',

     'IsSxsPassiveMode','ProductName','HasTpm','Census_DeviceFamily']



# DONT LOAD THIS

XXX = ['MachineIdentifier']
# Load all features as categories, then transform them

dtypes = {}

for x in OHE+NE+MM: dtypes[x] = 'category'

dtypes['HasDetections'] = 'int8' # label is numeric





# LOAD TRAIN CSV FILE

if Debug:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes, nrows=10000)

else:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes)

    

#Clear dirty data point

if 5244810 in df_train.index:

    df_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'

    df_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)

print ('Loaded',len(df_train),'rows of TRAIN.CSV!')



# SHUFFLE TRAIN DATA

df_train = df_train.sample(frac=1)

df_train.reset_index(drop=True,inplace=True)



# LOAD TEST CSV FILE

if Debug:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes,nrows=10000)

else:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes)

print ('Loaded',len(df_test),'rows of TEST.CSV!')
# 可以在调用这些函数的时候回来具体看原理

import math



# FACTORIZE

def factor_data(df_train, df_test, col):

    df_comb = pd.concat([df_train[col],df_test[col]],axis=0) # 函数的输入是训练集和测试集，为了得到所有数据的分布，首先合起来

    df_comb,_ = df_comb.factorize(sort=True) # pd原生factorize函数，相当于把所有的string/character类特征转换为了数字，一个特征对应一个unique的数字，并且sort

    # MAKE SMALLEST LABEL 1, RESERVE 0

    df_comb += 1 # factorize默认是从0开始，全部顺次+1

    # MAKE NAN LARGEST LABEL

    df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb) # 通过顺次+1后，理论上应该不存在0了，因此现在是0的代表曾经是NaN，则将这些列全部替换为max+1，作为最大的一种feature

                                                             # 这里保证了所有的unique value都不是0，是为了后面的statistical category encoding将那些不重要的数据点都换成0了

    df_train[col] = df_comb[:len(df_train)] # 拆分回去

    df_test[col] = df_comb[len(df_train):]

    del df_comb

    mx = max(df_train[col].max(),df_test[col].max())+1 #函数返回最大的一个label，其实就相当于得到了这个feature总共有多少种不同的值

    return mx

    

# OPTIMIZE MEMORY

def reduce_memory(df,col): 

    mx = df[col].max()

    if mx<256:

            df[col] = df[col].astype('uint8')

    elif mx<65536:

        df[col] = df[col].astype('uint16')

    else:

        df[col] = df[col].astype('uint32')

    

# LOG FREQUENCY ENCODE

def encode_FE_lg(df,col,verbose=1): # 这里df输入是整个数据集一样的数据，即df_train； col代表一个列，比如‘OSVersion’

    ln = 1/df[col].nunique()  # 首先看这一列特征，里面总共有多少个不同的值，取倒数

    vc = (df[col].value_counts(dropna=False, normalize=True)+ln).map(math.log).to_dict() # 几部分，第一，value_counts对每个特征计数，例如特征1总共出现了300次，特征2总共出现了1000次。因为normalize=True,因此

    # 进行了归一化，即特征1此时可能为0.0001。然后map(math.log).to_dict()进行了逐项取对数并转为字典

    nm = col+'_FE_lg'  # 重命名

    df[nm] = df[col].map(vc) # .map(vc)等同于调用了vc=这一行，然后替换掉df[col]里的对应位置

    df[nm] -= df[nm].min() # 这三行做了归一化

    df[nm] = df[nm]/df[nm].max()

    df[nm] = df[nm].astype('float32')

    if verbose==1:

        print('FE encoded',col)

    return [nm] #总体来说，encode_FE_lg即对一列特征进行了频率编码，然后归一化到[0, 1]之间



# STATISTICAL CATEGORY ENCODE

# 正如同上面，这里用的statistical category coding为：对于每一列，统计每一个unique value对HasDetection的概率分布，如果非常接近0.5，那么说明这个unique value对HasDetection的预测

# 作用不大，可以不考虑了，我们把这个unique value替换为0，因此每一列的unique value便可以减少很多

def encode_CE(df, col, filter, zscore, tar='HasDetections', m=0.5, verbose=1):  # 设置概率为0.5，因为整个数据集里HasDetection和非HasDetection的概率基本为0.5

    cv = pd.DataFrame( df[col].value_counts(dropna=False) ).reset_index()

    cv4 = df.groupby(col)[tar].mean().reset_index().rename({tar:'rate',col:'index'},axis=1)

    d1 = set(cv['index'].unique())

    cv = pd.merge(cv,cv4,on='index',how='left')

    if (len( cv[ cv['index'].isna() ])!=0 ):

        cv.loc[ cv['index'].isna(),'rate' ] = df.loc[ df[col].isna(),tar ].mean()

    cv = cv[ cv[col]> (filter * len(df)) ]

    cv['ratec'] = (df[tar].sum() - cv['rate']*cv[col])/(len(df)-cv[col])

    cv['sd'] = zscore * 0.5 / cv[col].map(lambda x: math.sqrt(x))

    cv = cv[ (abs(cv['rate']-m)>=cv['sd']) | (abs(cv['ratec']-1+m)>=cv['sd']) ]

    d2 = set(cv['index'].unique())

    d = list(d1 - d2)

    if (df[col].dtype.name=='category'):

        if (not 0 in df[col].cat.categories):

            df[col].cat.add_categories(0,inplace=True)

        else:

            print('###WARNING CAT 0 ALREADY EXISTS IN',col)

    df.loc[ df[col].isin(d),col ] = 0

    if verbose==1:

        print('CE encoded',col,'-',len(d2),'values. Removed',len(d),'values')

    mx = df[col].nunique() # mx是经过了编码还剩下多少unique values

    return [mx,d2]



# CATEGORY ENCODE FROM KEEP LIST

# 查看一下这一列中是否存在0，因为0代表着我们需要将所有的irrelevant value变成0，正常情况下只要经过了factor_data，每一列里应该都不存在0了

# 这个函数作用是确保我们已经进行了编码

def encode_CE_test(df,col,d):

    if (df[col].dtype.name=='category'):

        if (not 0 in df[col].cat.categories):

            df[col].cat.add_categories(0,inplace=True)

        else:

            print('###WARNING CAT 0 ALREADY EXISTS IN',col)

    df.loc[ ~df[col].isin(d),col ] = 0

    mx = df[col].nunique()

    return [mx,d]
# 这种feature engineering完全出自于对数据的分析所得，这是一个我们可以发掘的点，可以模仿这个函数去寻找是否有意义增加更多的feature column

# 直观上，添加feature的原理在于：如果Windows Denfender被升级到了最新版本18，那么很可能被检测病毒的概率就很低，然而AppVersion = 4.18.1807.18075进行factorization 

# 后只变成了0 1 2 。。。因此4.18.xxxx.xxxx和4.17.xxxx.xxxx，4.16.xxxx.xxxx的权重是一样的，将第二个数单独提取出来很可能是一个很强的feature，但在原始数据集中

# 没有被挖掘出来。同理，两个时间feature代表电脑是否AVSIGVersion过时，过时与否对感染病毒的影响可能很重要，但如果仅仅是时间本身可能并不包含这些信息。



def makeNew(df,verbose=1,add=0,TS=True,data=0):



    old = df.columns

    

    # FEATURE ENGINEER

    df['AppVersion2'] = df['AppVersion'].apply(lambda x: x.split('.')[1]).astype('category')



    if TS:

        from datetime import datetime, date, timedelta



        # AS timestamp

        datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

        df['DateAS'] = df['AvSigVersion'].map(datedictAS)



        # OS timestamp

        datedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy')[()]

        df['DateOS'] = df['Census_OSVersion'].map(datedictOS)



        df['Lag1'] = df['DateAS'] - df['DateOS']

        df['Lag1'] = df['Lag1'].map(lambda x: x.days//7)

        df['Lag1'] = df['Lag1']/52.0

        df['Lag1'] = df['Lag1'].astype('float32')

        df['Lag1'].fillna(0,inplace=True)

        

        if data!=0:

            if data==1:

                df['Lag5'] = datetime(2018,7,26) - df['DateAS'] # TRAIN

            elif data==2:

                df['Lag5'] = datetime(2018,9,27) - df['DateAS'] #PUBLIC TEST

            elif data==3:

                df['Lag5'] = datetime(2018,10,27) - df['DateAS'] #PRIVATE TEST

            df['Lag5'] = df['Lag5'].map(lambda x: x.days//1)

            df.loc[ df['Lag5']<0, 'Lag5' ] = 0

            df['Lag5'] = df['Lag5']/365.0

            df['Lag5'] = df['Lag5'].astype('float32')

            df['Lag5'].fillna(0,inplace=True)



        del df['DateAS'], df['DateOS']

        del datedictAS, datedictOS

        x=gc.collect()

    

    # NUMERIC ENCODE NE VARIABLES

    for col in NE:

        nm = col+'_NE'

        df[nm] = df[col].astype('float32')

        df[nm] /= np.std(df[nm])

    new = list(set(df.columns)-set(old))

    ret = []

    for x in new:

        if str(df[x].dtype)=='category': # if cat

            if add==1: OHE.append(x)

        else: 

            ret.append(x)

            df[x].fillna(df[x].mean(),inplace=True)

    if verbose==1:

        print('Engineered',len(new),'new features!')

    return ret
# GET FREQUENCY ENCODE LIST

FE = []

for col in df_train.columns:

    if col=='HasDetections': continue

    if df_train[col].nunique()>10: # 如果一列feature包含了多于10个不同的value，那么我们就使用频率编码，而不是one-hot-encoding

        FE.append(col) # FE即哪些列用频率编码



# FEATURE ENGINEER / NEUMERIC ENCODE

NUM = makeNew(df_train,verbose=0,add=1,data=1) 

makeNew(df_test,verbose=0,data=2) # 增加新的feature，如上一个函数所示

print('Engineered '+str(len(NUM))+' variables (including NE)')

ct = len(NUM)+1; cnew = ct

    

# FREQUENCY ENCODE

for x in FE: # 对于需要进行频率编码的column进行频率编码

    NUM += encode_FE_lg(df_train,x,verbose=0)

    encode_FE_lg(df_test,x,verbose=0)

    #print(str(ct)+': FE: '+x)

    ct += 1

print('Frequency encoded '+str(len(NUM)-cnew)+' variables')

    

# STATISTICAL CATEGORY ENCODE

inps={}; tt = 0

for col in OHE: # 对于需要one hot encoding的列进行one hot encoding编码

    factor_data(df_train,df_test,col) # 先将不好处理的string/character变成数值0，1，2，3。。。

    d = encode_CE(df_train,col,0.001,1)[1] # 然后进行statistical category encoding，减少每一列里不需要的value的数量

    encode_CE_test(df_test,col,d)

    inps[col] = factor_data(df_train,df_test,col)

    tt += inps[col]

    reduce_memory(df_train,col)

    reduce_memory(df_test,col)

    #print(str(ct)+': CE: '+col)

    ct += 1



# REMOVE UNNEEDED

for x in np.unique(NE+MM):

    del df_train[x]

    if x!='AvSigVersion': del df_test[x]

x = gc.collect()



mm = round(df_train.memory_usage(deep=True).sum() / 1024**2)

mm2 = round(df_test.memory_usage(deep=True).sum() / 1024**2)

print('Encoded '+str(len(NUM))+' non-CE variables and '+str(len(OHE))+' CE containing '+str(tt)+' unique values into '+str(mm)+' Mb memory')

print('Test memory is '+str(mm2)+' Mb')
from keras import callbacks

from sklearn.metrics import roc_auc_score



class printAUC(callbacks.Callback):

    def __init__(self, X_train, y_train, inps, fes, X_val, y_val, k, ee):

        super(printAUC, self).__init__()

        self.bestAUC = 0

        self.X_train = X_train

        self.y_train = y_train

        self.inps = inps

        self.fes = fes

        self.X_val = X_val

        self.y_val = y_val

        self.k = k

        self.ee = ee

        

    def on_epoch_end(self, epoch, logs={}):

        pred = self.model.predict([self.X_train[col] for col in self.inps] + [self.X_train[self.fes]])

        aucTR = roc_auc_score(self.y_train, pred)

        pred = self.model.predict([self.X_val[col] for col in self.inps] + [self.X_val[self.fes]])

        auc = roc_auc_score(self.y_val, pred)

        print ("Train AUC: " + str(round(aucTR,5))+" - Validation AUC: " + str(round(auc,5)))

        if (self.bestAUC < auc) :

            self.bestAUC = auc

            self.model.save("bestNet"+str(self.k)+".h5", overwrite=True)

        return
# DEFINE NETWORK ARCHITECTURE GROUPINGS

# (1) GEOGRAPHICAL, (2) SOFTWARE/VIRUS, (3) HARDWARE, (4) NAME/MODEL



# 这是这个模型的特点所在

# 按照地理位置，软件，硬件，以及模型参数对feature进行了分组

# 同一个组里的feature通过dense layer可以起到类似主成分分析（PCA）的降维作用，因为这些变量比较相关，优先在模型的初始阶段将他们进行组合，其在最开始阶段就能够提取到这些

# 相关变量的主成分，然后将更为具有代表性的feature输入到后面的网络。否则，不相关的变量相邻，无法起到相互影响的作用。



# 一个直观感受就是，['CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier','LocaleEnglishNameIdentifier',

# 'Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier', 'Wdft_RegionIdentifier']这一组中，其实很多feature是冗余的。例如一个国家

# 的大部分电脑可能都使用的同一种语言，并且大部分都用同一种OSUILocale，因此将相关的feature在最开始先做一次压缩降维，能够更简洁地输入网络



groups = [  ['CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier',

             'LocaleEnglishNameIdentifier','Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier',

            'Wdft_RegionIdentifier'],

            ['DefaultBrowsersIdentifier', 'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled',

             'IsProtected', 'SMode', 'IeVerIdentifier', 'SmartScreen', 'Firewall','Census_IsSecureBootEnabled',

            'Census_IsWIMBootEnabled','Wdft_IsGamer','Census_OSWUAutoUpdateOptionsName','Census_GenuineStateName',

            'AppVersion2'],

            ['Processor','Census_MDC2FormFactor','Census_DeviceFamily','Census_ProcessorCoreCount','Census_ProcessorClass',

            'Census_PrimaryDiskTypeName','Census_HasOpticalDiskDrive','Census_TotalPhysicalRAM','Census_ChassisTypeName',

            'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal',

            'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',

            'Census_InternalBatteryNumberOfCharges','Census_IsTouchEnabled','Census_IsPenCapable',

             'Census_IsAlwaysOnAlwaysConnectedCapable'],

            ['Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier',

            'Census_ProcessorModelIdentifier','Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier']

         ]
# Keras

from keras.models import Model

from keras.layers import Dense, Input, concatenate, BatchNormalization, Activation, Dropout, Embedding, Reshape

from keras.callbacks import LearningRateScheduler

from keras.optimizers import Adam



df_train_Y = df_train['HasDetections']

del df_train['HasDetections']

x=gc.collect()



#SPLIT TRAIN AND VALIDATION SET

# 手动定义5 fold训练

chunk = len(df_train)//5 # 5 fold validation

idx = range(chunk*0,chunk//2) 

idx2 = range(chunk//2,chunk)

idx3 = range(chunk,chunk*3)

idx4 = range(chunk*3,chunk*5)

X_val1 = df_train.loc[idx]

Y_val1 = df_train_Y.loc[idx]

X_val2 = df_train.loc[idx2]

Y_val2 = df_train_Y.loc[idx2]

X_train1 = df_train.loc[idx3]

Y_train1 = df_train_Y.loc[idx3]

X_train2 = df_train.loc[idx4]

Y_train2 = df_train_Y.loc[idx4]

del df_train, df_train_Y

x=gc.collect()
ins = []; outs = {}

# CREATE AN EMBEDDING FOR EACH CATEGORY VARIABLE

for k in inps.keys():

    x = Input(shape=(1,))

    ins.append(x)

    y = np.int(inps[k])

    x = Embedding(y, y, input_length=1)(x) # 这里使用的是keras的embedding [https://keras.io/layers/embeddings/]

    # Note: input_length - This argument is required if you are going to connect Flatten then Dense layers upstream (without it, the shape of the dense outputs cannot be computed).

    # input_length在这里应该是为了对齐形状的，不本质

    x = Reshape(target_shape=(y, ))(x)

    outs[k]=x 

    

# ORGANIZE EMBEDDINGS INTO GROUPS

all = set(inps.keys())

used = []

outs2 = []

for k in groups:

    g = [outs[x] for x in set(k).intersection(all)]

    used += list(set(k).intersection(all))

    x = concatenate(g)

    s = sum([inps[x] for x in set(k).intersection(all)])

    x = Dense(s//2,kernel_initializer='he_uniform')(x)

    x = BatchNormalization()(x)

    x = Activation('elu')(x)

    outs2.append(x)

g = [outs[x] for x in all-set(used)]

x = concatenate(g)

s = sum([inps[x] for x in all-set(used)])

x = Dense(s//2,kernel_initializer='he_uniform')(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

outs2.append(x)



# ORGANIZE FREQUENCY ENCODED AND NUMERICS INTO A GROUP

x = Input(shape=(len(NUM), ))

ins.append(x)

x = Dense(len(NUM)//2,kernel_initializer='he_uniform')(x)

x = BatchNormalization()(x)

x = Activation('elu')(x) 



# CONNECT GROUPS TO DENSE LAYERS

x = concatenate(outs2+[x])

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(1,activation='sigmoid')(x)



model = Model(inputs=ins, outputs=x)

model.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])

#annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)
epochs=10

batch=256

for k in range(epochs):

    # SPLIT TRAINING DATA IN HALF TO FIT INTO GPU MEMORY

    model.fit( [X_train1[col] for col in OHE] + [X_train1[NUM]],Y_train1,

        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 

        printAUC(X_train1, Y_train1, OHE, NUM, X_val1, Y_val1, 0, k)],

        validation_data = ([X_val1[col] for col in OHE] + [X_val1[NUM]],Y_val1) )

    model.fit( [X_train2[col] for col in OHE] + [X_train2[NUM]],Y_train2,

        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 

        printAUC(X_train2, Y_train2, OHE, NUM, X_val2, Y_val2, 0, k)],

        validation_data = ([X_val2[col] for col in OHE] + [X_val2[NUM]],Y_val2) )

    # SHUFFLE TRAIN

    X_train1['HasDetections'] = Y_train1

    X_train1 = X_train1.sample(frac=1)

    Y_train1 = X_train1['HasDetections']

    del X_train1['HasDetections']

    X_train2['HasDetections'] = Y_train2

    X_train2 = X_train2.sample(frac=1)

    Y_train2 = X_train2['HasDetections']

    del X_train2['HasDetections']

    x=gc.collect()
del model

del X_train1, Y_train1, X_val1, Y_val1

del X_train2, Y_train2, X_val2, Y_val2

del ins, outs, outs2, x

x = gc.collect()
# LOAD BEST SAVED NET

from keras.models import load_model



# PREDICT TEST

pred = np.zeros((len(df_test),1))

print('Predicting test...')

model = load_model('bestNet0.h5')

idx = 0; chunk = 1000000

if Debug: chunk = 5000

ct2 = 1;

while idx < len(df_test):

    idx2 = min(idx + chunk, len(df_test) )

    idx = range(idx, idx2)

    pred[idx] += model.predict( [df_test.iloc[idx][col] for col in OHE] + [df_test.iloc[idx][NUM]] )

    print(' part '+str(ct2)+' done')

    ct2 += 1

    idx = idx2

del model

x = gc.collect()
from datetime import datetime

datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

df_test['Date'] = df_test['AvSigVersion'].map(datedictAS)

df_test['HasDetections'] = pred

df_test['X'] = df_test['Date'] - datetime(2018,11,20,4,0) 

df_test['X'] = df_test['X'].map(lambda x: x.total_seconds()/86400)

df_test['X'].fillna(0,inplace=True)

s = 5.813888

df_test['F'] = 1.0

df_test['F'] = 1 - df_test['X']/s

df_test.loc[df_test['X']<=0,'F'] = 1.0

df_test.loc[df_test['X']>s,'F'] = 0

df_test['HasDetections'] *= df_test['F']

pred = df_test['HasDetections']
print('Writing submission file...')

if Debug:

    submit = pd.read_csv('../input/microsoft-malware-prediction/sample_submission.csv', nrows=10000)

else:

    submit = pd.read_csv('../input/microsoft-malware-prediction/sample_submission.csv')

submit['HasDetections'] = pred

submit.to_csv('submission.csv', index=False)

print('Done!')
import matplotlib.pyplot as plt    

b = plt.hist(pred, bins=200)
import calendar, math



def dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)

                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):

    # check for timestamps

    if 'Date' not in data:

        print('Error dynamicPlot: DataFrame needs column Date of datetimes')

        return

    

    # remove detection line if category density is too small

    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)

    cvd = cv.to_dict()

    nm = cv.index.values

    th = show * len(data)

    sum = 0; lnn2 = 0

    for x in nm:

        lnn2 += 1

        sum += cvd[x]

        if sum>th:

            break

    top = min(top,len(nm))

    top2 = min(top2,len(nm),lnn2,top)



    # calculate rate within each time interval

    diff = (end-start).days*24*3600 + (end-start).seconds

    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5

    data_counts = np.zeros([size,2*top+1],dtype=float)

    idx=0; idx2 = {}

    for i in range(top):

        idx2[nm[i]] = i+1

    low = start

    high = add_time(start,inc_mn,inc_dy,inc_hr)

    data_times = [low+(high-low)/2]

    while low<end:

        slice = data[ (data['Date']<high) & (data['Date']>=low) ]

        #data_counts[idx,0] = len(slice)

        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())

        for key in idx2:

            if nan_check(key): slice2 = slice[slice[col].isna()]

            else: slice2 = slice[slice[col]==key]

            data_counts[idx,idx2[key]] = len(slice2)

            if target in data:

                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()

        low = high

        high = add_time(high,inc_mn,inc_dy,inc_hr)

        data_times.append(low+(high-low)/2)

        idx += 1



    # plot lines

    fig = plt.figure(1,figsize=(15,3))

    cl = ['r','g','b','y','m']

    ax3 = fig.add_subplot(1,1,1)

    lines = []; labels = []

    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')

    for i in range(top):

        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])

        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')

        lines.append(tmp)

        labels.append(str(nm[i]))

    ax3.spines['left'].set_color('red')

    ax3.yaxis.label.set_color('red')

    ax3.tick_params(axis='y', colors='red')

    if col!='ones': ax3.set_ylabel('Category Density', color='r')

    else: ax3.set_ylabel('Data Density', color='r')

    #ax3.set_yticklabels([])

    if target in data:

        ax4 = ax3.twinx()

        for i in range(top2):

            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+":")

            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+"o")

        ax4.spines['left'].set_color('red')

        ax4.set_ylabel('Detection Rate', color='k')

    if title!='': plt.title(title)

    if legend==1: plt.legend(lines,labels,loc=2)

    plt.show()

        

# INCREMENT A DATETIME

def add_time(sdate,months=0,days=0,hours=0):

    month = sdate.month -1 + months

    year = sdate.year + month // 12

    month = month % 12 + 1

    day = sdate.day + days

    if day>calendar.monthrange(year,month)[1]:

        day -= calendar.monthrange(year,month)[1]

        month += 1

        if month>12:

            month = 1

            year += 1

    hour = sdate.hour + hours

    if hour>23:

        hour = 0

        day += 1

        if day>calendar.monthrange(year,month)[1]:

            day -= calendar.monthrange(year,month)[1]

            month += 1

            if month>12:

                month = 1

                year += 1

    return datetime(year,month,day,hour,sdate.minute)



# CHECK FOR NAN

def nan_check(x):

    if isinstance(x,float):

        if math.isnan(x):

            return True

    return False
df_test['ones'] = 1

dynamicPlot(df_test, 'ones', inc_dy=2, legend=0,

        title='Test.csv HasDetections Predictions. (Dotted line uses right y-axis. Solid uses left.)')
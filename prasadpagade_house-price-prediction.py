# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



from subprocess import check_output

print(check_output(["ls", "../input"]).decode("utf8"))



# Any results you write to the current directory are saved as output.
train = pd.read_csv('../input/train.csv')

test = pd.read_csv('../input/train.csv')

train.head()
print('Train data shape:', train.shape)

print('test data shape:', test.shape)
import matplotlib.pyplot as plt

plt.style.use(style='ggplot')

plt.rcParams['figure.figsize'] = (10,6)
# explore the data

train.SalePrice.describe()
# When performing regression, sometimes it makes sense to log-transform the target variable when it is skewed. One reason for this is to improve the linearity of the data. Although the justification is beyond the scope of this tutorial, more information can be found here.



# Importantly, the predictions generated by the final model will also be log-transformed, so we’ll need to convert these predictions back to their original form later.



#np.log() will transform the variable, and np.exp() will reverse the transformation
# Explore the Sales Price more

plt.hist(train.SalePrice, color='blue')

plt.show()
#Now we use np.log() to transform train.SalePric and calculate the skewness a second time, 

# as well as re-plot the data. A value closer to 0 means that we have improved the skewness of the data.

# We can see visually that the data will more resembles a normal distribution.
target = np.log(train.SalePrice)

print('Skew is:', target.skew())

plt.hist(target, color = 'blue')

plt.show()
# Now that we’ve transformed the target variable, let’s consider our features. First, we’ll check out the numerical features and make some plots. The 

# .select_dtypes() method will return a subset of columns matching the specified data types.
# Working with Numeric features

numeric_features = train.select_dtypes(include=[np.number])

numeric_features.dtypes
# Let's find the correlation between these features

corr = numeric_features.corr()



print(corr['SalePrice'].sort_values(ascending=False)[:5], '\n')

print(corr['SalePrice'].sort_values(ascending=False)[-5:])
train.OverallQual.unique()
quality_pivot = train.pivot_table(index='OverallQual', values='SalePrice', aggfunc=np.mean)



quality_pivot
quality_pivot.plot(kind='bar', color = 'blue')

plt.xlabel('Overall Quality')

plt.ylabel('Sale Price of the House')

plt.xticks(rotation=0)

plt.show()
plt.scatter(x=train['GrLivArea'], y=target)

plt.ylabel('Sale Price')

plt.xlabel('Above grade (ground) living area square feet')

plt.show()

plt.scatter(x=train['GarageArea'], y=target)

plt.ylabel('Sale Price')

plt.xlabel('Garage Area')

plt.show()
train = train[train['GarageArea'] < 1200]
plt.scatter(x=train['GarageArea'], y=np.log(train.SalePrice))

plt.xlim(-200,1600) # This forces the same scale as 

plt.ylabel('Sale Price')

plt.xlabel('Garage Area')

plt.show()
nulls = pd.DataFrame(train.isnull().sum().sort_values(ascending=False)[:25])

nulls.columns = ['Null Count']

nulls.index.name = 'Feature'

nulls
print ("Unique values are:", train.MiscFeature.unique())
categoricals = train.select_dtypes(exclude=[np.number])

categoricals.describe()
print ("Original: \n") 

print (train.Street.value_counts(), "\n")
train['enc_street'] = pd.get_dummies(train.Street, drop_first=True)

test['enc_street'] = pd.get_dummies(train.Street, drop_first=True)
print ('Encoded: \n') 

print (train.enc_street.value_counts())
condition_pivot = train.pivot_table(index='SaleCondition',

                                   values = 'SalePrice', aggfunc = np.median)

condition_pivot[:5]
condition_pivot.plot(kind='bar', color = 'blue')

plt.xlabel('Sale Condition')

plt.ylabel('Median Sale Price')

plt.xticks(rotation=0)

plt.show()
def encode(x): return 1 if x == 'Partial' else 0

train['enc_condition'] = train.SaleCondition.apply(encode)

test['enc_condition'] = test.SaleCondition.apply(encode)
condition_pivot = train.pivot_table(index='enc_condition', values='SalePrice', aggfunc=np.median)

condition_pivot.plot(kind='bar', color='blue')

plt.xlabel('Encoded Sale Condition')

plt.ylabel('Median Sale Price')

plt.xticks(rotation=0)

plt.show()
data = train.select_dtypes(include=[np.number]).interpolate().dropna()
sum(data.isnull().sum() != 0)
y = np.log(train.SalePrice)

X = data.drop(['SalePrice', 'Id'], axis=1)
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(

                                    X, y, random_state=42, test_size=.33)
from sklearn import linear_model

lr = linear_model.LinearRegression()
model = lr.fit(X_train,y_train)
print ("R^2 is: \n", model.score(X_test, y_test))
predictions = model.predict(X_test)
from sklearn.metrics import mean_squared_error

print ('RMSE is: \n', mean_squared_error(y_test, predictions))
actual_values = y_test

plt.scatter(predictions, actual_values, alpha=.75,

            color='b') #alpha helps to show overlapping data

plt.xlabel('Predicted Price')

plt.ylabel('Actual Price')

plt.title('Linear Regression Model')

plt.show()
for i in range (-2, 3):

    alpha = 10**i

    rm = linear_model.Ridge(alpha=alpha)

    ridge_model = rm.fit(X_train, y_train)

    preds_ridge = ridge_model.predict(X_test)

    

    plt.scatter(preds_ridge, actual_values, alpha=.75, color='b')

    plt.xlabel('Predicted Price')

    plt.ylabel('Actual Price')

    plt.title('Ridge Regularization with alpha = {}'.format(alpha))

    overlay = 'R^2 is: {}\nRMSE is: {}'.format(

                    ridge_model.score(X_test, y_test),

                    mean_squared_error(y_test, preds_ridge))

    plt.annotate(s=overlay,xy=(12.1,10.6),size='x-large')

    plt.show()
submission = pd.DataFrame()

submission['Id'] = test.Id

submission.head()
feats = test.select_dtypes(

        include=[np.number]).drop(['Id','SalePrice'], axis=1).interpolate()
pred = model.predict(feats)
final_predictions = np.exp(pred)
print ("Original predictions are: \n", predictions[:5], "\n")

print ("Final predictions are: \n", final_predictions[:5])
submission['SalePrice'] = final_predictions

submission.head()
pwd

submission.to_csv('submission1.csv', index=False)
import keras

import numpy as np

import pandas as pd

import matplotlib.pyplot as plt

from keras.models import Sequential

from keras import regularizers

from keras.utils import to_categorical

from mpl_toolkits.axes_grid1 import ImageGrid

from keras.layers import Conv2D, MaxPooling2D, BatchNormalization

from keras.layers import Dense, Dropout, Activation, Flatten, ZeroPadding2D

from keras.preprocessing.image import ImageDataGenerator
test_submission = pd.read_csv('beHappy.csv')

test_submission.head(3)
train_labels = pd.read_csv('train_labels.csv')

train_labels.head(3)
"""

train_images = []

for i in train_labels.ID.tolist():

    tmp = plt.imread('trainData/' + str(i) + '.png')

    dims = tmp.shape[0:2]

    tmp = tmp[:,:,0].reshape(dims + (1,)) # convert to a 48*48*1 pixel image

    train_images.append(np.array(tmp))

train_images = np.array(train_images)





test_images = []

for i in test_submission.ID.tolist():

    tmp = plt.imread('testData/' + str(i) + '.png')

    dims = tmp.shape[0:2]

    tmp = tmp[:,:,0].reshape(dims + (1,)) # convert to a 48*48*1 pixel image

    test_images.append(np.array(tmp))

test_images = np.array(test_images)





np.save('train_images.npy', train_images)

np.save('test_images.npy', test_images)

"""

train_images = np.load('train_images.npy')

test_images = np.load('test_images.npy')



trainLabels = train_labels.isHappy.tolist()
dims = train_images.shape[1:]

def createModel():

    model = Sequential()

    model.add(Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=dims))

    model.add(Conv2D(32, (3, 3), activation='relu'))

    model.add(MaxPooling2D(pool_size=(2, 2))) # reduces the size by select the max value

    model.add(Dropout(0.25)) # 25% of neurons randomly selected neurons are ignored during training



    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))

    model.add(Conv2D(64, (3, 3), activation='relu'))

    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dropout(0.25))



    model.add(Conv2D(64, (3, 3), padding='same', activation='relu'))

    model.add(Conv2D(64, (3, 3), activation='relu'))

    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Dropout(0.25))



    model.add(Flatten())# this converts our 3D feature maps to 1D feature vectors

    model.add(Dense(512, activation='relu'))

    model.add(Dropout(0.5))

    model.add(Dense(2, activation='softmax'))

    

    return model
model = createModel()

model.summary()
model.compile(loss='binary_crossentropy',

              optimizer='rmsprop',

              metrics=['accuracy'])
history = model.fit(train_images, 

                    to_categorical(trainLabels),

                    batch_size=32,

                    epochs=50,

                    validation_split=0.1,

                    shuffle=True)

model.save_weights('first_try.h5')  # always save your weights after training or during training
plt.figure(figsize = (18,6))

plt.subplot(121)

plt.plot(history.history['acc'], label='train')

plt.plot(history.history['val_acc'], label='validation')

plt.title('Accuracy over time')

plt.ylabel('accuracy')

plt.xlabel('epoch')

plt.legend()



plt.subplot(122)

plt.plot(history.history['loss'], label='train')

plt.plot(history.history['val_loss'], label='validation')

plt.title('Cross-entropy loss over time ')

plt.ylabel('loss')

plt.xlabel('epoch')

plt.legend()



plt.show()
# Split train test

train_data = train_images[:-500]

train_labels_one_hot = to_categorical(trainLabels)[:-500]



test_data = train_images[-500:] # use 500 image to test

test_labels_one_hot = to_categorical(trainLabels)[-500:]





model2 = createModel()

 

model2.compile(loss='binary_crossentropy',

              optimizer='rmsprop',

              metrics=['accuracy'])



batch_size = 256

epochs = 100

datagen = ImageDataGenerator(

#         zoom_range=0.2, # randomly zoom into images

#         rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)

        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)

        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)

        horizontal_flip=True,  # randomly flip images

        vertical_flip=False)  # randomly flip images

 

# Fit the model on the batches generated by datagen.flow().

history2 = model2.fit_generator(datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),

                              steps_per_epoch=int(np.ceil(train_data.shape[0] / float(batch_size))),

                              epochs=epochs,

                              validation_data=(test_data, test_labels_one_hot),

                              workers=4)



model2.evaluate(test_data, test_labels_one_hot)

model2.save_weights('second_try.h5')
plt.figure(figsize = (18,6))

plt.subplot(121)

plt.plot(history2.history['acc'], label='train')

plt.plot(history2.history['val_acc'], label='validation')

plt.title('Accuracy over time')

plt.ylabel('accuracy')

plt.xlabel('epoch')

plt.legend()



plt.subplot(122)

plt.plot(history2.history['loss'], label='train')

plt.plot(history2.history['val_loss'], label='validation')

plt.title('Cross-entropy loss over time ')

plt.ylabel('loss')

plt.xlabel('epoch')

plt.legend()



plt.show()
# Split train test

testNum = 1000

train_data = train_images[:-testNum]

train_labels_one_hot = to_categorical(trainLabels)[:-testNum]



test_data = train_images[-testNum:] # use 500 image to test

test_labels_one_hot = to_categorical(trainLabels)[-testNum:]





model3 = createModel()

 

model3.compile(loss='binary_crossentropy',

              optimizer='rmsprop',

              metrics=['accuracy'])



batch_size = 1024

epochs = 150

datagen = ImageDataGenerator(

         zoom_range=0.3, # randomly zoom into images

         rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)

        width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)

        height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)

        horizontal_flip=False,  # randomly flip images

        vertical_flip=False)  # randomly flip images

 

# Fit the model on the batches generated by datagen.flow().

history3 = model3.fit_generator(datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),

                              steps_per_epoch=30,

                              epochs=epochs,

                              validation_data=(test_data, test_labels_one_hot),

                              workers=4)



model3.evaluate(test_data, test_labels_one_hot)

model3.save_weights('third_try.h5')
plt.figure(figsize = (18,6))

plt.subplot(121)

plt.plot(history3.history['acc'], label='train')

plt.plot(history3.history['val_acc'], label='validation')

plt.title('Accuracy over time')

plt.ylabel('accuracy')

plt.xlabel('epoch')

plt.legend()



plt.subplot(122)

plt.plot(history3.history['loss'], label='train')

plt.plot(history3.history['val_loss'], label='validation')

plt.title('Cross-entropy loss over time ')

plt.ylabel('loss')

plt.xlabel('epoch')

plt.legend()



plt.show()
# Split train test

train_data = train_images

train_labels_one_hot = to_categorical(trainLabels)



model4 = createModel()

 

model4.compile(loss='binary_crossentropy',

              optimizer='rmsprop',

              metrics=['accuracy'])



batch_size = 1024

epochs = 150

datagen = ImageDataGenerator(

         zoom_range=0.25, # randomly zoom into images

         rotation_range=25,  # randomly rotate images in the range (degrees, 0 to 180)

        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)

        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)

        horizontal_flip=False,  # randomly flip images

        vertical_flip=False)  # randomly flip images

 

# Fit the model on the batches generated by datagen.flow().

history4 = model4.fit_generator(datagen.flow(train_data, train_labels_one_hot, batch_size=batch_size),

                              steps_per_epoch=30,

                              epochs=epochs,

                              workers=4)



#model4.evaluate(test_data, test_labels_one_hot)

model4.save_weights('forth_try.h5')
Q = model4.predict(test_images)

test_submission['isHappy'] = [int(i) for i in (Q[:,1] > 0.5)] 

# if happiness probability >0.5, it is happy

test_submission.head(10)
test_submission.to_csv('beHappy.csv', index=False)
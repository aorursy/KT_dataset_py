#Reference TFIDF-LightGBM Pipeline with RandomSearchCV

#https://www.kaggle.com/nikitabu/tfidf-lightgbm-pipeline-with-randomsearchcv



# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory



import os

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))



# Any results you write to the current directory are saved as output.
seed = 42



import random

random.seed(seed)



import torch

torch.manual_seed(seed)

torch.backends.cudnn.deterministic = True

if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)

    

import numpy as np

np.random.seed(seed)

param_dist = {

    #"vect__word_vect__ngram_range":  [(1, 1), (1, 2), (1, 3)],

    "word_vect__ngram_range":  [(1, 1), (1, 2), (1, 3)],

    #"vect__char_vect__ngram_range":  [(1, 1), (1, 2), (1, 3)],

    "clf__n_estimators":             np.arange(2000,4000,500),

    "clf__learning_rate":            [0.001, 0.005, 0.01, 0.05, 0.07, 0.1, 0.5],

    "clf__num_leaves":               np.arange(64,1024,128),

    "clf__max_bin":                  [4, 8, 16, 32, 64],

    "clf__subsample":                np.arange(0.5, 1.0, 0.05),

    "clf__subsample_freq":           [1, 2, 4, 8, 16, 32, 64, 128, 256],

    "clf__reg_lambda":               [1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 0]

}



n_iter_search = 10 # Define number of search iterations



n_folds = 5 # Define number of CV folds



max_vec_features = 10000 # Max number of features generated by TFIDF
from sklearn.feature_extraction.text import TfidfVectorizer

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import cross_val_score, RandomizedSearchCV

from scipy.sparse import hstack

from scipy.special import logit, expit

from sklearn.pipeline import Pipeline, FeatureUnion

from time import time

from lightgbm import LGBMClassifier;
class_names = ['target']



train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv').fillna(' ')

print(train.shape)

test = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv').fillna(' ')

print(test.shape)



#train_text = train['text']

#test_text = test['text']



#all_text = pd.concat([train_text, test_text])



from nltk.tokenize import TweetTokenizer

twt = TweetTokenizer(strip_handles=True)



import re

def tweets(r):

    s = ' '.join(twt.tokenize(r['text']))

    s = re.sub(r'http\S+', '', s)

    s = re.sub(r'https\S+', '', s)    

    return s

train['ptext'] = train.apply(tweets, axis=1)

test['ptext'] = test.apply(tweets, axis=1)



train_text = train['ptext']

test_text = test['ptext']



all_text = pd.concat([train_text, test_text])
word_vectorizer = TfidfVectorizer(sublinear_tf=True,

                                  strip_accents='unicode',

                                  analyzer='word',

                                  token_pattern=r'\w{1,}',

                                  max_features=max_vec_features)



char_vectorizer = TfidfVectorizer(sublinear_tf=True,

                                  strip_accents='unicode',

                                  analyzer='char',

                                  max_features=max_vec_features)



classifier = LGBMClassifier()



word_char_vectorizer = FeatureUnion([

    ('word_vect', word_vectorizer),

    ('char_vect', char_vectorizer),

     ])

     

#pipeline = Pipeline([

#    ('vect', word_char_vectorizer),

#    ('clf', classifier),

#     ])

pipeline = Pipeline([

    ('word_vect', word_vectorizer),

    ('clf', classifier),

     ])
random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, 

                                   n_iter=n_iter_search, cv=n_folds, 

                                   scoring='roc_auc', n_jobs=-1, verbose=2)



start = time()



class_name = class_names[0]

train_target = train[class_name]



random_search.fit(train_text.values, train_target.values)



print("RandomizedSearchCV took %.2f seconds for %d candidates"

      " parameter settings." % ((time() - start), n_iter_search))

      

print("\nBest Score = " + str(random_search.best_score_))



print("\nBest Parameters = " + str(random_search.best_params_))

start = time()

losses = []

test_predictions  = {'id': test['id']}

classifier = pipeline.set_params(

    #vect__word_vect__ngram_range = random_search.best_params_['vect__word_vect__ngram_range'], 

    word_vect__ngram_range = random_search.best_params_['word_vect__ngram_range'], 

    #vect__char_vect__ngram_range = random_search.best_params_['vect__char_vect__ngram_range'], 

    clf__n_estimators            = random_search.best_params_['clf__n_estimators'], 

    clf__learning_rate           = random_search.best_params_['clf__learning_rate'], 

    clf__num_leaves              = random_search.best_params_['clf__num_leaves'], 

    clf__max_bin                 = random_search.best_params_['clf__max_bin'], 

    clf__subsample               = random_search.best_params_['clf__subsample'], 

    clf__subsample_freq          = random_search.best_params_['clf__subsample_freq'], 

    clf__reg_lambda              = random_search.best_params_['clf__reg_lambda'] 

    )

                                     

cv_loss = np.mean(cross_val_score(classifier, train_text.values, train_target, 

                                  cv=n_folds, scoring='roc_auc'))

losses.append(cv_loss)

print('CV score for class {} is {}'.format(class_name, cv_loss))



classifier.fit(train_text.values, train_target)

test_predictions[class_name] = classifier.predict_proba(test_text.values)[:, 1]

test_predictions[class_name] = [1 if x>=0.5 else 0 for x in test_predictions[class_name]]

print("Fitting and predictions took %.2f seconds" % (time() - start) )



test_submission = pd.DataFrame.from_dict(test_predictions)

test_submission.to_csv('submission.csv', index=False)
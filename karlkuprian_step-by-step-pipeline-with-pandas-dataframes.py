# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory



import os

for dirname, _, filenames in os.walk('/kaggle/input'):

    for filename in filenames:

        print(os.path.join(dirname, filename))



# Any results you write to the current directory are saved as output.
import pandas as pd

import matplotlib.pyplot as plt

import numpy as np

import seaborn as sns

import numpy as np
df_train = pd.read_csv("../input/titanic/train.csv")

df_train.head()
#create copy for further processing

df = df_train.copy()

df.isnull().sum()
from sklearn.impute import SimpleImputer

imputer = SimpleImputer()

df[['Age']] = imputer.fit_transform(df[['Age']])

df.isnull().sum()
class df_SimpleImputer(SimpleImputer):  #(BaseEstimator, TransformerMixin):

    '''

    Just a wrapper for the SimpleImputer that keeps the dataframe structure

    '''

    def transform(self, X,**kwargs):

        Array = SimpleImputer.transform(self,X,**kwargs)

        return pd.DataFrame(Array, index=X.index, columns=X.columns)



imputer = df_SimpleImputer()

imputer.fit_transform(df[['Age']]).transpose()
from sklearn.pipeline import Pipeline

from sklearn.impute import SimpleImputer

from sklearn.compose import ColumnTransformer





preprocessor = ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age'])

    ], remainder = 'passthrough')



df = df_train.copy()

array = preprocessor.fit_transform(df)

moddf = pd.DataFrame(array, index=df.index, columns=df.columns)

moddf.head()

# Note!: Columns are incorrectly assigned, chronology of column needs to be modified!
preprocessor = ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age'])

    ], remainder = 'passthrough')



df = df_train.copy()

nparray = preprocessor.fit_transform(df)

nparray
class df_ColumnTransformer(ColumnTransformer):

    """

    I cannot garanty that this Transformer works well in combination all sklearn Transformers

    Tested with: SimpleImputer, OneHotEncoder, OrdinalEncoder

    Applies transformers to columns of an array or pandas DataFrame.

    In contrast to the sklearn ColumnTransfromer, this transfomer returns a pandas Dataframe



    This estimator allows different columns or column subsets of the input

    to be transformed separately and the features generated by each transformer

    will be concatenated to form a single feature space.

    This is useful for heterogeneous or columnar data, to combine several

    feature extraction mechanisms or transformations into a single transformer.

    """



    def fit_transform(self, X, *args, **kwargs):

        ColumnTransformer.fit_transform(self, X)

        return df_ColumnTransformer.transform(self, X, *args, **kwargs)



    def transform(self, X, *args, **kwargs):



        ret_df = pd.DataFrame(index=X.index) #columns=cols)

        for row in self.transformers_: # row includes all information for one transformer step.

            transformer = row[1] # tranformer to be applied

            lst_of_cols_transformed = row[2] #columns to be transformed with this transformer

            if isinstance(transformer, str):

                # passthrough other elments if passthrough is set

                if transformer == 'passthrough':

                    cols = []

                    for index in lst_of_cols_transformed:

                        cols.append(X.columns[index])

                    ret_df[cols] = X[cols]

            else:

                transformed = transformer.transform(+X[lst_of_cols_transformed])

                if transformed.shape[1] == len(lst_of_cols_transformed):

                    #Set provided DataFrame columns as given in the cols

                    for i, col in enumerate(lst_of_cols_transformed):

                        if isinstance(transformed,pd.DataFrame):

                            transformed = transformed.values

                        ret_df[col] = transformed[:,i]

                else:

                    # e.g. with OneHotEncoder, more columns are provided, in this case it is not always possible to find

                    # out which output column is related to which input column

                    if isinstance(transformed, pd.DataFrame):

                        ret_df = pd.concat([ret_df, transformed], axis=1)

                    else:

                        cols = []

                        for i in range(1, transformed.shape[1] + 1):

                            cols.append('-'.join(lst_of_cols_transformed) + '_' + str(i))

                        print(type(transformed))

                        # if tranformer returns ndarray convert to Dataframe

                        if isinstance(transformed,np.ndarray):

                            temp_df = pd.DataFrame(transformed, index=X.index, columns=cols)

                        else:

                        # if sparse matrix is returned by transformer

                            temp_df = pd.DataFrame.sparse.from_spmatrix(transformed, index=X.index, columns=cols)

                            temp_df = temp_df.sparse.to_dense()

                        ret_df = pd.concat([ret_df, temp_df], axis=1)



        return ret_df
#Preprocessor #1

preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head(3)
moddf.isnull().sum()
from sklearn.preprocessing import OrdinalEncoder



#Preprocessor #2

process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')),

    ('label_encoder', OrdinalEncoder())

])



preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head()
df[['Age','Pclass']].groupby('Pclass').mean()
from sklearn.preprocessing import OneHotEncoder



#Preprocessor #3

process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')),

    ('label_encoder', OneHotEncoder())

])



preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head()
class df_OneHotEncoder(OneHotEncoder):

    def transform(self,X,*args,**kwargs):

        uniquedict = {}

        for col in X.columns:

            lst = list(X[col].unique())

            lst.sort()

            uniquedict[col] = lst

        Array = OneHotEncoder.transform(self,X,*args,**kwargs)

        newcolumns = []

        for key in uniquedict:

            for value in uniquedict[key]:

                newcolumns.append(str(key)+'_'+str(value))

        temp_df = pd.DataFrame.sparse.from_spmatrix(Array,columns=newcolumns,index=X.index)

        temp_df = temp_df.sparse.to_dense()

        return  temp_df
#Preprocessor #4



process_embarked = Pipeline(steps=[

    ('imputer', df_SimpleImputer(strategy='most_frequent')), 

    #The df_OneHotEncoder requires a Dataframe as input, therefore df_SimpleImputer must be used instead of the SimpleImputer.

    ('label_encoder', df_OneHotEncoder())

])



preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head()
from sklearn.base import BaseEstimator, TransformerMixin



class df_Process_Cabin(BaseEstimator, TransformerMixin):

    '''

    If a value is given in cell it is set to 1, if nan cell is set to 0

    Empty cabine fields are assigned with 'None'

    '''

    

    def fit(self,*args,**kwargs):

        return self

    

    def transform(self,X):

        for col in X.columns:

            X[col+"_Deck"] = X[col].str.extract('([A-Za-z]).', expand=False)

            X[col+"_Deck"] = X[col+"_Deck"].fillna('None')

            X[col] = X[col].notnull().astype("int")

        return X



#Just to have this without df as well

class Process_Cabin(df_Process_Cabin):

    #no changes the sklearn transformes will convert this to numpy arrays anyway

    def fit(self,*args,**kwargs):

        return self
#Preprocessor #5



process_embarked = Pipeline(steps=[

    ('imputer', df_SimpleImputer(strategy='most_frequent')), 

    #The df_OneHotEncoder requires a Dataframe as input, therefore df_SimpleImputer must be used instead of the SimpleImputer.

    ('label_encoder', df_OneHotEncoder())

])





preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', df_Process_Cabin(), ['Cabin'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head(6)
sns.barplot(x= "Cabin_Deck", y = "Survived", data= moddf);
#Preprocessor #6



process_cabin =  Pipeline(steps=[

    ('cab', df_Process_Cabin()), 

    ('label_encoder', df_ColumnTransformer(

    transformers=[ ('deck', OrdinalEncoder(), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', df_SimpleImputer(strategy='most_frequent')), 

    #The df_OneHotEncoder requires a Dataframe as input, therefore df_SimpleImputer must be used instead of the SimpleImputer.

    ('label_encoder', df_OneHotEncoder())

])





preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', process_cabin, ['Cabin'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head(6)
class ExtractTitle(BaseEstimator, TransformerMixin):

    """

    Extracts title from name in titanic machine learning competion

    Basic extraction reused from:

    https://www.kaggle.com/muhammetcimci/easy-titanic-survival-prediction-notebook

    """

    def fit(*args, **kwargs):

        #Nothing to do

        return



    def fit_transform(self,X,*args):

        # Just transform

        return self.transform(X)



    def transform(self, X):

        # Create a list of series from input data.

        # This is needed, as the ColumnTransformer will provide a numpy array

        # I included several options for flexibility, Normally one would be sufficient.

        list_of_Series = []

        if isinstance(X,pd.core.series.Series):

            list_of_Series = [X]

        elif isinstance(X,np.ndarray):

            for i in range(0,X.shape[1]):

                list_of_Series.append(pd.Series(X[:,i]))

        elif isinstance(X,pd.core.frame.DataFrame):

            for col in X.columns:

                list_of_Series.append(X[col])

        else:

            warnings.warn('Datatype "'+str(type(X))+'" not suppoted!')

            

        #Create empty numpy array, which holds result later.

        result = np.empty(shape=(len(list_of_Series[0]),len(list_of_Series)), dtype = 'O')

        

        for i,series in enumerate(list_of_Series):

            title = series.str.extract(' ([A-Za-z]+)\.', expand=False)

            title = title.replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')

            title = title.replace(['Countess', 'Lady', 'Sir'], 'Royal')

            title = title.replace('Mlle', 'Miss')

            title = title.replace('Ms', 'Miss')

            title = title.replace('Mme', 'Mrs')

            result[:,i] = title.values

        return result



# 

class df_ExtractTitle(ExtractTitle):

    """

    Extracts title from name in titanic machine learning competion

    Returns Dataframe

    """



    def transform(self, X):

        columns = list(X.columns)

        for i,col in enumerate(columns):

            columns[i] = col+'_Title'

        matrix = ExtractTitle.transform(self, X)

        return pd.DataFrame(matrix, index = X.index, columns = columns)
#Preprocessor #7



process_cabin =  Pipeline(steps=[

    ('cab', df_Process_Cabin()), 

    ('label_encoder', df_ColumnTransformer(

    transformers=[ ('deck', OrdinalEncoder(), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', df_SimpleImputer(strategy='most_frequent')), 

    #The df_OneHotEncoder requires a Dataframe as input, therefore df_SimpleImputer must be used instead of the SimpleImputer.

    ('label_encoder', df_OneHotEncoder())

])

process_title = Pipeline(steps=[

    ('title_extract', df_ExtractTitle() ),

    ('encode', df_OneHotEncoder())

])



preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', process_cabin, ['Cabin']),

        ('name', process_title, ['Name']),

        ('sex', OrdinalEncoder(), ['Sex'])

    ], remainder = 'passthrough')



df = df_train.copy()

moddf = preprocessor.fit_transform(df)

moddf.head(4)
class Copier(BaseEstimator, TransformerMixin):

    """

    Simply takes over column as they are.

    """

    def fit(*args, **kwargs):

        #Nothing to do

        return



    def fit_transform(self,X,*args):

        # Just transform

        return self.transform(X)



    def transform(self, X):

        return X
#Preprocessor #8



process_cabin =  Pipeline(steps=[

    ('cab', df_Process_Cabin()), 

    ('label_encoder', df_ColumnTransformer(

    transformers=[ ('deck', OrdinalEncoder(), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', df_SimpleImputer(strategy='most_frequent')), 

    #The df_OneHotEncoder requires a Dataframe as input, therefore df_SimpleImputer must be used instead of the SimpleImputer.

    ('label_encoder', df_OneHotEncoder())

])

process_title = Pipeline(steps=[

    ('title_extract', df_ExtractTitle() ),

    ('encode', df_OneHotEncoder())

])



preprocessor = df_ColumnTransformer(

    transformers=[

        ('num', df_SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', process_cabin, ['Cabin']),

        ('name', process_title, ['Name']),

        ('sex', OrdinalEncoder(), ['Sex']),

        ('onehot', df_OneHotEncoder(), ['Pclass']),

        ('unmodified', Copier(), ['Fare','Parch','SibSp'])

    ], remainder = 'drop') #drops ticket



df = df_train.copy()
%%time

moddf = preprocessor.fit_transform(df)

moddf.head(3)
#Sklearn Preprocessor



process_cabin =  Pipeline(steps=[

    ('cab', Process_Cabin()), 

    ('label_encoder', ColumnTransformer(

    transformers=[ ('deck', OrdinalEncoder(), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')), 

    ('label_encoder', OneHotEncoder())

])

process_title = Pipeline(steps=[

    ('title_extract', ExtractTitle() ),

    ('encode', OneHotEncoder())

])



preprocessor = ColumnTransformer(

    transformers=[

        ('num', SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', process_cabin, ['Cabin']),

        ('name', process_title, ['Name']),

        ('sex', OrdinalEncoder(), ['Sex']),

        ('onehot', OneHotEncoder(), ['Pclass']),

        ('unmodified', Copier(), ['Fare','Parch','SibSp'])

    ], remainder = 'drop') #drops ticket



df = df_train.copy()
%%time

nparray = preprocessor.fit_transform(df)

nparray[1,:]
if np.sum(np.equal(nparray,moddf.values)) == nparray.size:

    print('All elements are equal!')
#Preproccesing-Modeling Pipeline #1

from sklearn.ensemble import GradientBoostingClassifier



process_cabin =  Pipeline(steps=[

    ('cab', Process_Cabin()), 

    ('label_encoder', ColumnTransformer(

    transformers=[ ('deck', OrdinalEncoder(), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')), 

    ('label_encoder', OneHotEncoder(handle_unknown='ignore'))

])

process_title = Pipeline(steps=[

    ('title_extract', ExtractTitle() ),

    ('encode', OneHotEncoder())

])



preprocessor = ColumnTransformer(

    transformers=[

        ('num', SimpleImputer(), ['Age']),

        ('embarked', process_embarked, ['Embarked']),

        ('cabin', process_cabin, ['Cabin']),

        ('name', process_title, ['Name']),

        ('sex', OrdinalEncoder(), ['Sex']),

        ('onehot', OneHotEncoder(handle_unknown='ignore'), ['Pclass']),

        ('unmodified', Copier(), ['Fare','Parch','SibSp'])

    ], remainder = 'drop') #drops ticket

model = GradientBoostingClassifier(random_state=0)



process = Pipeline(steps=[('preprocessor', preprocessor),

                              ('model', model)

                             ])
from sklearn.model_selection import cross_val_score

df = df_train.copy()

y = df['Survived']

df = df.drop('Survived',axis = 1) #make sure that Survived is really not used in fitting

X = df



cross_val_score(process,X,y).mean()
#Preproccesing-Modeling Pipeline #2

from xgboost import XGBClassifier

from sklearn.model_selection import train_test_split



process_cabin =  Pipeline(steps=[

    ('cab', Process_Cabin()), 

    ('label_encoder', ColumnTransformer(

    transformers=[ ('deck', OneHotEncoder(handle_unknown='ignore',categories='auto'), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')), 

    ('label_encoder', OneHotEncoder(handle_unknown='ignore',categories='auto'))

])

process_title = Pipeline(steps=[

    ('title_extract', ExtractTitle() ),

    ('encode', OneHotEncoder(handle_unknown='ignore',categories='auto'))

])



preprocessor = ColumnTransformer(

    transformers=[

        ('num', SimpleImputer(), ['Age','Fare']), #Fare has nan in the test data

        ('cab', process_cabin, ['Cabin']), #Categorize and imputes at the same time

        ('cat', OrdinalEncoder(), ['Sex']),

        ('class', OrdinalEncoder(), ['Pclass']),

        #('parch', impute_onehot, ['Parch']),

        ('embarked', process_embarked, ['Embarked']),

        ('name', process_title, ['Name']),

        #('copy', Copier(), ['SibSp'])

    ], remainder = 'drop') #drops ticket





# Validate

df = df_train.copy()

y = df['Survived'].astype(int).values

df = df.drop('Survived',axis = 1) #make sure that Survived is really not used in fitting

X = df

# Validation set for stopping round of XGBclassifier

__, val_x, __, val_y = train_test_split(X, y, test_size = 0.30, random_state=4)



model = XGBClassifier(early_stopping_round=5, eval_set=(val_x, val_y),learning_rate = 0.08,random_state = 8)



process = Pipeline(steps=[('preprocessor', preprocessor),

                              ('model', model)

                             ])



cross_val_score(process,X,y,cv=4, scoring = 'accuracy').mean()
# train

df = df_train.copy()

y_train = df_train['Survived'].values

df = df.drop(['Survived'],axis = 1)

X_train = df

X_test = pd.read_csv("../input/titanic/test.csv")



# Validation set for stopping round of XGBclassifier -> Complete X_train and y_train can be used.

X_train_class = preprocessor.fit_transform(X_train) # preprocessor should be performed on this data as well.

model = XGBClassifier(early_stopping_round=8, eval_set=(X_train_class, y_train),learning_rate = 0.08,random_state = 8)



process = Pipeline(steps=[('preprocessor', preprocessor),

                              ('model', model)

                             ])

                                            

process.fit(X_train,y_train)

y_pred = process.predict(X_test)

y_pred
output = pd.DataFrame({ 'PassengerId' : X_test['PassengerId'].values, 'Survived': y_pred })

output
output.to_csv('submission.csv',index=False)
def auto_model_cross_val(X, y,preprocessor, model_class_ptr, optimize_param, param_list, model_kwargs=dict(), cross_val_kwargs={'scoring':'accuracy','cv':5}):

    """

    Preform multiple cross_validations of model with modified model parameter

    :param val_set:

    :param y_name:

    :param model_class_ptr:

    :param optimize_param:

    :param param_list:

    :param model_kwargs:

    :return:

    """

    from sklearn.model_selection import cross_val_score

    from sklearn.pipeline import Pipeline

    import pandas as pd



    score_df = pd.DataFrame()

    score_df[optimize_param] = param_list

    score_df = score_df.set_index(optimize_param)

    score_list = []

    for param in param_list:

        model_kwargs[optimize_param] = param

        model_instance = model_class_ptr(**model_kwargs)

        process = Pipeline(steps=[('preprocessor', preprocessor),

                                  ('model', model_instance)

                                  ])

        score = cross_val_score(process, X, y, **cross_val_kwargs).mean()

        score_list.append(score)

    score_df[cross_val_kwargs['scoring']] = score_list

    return score_df
from xgboost import XGBClassifier



y_train = df_train['Survived'].astype(int).values



process_cabin =  Pipeline(steps=[

    ('cab', Process_Cabin()), 

    ('label_encoder', ColumnTransformer(

    transformers=[ ('deck', OneHotEncoder(handle_unknown='ignore',categories='auto'), ['Cabin_Deck']),

                 ] , remainder = 'passthrough'))

])



process_embarked = Pipeline(steps=[

    ('imputer', SimpleImputer(strategy='most_frequent')), 

    ('label_encoder', OneHotEncoder(handle_unknown='ignore',categories='auto'))

])

process_title = Pipeline(steps=[

    ('title_extract', ExtractTitle() ),

    ('encode', OneHotEncoder(handle_unknown='ignore',categories='auto'))

])



preprocessor = ColumnTransformer(

    transformers=[

        ('num', SimpleImputer(), ['Age','Fare']), #Fare has nan in the test data

        ('cab', process_cabin, ['Cabin']), #Categorize and imputes at the same time

        ('cat', OrdinalEncoder(), ['Sex']),

        ('class', OrdinalEncoder(), ['Pclass']),

        #('parch', impute_onehot, ['Parch']),

        ('embarked', process_embarked, ['Embarked']),

        ('name', process_title, ['Name']),

        #('copy', Copier(), ['SibSp'])

    ], remainder = 'drop') #drops ticket



__, val_x, __, val_y = train_test_split(df_train, y_train, test_size = 0.30, random_state=8)



auto_model_cross_val(df_train, y_train,preprocessor, XGBClassifier, 'learning_rate',param_list=[0.06,0.07,0.08,0.09,0.1,0.11], 

                         model_kwargs = {'early_stopping_rounds':8, 'eval_set':[(val_x, val_y)]})